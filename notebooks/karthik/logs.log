2025-02-24 21:58:28,532:INFO:PyCaret RegressionExperiment
2025-02-24 21:58:28,533:INFO:Logging name: reg-default-name
2025-02-24 21:58:28,533:INFO:ML Usecase: MLUsecase.REGRESSION
2025-02-24 21:58:28,533:INFO:version 3.3.2
2025-02-24 21:58:28,533:INFO:Initializing setup()
2025-02-24 21:58:28,533:INFO:self.USI: 3379
2025-02-24 21:58:28,533:INFO:self._variable_keys: {'exp_id', 'exp_name_log', 'gpu_n_jobs_param', 'fold_shuffle_param', 'X_train', 'y_test', 'target_param', 'X_test', 'seed', '_ml_usecase', 'USI', 'data', 'n_jobs_param', 'fold_generator', 'fold_groups_param', 'pipeline', 'idx', '_available_plots', 'y', 'gpu_param', 'log_plots_param', 'memory', 'X', 'y_train', 'html_param', 'transform_target_param', 'logging_param'}
2025-02-24 21:58:28,533:INFO:Checking environment
2025-02-24 21:58:28,533:INFO:python_version: 3.10.15
2025-02-24 21:58:28,533:INFO:python_build: ('main', 'Oct  3 2024 02:24:49')
2025-02-24 21:58:28,533:INFO:machine: arm64
2025-02-24 21:58:28,533:INFO:platform: macOS-14.1-arm64-arm-64bit
2025-02-24 21:58:28,533:INFO:Memory: svmem(total=17179869184, available=5840076800, percent=66.0, used=7654162432, free=169525248, active=5722750976, inactive=5661687808, wired=1931411456)
2025-02-24 21:58:28,533:INFO:Physical Core: 8
2025-02-24 21:58:28,533:INFO:Logical Core: 8
2025-02-24 21:58:28,533:INFO:Checking libraries
2025-02-24 21:58:28,533:INFO:System:
2025-02-24 21:58:28,533:INFO:    python: 3.10.15 (main, Oct  3 2024, 02:24:49) [Clang 14.0.6 ]
2025-02-24 21:58:28,533:INFO:executable: /Users/daaa/opt/miniconda3/envs/mlops/bin/python
2025-02-24 21:58:28,533:INFO:   machine: macOS-14.1-arm64-arm-64bit
2025-02-24 21:58:28,533:INFO:PyCaret required dependencies:
2025-02-24 21:58:28,533:INFO:                 pip: 24.2
2025-02-24 21:58:28,533:INFO:          setuptools: 75.1.0
2025-02-24 21:58:28,533:INFO:             pycaret: 3.3.2
2025-02-24 21:58:28,533:INFO:             IPython: 8.30.0
2025-02-24 21:58:28,533:INFO:          ipywidgets: 8.1.5
2025-02-24 21:58:28,533:INFO:                tqdm: 4.67.1
2025-02-24 21:58:28,533:INFO:               numpy: 1.26.4
2025-02-24 21:58:28,533:INFO:              pandas: 2.1.4
2025-02-24 21:58:28,533:INFO:              jinja2: 3.1.4
2025-02-24 21:58:28,533:INFO:               scipy: 1.11.4
2025-02-24 21:58:28,533:INFO:              joblib: 1.3.2
2025-02-24 21:58:28,533:INFO:             sklearn: 1.4.2
2025-02-24 21:58:28,533:INFO:                pyod: 2.0.2
2025-02-24 21:58:28,533:INFO:            imblearn: 0.12.4
2025-02-24 21:58:28,533:INFO:   category_encoders: 2.6.4
2025-02-24 21:58:28,533:INFO:            lightgbm: 4.5.0
2025-02-24 21:58:28,533:INFO:               numba: 0.60.0
2025-02-24 21:58:28,533:INFO:            requests: 2.32.3
2025-02-24 21:58:28,533:INFO:          matplotlib: 3.7.5
2025-02-24 21:58:28,533:INFO:          scikitplot: 0.3.7
2025-02-24 21:58:28,533:INFO:         yellowbrick: 1.5
2025-02-24 21:58:28,533:INFO:              plotly: 5.24.1
2025-02-24 21:58:28,533:INFO:    plotly-resampler: Not installed
2025-02-24 21:58:28,533:INFO:             kaleido: 0.2.1
2025-02-24 21:58:28,533:INFO:           schemdraw: 0.15
2025-02-24 21:58:28,533:INFO:         statsmodels: 0.14.4
2025-02-24 21:58:28,533:INFO:              sktime: 0.26.0
2025-02-24 21:58:28,533:INFO:               tbats: 1.1.3
2025-02-24 21:58:28,533:INFO:            pmdarima: 2.0.4
2025-02-24 21:58:28,533:INFO:              psutil: 6.1.0
2025-02-24 21:58:28,533:INFO:          markupsafe: 2.1.5
2025-02-24 21:58:28,534:INFO:             pickle5: Not installed
2025-02-24 21:58:28,534:INFO:         cloudpickle: 3.1.0
2025-02-24 21:58:28,534:INFO:         deprecation: 2.1.0
2025-02-24 21:58:28,534:INFO:              xxhash: 3.5.0
2025-02-24 21:58:28,534:INFO:           wurlitzer: 3.1.1
2025-02-24 21:58:28,534:INFO:PyCaret optional dependencies:
2025-02-24 21:58:28,534:INFO:                shap: 0.44.1
2025-02-24 21:58:28,534:INFO:           interpret: 0.6.9
2025-02-24 21:58:28,534:INFO:                umap: 0.5.7
2025-02-24 21:58:28,534:INFO:     ydata_profiling: 4.12.1
2025-02-24 21:58:28,534:INFO:  explainerdashboard: 0.4.8
2025-02-24 21:58:28,534:INFO:             autoviz: Not installed
2025-02-24 21:58:28,534:INFO:           fairlearn: 0.7.0
2025-02-24 21:58:28,534:INFO:          deepchecks: Not installed
2025-02-24 21:58:28,534:INFO:             xgboost: 2.1.3
2025-02-24 21:58:28,534:INFO:            catboost: 1.1.1
2025-02-24 21:58:28,534:INFO:              kmodes: 0.12.2
2025-02-24 21:58:28,534:INFO:             mlxtend: 0.23.3
2025-02-24 21:58:28,534:INFO:       statsforecast: 1.5.0
2025-02-24 21:58:28,534:INFO:        tune_sklearn: 0.5.0
2025-02-24 21:58:28,534:INFO:                 ray: 2.40.0
2025-02-24 21:58:28,534:INFO:            hyperopt: 0.2.7
2025-02-24 21:58:28,534:INFO:              optuna: 4.1.0
2025-02-24 21:58:28,534:INFO:               skopt: 0.10.2
2025-02-24 21:58:28,534:INFO:              mlflow: 2.19.0
2025-02-24 21:58:28,534:INFO:              gradio: 5.12.0
2025-02-24 21:58:28,534:INFO:             fastapi: 0.115.6
2025-02-24 21:58:28,534:INFO:             uvicorn: 0.34.0
2025-02-24 21:58:28,534:INFO:              m2cgen: 0.10.0
2025-02-24 21:58:28,534:INFO:           evidently: 0.4.40
2025-02-24 21:58:28,534:INFO:               fugue: 0.8.7
2025-02-24 21:58:28,534:INFO:           streamlit: Not installed
2025-02-24 21:58:28,534:INFO:             prophet: Not installed
2025-02-24 21:58:28,534:INFO:None
2025-02-24 21:58:28,534:INFO:Set up data.
2025-02-24 21:58:28,555:INFO:Set up folding strategy.
2025-02-24 21:58:28,555:INFO:Set up train/test split.
2025-02-24 21:58:28,566:INFO:Set up index.
2025-02-24 21:58:28,566:INFO:Assigning column types.
2025-02-24 21:58:28,570:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-02-24 21:58:28,570:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2025-02-24 21:58:28,573:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-24 21:58:28,575:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-24 21:58:28,608:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-24 21:58:28,632:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-24 21:58:28,633:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-24 21:58:28,634:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-24 21:58:28,635:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2025-02-24 21:58:28,637:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-24 21:58:28,639:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-24 21:58:28,673:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-24 21:58:28,697:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-24 21:58:28,698:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-24 21:58:28,699:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-24 21:58:28,699:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2025-02-24 21:58:28,702:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-24 21:58:28,704:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-24 21:58:28,737:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-24 21:58:28,761:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-24 21:58:28,761:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-24 21:58:28,763:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-24 21:58:28,765:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-24 21:58:28,768:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-24 21:58:28,800:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-24 21:58:28,824:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-24 21:58:28,824:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-24 21:58:28,826:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-24 21:58:28,826:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2025-02-24 21:58:28,831:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-24 21:58:28,865:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-24 21:58:28,888:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-24 21:58:28,888:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-24 21:58:28,889:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-24 21:58:28,894:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-24 21:58:28,927:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-24 21:58:28,950:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-24 21:58:28,950:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-24 21:58:28,952:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-24 21:58:28,952:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2025-02-24 21:58:28,989:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-24 21:58:29,013:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-24 21:58:29,013:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-24 21:58:29,014:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-24 21:58:29,052:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-24 21:58:29,075:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-24 21:58:29,075:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-24 21:58:29,077:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-24 21:58:29,077:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-02-24 21:58:29,114:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-24 21:58:29,137:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-24 21:58:29,138:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-24 21:58:29,177:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-24 21:58:29,199:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-24 21:58:29,201:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-24 21:58:29,201:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2025-02-24 21:58:29,261:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-24 21:58:29,262:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-24 21:58:29,324:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-24 21:58:29,325:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-24 21:58:29,326:INFO:Preparing preprocessing pipeline...
2025-02-24 21:58:29,326:INFO:Set up date feature engineering.
2025-02-24 21:58:29,326:INFO:Set up simple imputation.
2025-02-24 21:58:29,330:INFO:Set up encoding of categorical features.
2025-02-24 21:58:29,508:INFO:Finished creating preprocessing pipeline.
2025-02-24 21:58:29,514:INFO:Pipeline: Pipeline(memory=FastMemory(location=/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/joblib),
         steps=[('date_feature_extractor',
                 TransformerWrapper(include=['Date'],
                                    transformer=ExtractDateTimeFeatures())),
                ('numerical_imputer',
                 TransformerWrapper(include=['Rooms', 'Distance', 'Postcode',
                                             'Bedroom2', 'Bathroom', 'Car',
                                             'Landsize', 'BuildingArea',
                                             'YearBuilt', 'La...
                 TransformerWrapper(include=['Type', 'Method', 'Region'],
                                    transformer=OneHotEncoder(cols=['Type',
                                                                    'Method',
                                                                    'Region'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('rest_encoding',
                 TransformerWrapper(include=['Suburb', 'Address', 'Seller',
                                             'CouncilArea'],
                                    transformer=TargetEncoder(cols=['Suburb',
                                                                    'Address',
                                                                    'Seller',
                                                                    'CouncilArea'],
                                                              handle_missing='return_nan')))])
2025-02-24 21:58:29,514:INFO:Creating final display dataframe.
2025-02-24 21:58:29,950:INFO:Setup _display_container:                     Description             Value
0                    Session id              2398
1                        Target             Price
2                   Target type        Regression
3           Original data shape       (13580, 22)
4        Transformed data shape       (13580, 37)
5   Transformed train set shape        (9506, 37)
6    Transformed test set shape        (4074, 37)
7              Numeric features                13
8                 Date features                 1
9          Categorical features                 7
10     Rows with missing values             54.4%
11                   Preprocess              True
12              Imputation type            simple
13           Numeric imputation              mean
14       Categorical imputation              mode
15     Maximum one-hot encoding                25
16              Encoding method              None
17               Fold Generator             KFold
18                  Fold Number                10
19                     CPU Jobs                -1
20                      Use GPU             False
21               Log Experiment             False
22              Experiment Name  reg-default-name
23                          USI              3379
2025-02-24 21:58:30,015:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-24 21:58:30,017:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-24 21:58:30,078:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-24 21:58:30,079:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-24 21:58:30,080:INFO:setup() successfully completed in 1.55s...............
2025-02-24 22:03:43,901:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_5323/4239120124.py:4: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.boxplot(x='Propertycount_Bin', y='Price', data=df, palette="coolwarm")

2025-02-24 22:42:16,505:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_5323/47321870.py:2: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.boxplot(data=df, x="CouncilArea", y="Price", palette="coolwarm")

2025-02-24 22:42:28,887:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_5323/132642643.py:2: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.boxplot(data=df, x="CouncilArea", y="Price", palette="viridis")

2025-02-24 23:27:03,662:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_5323/822787353.py:3: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.boxplot(data=df, x="Bedroom2", y="Price", palette="coolwarm", ax=axes[0])

2025-02-24 23:27:03,746:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_5323/822787353.py:8: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.boxplot(data=df, x="Bathroom", y="Price", palette="coolwarm", ax=axes[1])

2025-02-24 23:27:03,812:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_5323/822787353.py:13: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.boxplot(data=df, x="Car", y="Price", palette="coolwarm", ax=axes[2])

2025-02-24 23:27:25,749:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_5323/2079386652.py:3: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.violinplot(data=df, x="Bedroom2", y="Price", palette="coolwarm", ax=axes[0])

2025-02-24 23:27:25,854:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_5323/2079386652.py:8: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.violinplot(data=df, x="Bathroom", y="Price", palette="coolwarm", ax=axes[1])

2025-02-24 23:27:25,999:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_5323/2079386652.py:13: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.violinplot(data=df, x="Car", y="Price", palette="coolwarm", ax=axes[2])

2025-02-24 23:27:50,924:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_5323/2157425422.py:3: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.violinplot(data=df, x="Bedroom2", y="Price", palette="viridis", ax=axes[0])

2025-02-24 23:27:51,015:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_5323/2157425422.py:8: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.violinplot(data=df, x="Bathroom", y="Price", palette="viridis", ax=axes[1])

2025-02-24 23:27:51,163:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_5323/2157425422.py:13: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.violinplot(data=df, x="Car", y="Price", palette="viridis", ax=axes[2])

2025-02-24 23:28:00,674:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_5323/763974249.py:3: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.violinplot(data=df, x="Bedroom2", y="Price", palette="viridis", ax=axes[0])

2025-02-24 23:28:00,776:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_5323/763974249.py:8: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.violinplot(data=df, x="Bathroom", y="Price", palette="viridis", ax=axes[1])

2025-02-24 23:28:00,933:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_5323/763974249.py:13: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.violinplot(data=df, x="Car", y="Price", palette="purples", ax=axes[2])

2025-02-24 23:28:04,227:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_5323/1485147438.py:3: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.violinplot(data=df, x="Bedroom2", y="Price", palette="viridis", ax=axes[0])

2025-02-24 23:28:04,330:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_5323/1485147438.py:8: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.violinplot(data=df, x="Bathroom", y="Price", palette="viridis", ax=axes[1])

2025-02-24 23:28:04,439:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_5323/1485147438.py:13: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.violinplot(data=df, x="Car", y="Price", palette="greens", ax=axes[2])

2025-02-24 23:28:06,909:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_5323/2014479748.py:3: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.violinplot(data=df, x="Bedroom2", y="Price", palette="viridis", ax=axes[0])

2025-02-24 23:28:06,995:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_5323/2014479748.py:8: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.violinplot(data=df, x="Bathroom", y="Price", palette="viridis", ax=axes[1])

2025-02-24 23:28:07,150:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_5323/2014479748.py:13: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.violinplot(data=df, x="Car", y="Price", palette="green", ax=axes[2])

2025-02-24 23:28:08,754:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_5323/2437038516.py:3: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.violinplot(data=df, x="Bedroom2", y="Price", palette="viridis", ax=axes[0])

2025-02-24 23:28:08,854:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_5323/2437038516.py:8: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.violinplot(data=df, x="Bathroom", y="Price", palette="viridis", ax=axes[1])

2025-02-24 23:28:08,987:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_5323/2437038516.py:13: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.violinplot(data=df, x="Car", y="Price", palette="reds", ax=axes[2])

2025-02-24 23:28:10,939:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_5323/2157425422.py:3: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.violinplot(data=df, x="Bedroom2", y="Price", palette="viridis", ax=axes[0])

2025-02-24 23:28:11,038:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_5323/2157425422.py:8: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.violinplot(data=df, x="Bathroom", y="Price", palette="viridis", ax=axes[1])

2025-02-24 23:28:11,215:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_5323/2157425422.py:13: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.violinplot(data=df, x="Car", y="Price", palette="viridis", ax=axes[2])

2025-02-24 23:29:05,203:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_5323/2934050365.py:2: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.boxplot(data=df, x="Method", y="Price", palette="coolwarm")

2025-02-24 23:29:15,149:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_5323/1746762345.py:2: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.boxplot(data=df, x="Method", y="Price", palette="coolwarm")

2025-02-24 23:29:34,158:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_5323/3936130528.py:4: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.barplot(x=avg_price_by_seller.index, y=avg_price_by_seller.values, palette="coolwarm")

2025-02-24 23:29:46,950:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_5323/142366555.py:4: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.barplot(x=avg_price_by_seller.index, y=avg_price_by_seller.values, palette="coolwarm")

2025-02-24 23:30:00,535:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_5323/2953034903.py:4: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.barplot(x=avg_price_by_seller.index, y=avg_price_by_seller.values, palette="coolwarm")

2025-02-24 23:30:19,914:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_5323/2838843215.py:2: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.boxplot(data=df, x="Method", y="Price", palette="viridis")

2025-02-24 23:30:21,257:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_5323/2297183518.py:4: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.barplot(x=avg_price_by_seller.index, y=avg_price_by_seller.values, palette="viridis")

2025-02-24 23:30:30,421:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_5323/3264285617.py:4: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.boxplot(x='Propertycount_Bin', y='Price', data=df, palette="viridis")

2025-02-25 23:25:38,377:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-25 23:25:38,378:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-25 23:25:38,378:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-25 23:25:38,378:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-25 23:25:39,788:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_22537/2838843215.py:2: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.boxplot(data=df, x="Method", y="Price", palette="viridis")

2025-02-25 23:25:39,914:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_22537/2297183518.py:4: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.barplot(x=avg_price_by_seller.index, y=avg_price_by_seller.values, palette="viridis")

2025-02-25 23:25:41,230:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_22537/1639960328.py:2: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.boxplot(

2025-02-25 23:25:41,799:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_22537/2157425422.py:3: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.violinplot(data=df, x="Bedroom2", y="Price", palette="viridis", ax=axes[0])

2025-02-25 23:25:41,908:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_22537/2157425422.py:8: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.violinplot(data=df, x="Bathroom", y="Price", palette="viridis", ax=axes[1])

2025-02-25 23:25:41,989:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_22537/2157425422.py:13: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.violinplot(data=df, x="Car", y="Price", palette="viridis", ax=axes[2])

2025-02-25 23:25:43,334:INFO:PyCaret RegressionExperiment
2025-02-25 23:25:43,334:INFO:Logging name: house_pricing
2025-02-25 23:25:43,334:INFO:ML Usecase: MLUsecase.REGRESSION
2025-02-25 23:25:43,334:INFO:version 3.3.2
2025-02-25 23:25:43,334:INFO:Initializing setup()
2025-02-25 23:25:43,334:INFO:self.USI: 0f5f
2025-02-25 23:25:43,334:INFO:self._variable_keys: {'target_param', 'fold_groups_param', 'log_plots_param', 'fold_shuffle_param', 'y', 'X_train', 'USI', 'y_train', '_available_plots', 'X_test', 'fold_generator', 'logging_param', 'n_jobs_param', 'data', 'idx', 'y_test', 'X', 'memory', '_ml_usecase', 'transform_target_param', 'gpu_n_jobs_param', 'exp_name_log', 'exp_id', 'pipeline', 'html_param', 'seed', 'gpu_param'}
2025-02-25 23:25:43,334:INFO:Checking environment
2025-02-25 23:25:43,334:INFO:python_version: 3.10.15
2025-02-25 23:25:43,334:INFO:python_build: ('main', 'Oct  3 2024 02:24:49')
2025-02-25 23:25:43,334:INFO:machine: arm64
2025-02-25 23:25:43,334:INFO:platform: macOS-14.1-arm64-arm-64bit
2025-02-25 23:25:43,334:INFO:Memory: svmem(total=17179869184, available=4688003072, percent=72.7, used=6581403648, free=50102272, active=4657135616, inactive=4572856320, wired=1924268032)
2025-02-25 23:25:43,335:INFO:Physical Core: 8
2025-02-25 23:25:43,335:INFO:Logical Core: 8
2025-02-25 23:25:43,335:INFO:Checking libraries
2025-02-25 23:25:43,335:INFO:System:
2025-02-25 23:25:43,335:INFO:    python: 3.10.15 (main, Oct  3 2024, 02:24:49) [Clang 14.0.6 ]
2025-02-25 23:25:43,335:INFO:executable: /Users/daaa/opt/miniconda3/envs/mlops/bin/python
2025-02-25 23:25:43,335:INFO:   machine: macOS-14.1-arm64-arm-64bit
2025-02-25 23:25:43,335:INFO:PyCaret required dependencies:
2025-02-25 23:25:43,959:INFO:                 pip: 24.2
2025-02-25 23:25:43,960:INFO:          setuptools: 75.1.0
2025-02-25 23:25:43,960:INFO:             pycaret: 3.3.2
2025-02-25 23:25:43,960:INFO:             IPython: 8.30.0
2025-02-25 23:25:43,960:INFO:          ipywidgets: 8.1.5
2025-02-25 23:25:43,960:INFO:                tqdm: 4.67.1
2025-02-25 23:25:43,960:INFO:               numpy: 1.26.4
2025-02-25 23:25:43,960:INFO:              pandas: 2.1.4
2025-02-25 23:25:43,960:INFO:              jinja2: 3.1.4
2025-02-25 23:25:43,960:INFO:               scipy: 1.11.4
2025-02-25 23:25:43,960:INFO:              joblib: 1.3.2
2025-02-25 23:25:43,960:INFO:             sklearn: 1.4.2
2025-02-25 23:25:43,960:INFO:                pyod: 2.0.2
2025-02-25 23:25:43,960:INFO:            imblearn: 0.12.4
2025-02-25 23:25:43,960:INFO:   category_encoders: 2.6.4
2025-02-25 23:25:43,960:INFO:            lightgbm: 4.5.0
2025-02-25 23:25:43,960:INFO:               numba: 0.60.0
2025-02-25 23:25:43,960:INFO:            requests: 2.32.3
2025-02-25 23:25:43,960:INFO:          matplotlib: 3.10.0
2025-02-25 23:25:43,960:INFO:          scikitplot: 0.3.7
2025-02-25 23:25:43,960:INFO:         yellowbrick: 1.5
2025-02-25 23:25:43,960:INFO:              plotly: 5.24.1
2025-02-25 23:25:43,960:INFO:    plotly-resampler: Not installed
2025-02-25 23:25:43,960:INFO:             kaleido: 0.2.1
2025-02-25 23:25:43,960:INFO:           schemdraw: 0.15
2025-02-25 23:25:43,960:INFO:         statsmodels: 0.14.4
2025-02-25 23:25:43,960:INFO:              sktime: 0.26.0
2025-02-25 23:25:43,960:INFO:               tbats: 1.1.3
2025-02-25 23:25:43,960:INFO:            pmdarima: 2.0.4
2025-02-25 23:25:43,960:INFO:              psutil: 6.1.0
2025-02-25 23:25:43,960:INFO:          markupsafe: 2.1.5
2025-02-25 23:25:43,960:INFO:             pickle5: Not installed
2025-02-25 23:25:43,960:INFO:         cloudpickle: 3.1.0
2025-02-25 23:25:43,960:INFO:         deprecation: 2.1.0
2025-02-25 23:25:43,960:INFO:              xxhash: 3.5.0
2025-02-25 23:25:43,960:INFO:           wurlitzer: 3.1.1
2025-02-25 23:25:43,960:INFO:PyCaret optional dependencies:
2025-02-25 23:25:45,705:INFO:                shap: 0.44.1
2025-02-25 23:25:45,705:INFO:           interpret: 0.6.9
2025-02-25 23:25:45,705:INFO:                umap: 0.5.7
2025-02-25 23:25:45,705:INFO:     ydata_profiling: 4.12.1
2025-02-25 23:25:45,705:INFO:  explainerdashboard: 0.4.8
2025-02-25 23:25:45,705:INFO:             autoviz: Not installed
2025-02-25 23:25:45,706:INFO:           fairlearn: 0.7.0
2025-02-25 23:25:45,706:INFO:          deepchecks: Not installed
2025-02-25 23:25:45,706:INFO:             xgboost: 2.1.3
2025-02-25 23:25:45,706:INFO:            catboost: 1.1.1
2025-02-25 23:25:45,706:INFO:              kmodes: 0.12.2
2025-02-25 23:25:45,706:INFO:             mlxtend: 0.23.3
2025-02-25 23:25:45,706:INFO:       statsforecast: 1.5.0
2025-02-25 23:25:45,706:INFO:        tune_sklearn: 0.5.0
2025-02-25 23:25:45,706:INFO:                 ray: 2.40.0
2025-02-25 23:25:45,706:INFO:            hyperopt: 0.2.7
2025-02-25 23:25:45,706:INFO:              optuna: 4.1.0
2025-02-25 23:25:45,706:INFO:               skopt: 0.10.2
2025-02-25 23:25:45,706:INFO:              mlflow: 2.16.0
2025-02-25 23:25:45,706:INFO:              gradio: 5.12.0
2025-02-25 23:25:45,706:INFO:             fastapi: 0.115.6
2025-02-25 23:25:45,706:INFO:             uvicorn: 0.34.0
2025-02-25 23:25:45,706:INFO:              m2cgen: 0.10.0
2025-02-25 23:25:45,706:INFO:           evidently: 0.4.40
2025-02-25 23:25:45,706:INFO:               fugue: 0.8.7
2025-02-25 23:25:45,706:INFO:           streamlit: Not installed
2025-02-25 23:25:45,706:INFO:             prophet: Not installed
2025-02-25 23:25:45,706:INFO:None
2025-02-25 23:25:45,706:INFO:Set up data.
2025-02-25 23:25:45,728:INFO:Set up folding strategy.
2025-02-25 23:25:45,728:INFO:Set up train/test split.
2025-02-25 23:25:45,738:INFO:Set up index.
2025-02-25 23:25:45,738:INFO:Assigning column types.
2025-02-25 23:25:45,742:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-02-25 23:25:45,742:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2025-02-25 23:25:45,745:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-25 23:25:45,747:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-25 23:25:45,781:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:25:45,804:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-25 23:25:45,804:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:25:45,806:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:25:46,731:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2025-02-25 23:25:46,734:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-25 23:25:46,737:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-25 23:25:46,772:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:25:46,796:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-25 23:25:46,796:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:25:46,798:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:25:46,798:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2025-02-25 23:25:46,801:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-25 23:25:46,803:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-25 23:25:46,835:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:25:46,859:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-25 23:25:46,859:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:25:46,860:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:25:46,863:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-25 23:25:46,866:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-25 23:25:46,900:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:25:46,924:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-25 23:25:46,924:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:25:46,925:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:25:46,926:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2025-02-25 23:25:46,930:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-25 23:25:46,963:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:25:46,987:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-25 23:25:46,987:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:25:46,988:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:25:46,993:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-25 23:25:47,026:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:25:47,050:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-25 23:25:47,050:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:25:47,051:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:25:47,052:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2025-02-25 23:25:47,089:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:25:47,113:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-25 23:25:47,114:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:25:47,115:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:25:47,153:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:25:47,176:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-25 23:25:47,176:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:25:47,177:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:25:47,178:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-02-25 23:25:47,215:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:25:47,239:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:25:47,241:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:25:47,278:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:25:47,302:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:25:47,303:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:25:47,304:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2025-02-25 23:25:47,365:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:25:47,367:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:25:47,428:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:25:47,430:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:25:47,431:INFO:Preparing preprocessing pipeline...
2025-02-25 23:25:47,431:INFO:Set up date feature engineering.
2025-02-25 23:25:47,431:INFO:Set up iterative imputation.
2025-02-25 23:25:47,493:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:25:47,495:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:25:47,521:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-25 23:25:47,540:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:25:47,544:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:25:47,563:INFO:Set up encoding of ordinal features.
2025-02-25 23:25:47,567:WARNING:The number of classes passed to feature Method in the ordinal_features parameter (11) don't match with the number of classes in the data (5).
2025-02-25 23:25:47,567:INFO:Set up encoding of categorical features.
2025-02-25 23:25:47,567:INFO:Set up polynomial features.
2025-02-25 23:25:47,567:INFO:Set up removing multicollinearity.
2025-02-25 23:25:47,567:INFO:Set up feature normalization.
2025-02-25 23:25:47,567:INFO:Set up feature selection.
2025-02-25 23:25:47,627:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:25:47,628:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:25:48,086:INFO:Finished creating preprocessing pipeline.
2025-02-25 23:25:48,132:INFO:Pipeline: Pipeline(memory=FastMemory(location=/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/joblib),
         steps=[('date_feature_extractor',
                 TransformerWrapper(include=['Date'],
                                    transformer=ExtractDateTimeFeatures())),
                ('iterative_imputer',
                 TransformerWrapper(transformer=IterativeImputer(cat_estimator=LGBMClassifier(n_jobs=-1,
                                                                                              random_state=123),
                                                                 cat_estimator_prepare_...
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('remove_multicollinearity',
                 TransformerWrapper(exclude=[],
                                    transformer=RemoveMulticollinearity(threshold=0.55))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('feature_selection',
                 TransformerWrapper(exclude=[],
                                    transformer=SelectFromModel(estimator=LGBMRegressor(),
                                                                max_features=12,
                                                                threshold=-inf)))])
2025-02-25 23:25:48,132:INFO:Creating final display dataframe.
2025-02-25 23:29:56,373:INFO:PyCaret RegressionExperiment
2025-02-25 23:29:56,374:INFO:Logging name: house_pricing
2025-02-25 23:29:56,374:INFO:ML Usecase: MLUsecase.REGRESSION
2025-02-25 23:29:56,374:INFO:version 3.3.2
2025-02-25 23:29:56,374:INFO:Initializing setup()
2025-02-25 23:29:56,374:INFO:self.USI: 8bbd
2025-02-25 23:29:56,374:INFO:self._variable_keys: {'target_param', 'fold_groups_param', 'log_plots_param', 'fold_shuffle_param', 'y', 'X_train', 'USI', 'y_train', '_available_plots', 'X_test', 'fold_generator', 'logging_param', 'n_jobs_param', 'data', 'idx', 'y_test', 'X', 'memory', '_ml_usecase', 'transform_target_param', 'gpu_n_jobs_param', 'exp_name_log', 'exp_id', 'pipeline', 'html_param', 'seed', 'gpu_param'}
2025-02-25 23:29:56,374:INFO:Checking environment
2025-02-25 23:29:56,374:INFO:python_version: 3.10.15
2025-02-25 23:29:56,374:INFO:python_build: ('main', 'Oct  3 2024 02:24:49')
2025-02-25 23:29:56,374:INFO:machine: arm64
2025-02-25 23:29:56,374:INFO:platform: macOS-14.1-arm64-arm-64bit
2025-02-25 23:29:56,374:INFO:Memory: svmem(total=17179869184, available=4656742400, percent=72.9, used=6639157248, free=44122112, active=4631625728, inactive=4606541824, wired=2007531520)
2025-02-25 23:29:56,374:INFO:Physical Core: 8
2025-02-25 23:29:56,374:INFO:Logical Core: 8
2025-02-25 23:29:56,375:INFO:Checking libraries
2025-02-25 23:29:56,375:INFO:System:
2025-02-25 23:29:56,375:INFO:    python: 3.10.15 (main, Oct  3 2024, 02:24:49) [Clang 14.0.6 ]
2025-02-25 23:29:56,375:INFO:executable: /Users/daaa/opt/miniconda3/envs/mlops/bin/python
2025-02-25 23:29:56,375:INFO:   machine: macOS-14.1-arm64-arm-64bit
2025-02-25 23:29:56,375:INFO:PyCaret required dependencies:
2025-02-25 23:29:56,375:INFO:                 pip: 24.2
2025-02-25 23:29:56,375:INFO:          setuptools: 75.1.0
2025-02-25 23:29:56,375:INFO:             pycaret: 3.3.2
2025-02-25 23:29:56,375:INFO:             IPython: 8.30.0
2025-02-25 23:29:56,375:INFO:          ipywidgets: 8.1.5
2025-02-25 23:29:56,375:INFO:                tqdm: 4.67.1
2025-02-25 23:29:56,375:INFO:               numpy: 1.26.4
2025-02-25 23:29:56,375:INFO:              pandas: 2.1.4
2025-02-25 23:29:56,375:INFO:              jinja2: 3.1.4
2025-02-25 23:29:56,375:INFO:               scipy: 1.11.4
2025-02-25 23:29:56,375:INFO:              joblib: 1.3.2
2025-02-25 23:29:56,375:INFO:             sklearn: 1.4.2
2025-02-25 23:29:56,375:INFO:                pyod: 2.0.2
2025-02-25 23:29:56,375:INFO:            imblearn: 0.12.4
2025-02-25 23:29:56,375:INFO:   category_encoders: 2.6.4
2025-02-25 23:29:56,375:INFO:            lightgbm: 4.5.0
2025-02-25 23:29:56,375:INFO:               numba: 0.60.0
2025-02-25 23:29:56,375:INFO:            requests: 2.32.3
2025-02-25 23:29:56,375:INFO:          matplotlib: 3.10.0
2025-02-25 23:29:56,375:INFO:          scikitplot: 0.3.7
2025-02-25 23:29:56,375:INFO:         yellowbrick: 1.5
2025-02-25 23:29:56,375:INFO:              plotly: 5.24.1
2025-02-25 23:29:56,375:INFO:    plotly-resampler: Not installed
2025-02-25 23:29:56,375:INFO:             kaleido: 0.2.1
2025-02-25 23:29:56,375:INFO:           schemdraw: 0.15
2025-02-25 23:29:56,375:INFO:         statsmodels: 0.14.4
2025-02-25 23:29:56,375:INFO:              sktime: 0.26.0
2025-02-25 23:29:56,375:INFO:               tbats: 1.1.3
2025-02-25 23:29:56,375:INFO:            pmdarima: 2.0.4
2025-02-25 23:29:56,375:INFO:              psutil: 6.1.0
2025-02-25 23:29:56,375:INFO:          markupsafe: 2.1.5
2025-02-25 23:29:56,375:INFO:             pickle5: Not installed
2025-02-25 23:29:56,375:INFO:         cloudpickle: 3.1.0
2025-02-25 23:29:56,375:INFO:         deprecation: 2.1.0
2025-02-25 23:29:56,375:INFO:              xxhash: 3.5.0
2025-02-25 23:29:56,375:INFO:           wurlitzer: 3.1.1
2025-02-25 23:29:56,375:INFO:PyCaret optional dependencies:
2025-02-25 23:29:56,376:INFO:                shap: 0.44.1
2025-02-25 23:29:56,376:INFO:           interpret: 0.6.9
2025-02-25 23:29:56,376:INFO:                umap: 0.5.7
2025-02-25 23:29:56,376:INFO:     ydata_profiling: 4.12.1
2025-02-25 23:29:56,376:INFO:  explainerdashboard: 0.4.8
2025-02-25 23:29:56,376:INFO:             autoviz: Not installed
2025-02-25 23:29:56,376:INFO:           fairlearn: 0.7.0
2025-02-25 23:29:56,376:INFO:          deepchecks: Not installed
2025-02-25 23:29:56,376:INFO:             xgboost: 2.1.3
2025-02-25 23:29:56,376:INFO:            catboost: 1.1.1
2025-02-25 23:29:56,376:INFO:              kmodes: 0.12.2
2025-02-25 23:29:56,376:INFO:             mlxtend: 0.23.3
2025-02-25 23:29:56,376:INFO:       statsforecast: 1.5.0
2025-02-25 23:29:56,376:INFO:        tune_sklearn: 0.5.0
2025-02-25 23:29:56,376:INFO:                 ray: 2.40.0
2025-02-25 23:29:56,376:INFO:            hyperopt: 0.2.7
2025-02-25 23:29:56,376:INFO:              optuna: 4.1.0
2025-02-25 23:29:56,376:INFO:               skopt: 0.10.2
2025-02-25 23:29:56,376:INFO:              mlflow: 2.16.0
2025-02-25 23:29:56,376:INFO:              gradio: 5.12.0
2025-02-25 23:29:56,376:INFO:             fastapi: 0.115.6
2025-02-25 23:29:56,376:INFO:             uvicorn: 0.34.0
2025-02-25 23:29:56,376:INFO:              m2cgen: 0.10.0
2025-02-25 23:29:56,376:INFO:           evidently: 0.4.40
2025-02-25 23:29:56,376:INFO:               fugue: 0.8.7
2025-02-25 23:29:56,376:INFO:           streamlit: Not installed
2025-02-25 23:29:56,376:INFO:             prophet: Not installed
2025-02-25 23:29:56,376:INFO:None
2025-02-25 23:29:56,376:INFO:Set up data.
2025-02-25 23:29:56,463:INFO:Set up folding strategy.
2025-02-25 23:29:56,463:INFO:Set up train/test split.
2025-02-25 23:29:56,471:INFO:Set up index.
2025-02-25 23:29:56,472:INFO:Assigning column types.
2025-02-25 23:29:56,476:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-02-25 23:29:56,476:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2025-02-25 23:29:56,479:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-25 23:29:56,481:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-25 23:29:56,514:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:29:56,538:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-25 23:29:56,538:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:29:56,540:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:29:56,540:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2025-02-25 23:29:56,543:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-25 23:29:56,545:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-25 23:29:56,578:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:29:56,602:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-25 23:29:56,602:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:29:56,603:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:29:56,604:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2025-02-25 23:29:56,606:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-25 23:29:56,609:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-25 23:29:56,641:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:29:56,665:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-25 23:29:56,665:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:29:56,667:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:29:56,669:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-25 23:29:56,672:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-25 23:29:56,705:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:29:56,728:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-25 23:29:56,729:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:29:56,730:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:29:56,730:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2025-02-25 23:29:56,735:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-25 23:29:56,768:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:29:56,792:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-25 23:29:56,792:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:29:56,794:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:29:56,799:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-25 23:29:56,832:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:29:56,855:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-25 23:29:56,856:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:29:56,857:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:29:56,857:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2025-02-25 23:29:56,895:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:29:56,918:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-25 23:29:56,918:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:29:56,920:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:29:56,958:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:29:56,982:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-25 23:29:56,982:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:29:56,983:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:29:56,984:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-02-25 23:29:57,022:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:29:57,046:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:29:57,047:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:29:57,085:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:29:57,108:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:29:57,110:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:29:57,110:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2025-02-25 23:29:57,170:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:29:57,172:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:29:57,236:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:29:57,237:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:29:57,239:INFO:Preparing preprocessing pipeline...
2025-02-25 23:29:57,239:INFO:Set up date feature engineering.
2025-02-25 23:29:57,239:INFO:Set up iterative imputation.
2025-02-25 23:29:57,300:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:29:57,302:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:29:57,327:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-25 23:29:57,342:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:29:57,344:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:29:57,362:INFO:Set up encoding of ordinal features.
2025-02-25 23:29:57,365:WARNING:The number of classes passed to feature Method in the ordinal_features parameter (11) don't match with the number of classes in the data (5).
2025-02-25 23:29:57,366:INFO:Set up encoding of categorical features.
2025-02-25 23:29:57,366:INFO:Set up polynomial features.
2025-02-25 23:29:57,366:INFO:Set up removing multicollinearity.
2025-02-25 23:29:57,366:INFO:Set up feature normalization.
2025-02-25 23:29:58,000:INFO:Finished creating preprocessing pipeline.
2025-02-25 23:29:58,043:INFO:Pipeline: Pipeline(memory=FastMemory(location=/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/joblib),
         steps=[('date_feature_extractor',
                 TransformerWrapper(include=['Date'],
                                    transformer=ExtractDateTimeFeatures())),
                ('iterative_imputer',
                 TransformerWrapper(transformer=IterativeImputer(cat_estimator=LGBMClassifier(n_jobs=-1,
                                                                                              random_state=123),
                                                                 cat_estimator_prepare_...
                                    transformer=TargetEncoder(cols=['Suburb',
                                                                    'Seller',
                                                                    'CouncilArea'],
                                                              handle_missing='return_nan'))),
                ('polynomial_features',
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('remove_multicollinearity',
                 TransformerWrapper(exclude=[],
                                    transformer=RemoveMulticollinearity(threshold=0.55))),
                ('normalize',
                 TransformerWrapper(transformer=StandardScaler()))])
2025-02-25 23:29:58,043:INFO:Creating final display dataframe.
2025-02-25 23:29:59,311:INFO:Setup _display_container:                         Description          Value
0                        Session id            123
1                            Target          Price
2                       Target type     Regression
3               Original data shape    (13580, 21)
4            Transformed data shape   (13580, 134)
5       Transformed train set shape    (9506, 134)
6        Transformed test set shape    (4074, 134)
7                   Ignore features              4
8                  Ordinal features              3
9                  Numeric features              8
10                    Date features              1
11             Categorical features              6
12         Rows with missing values          54.4%
13                       Preprocess           True
14                  Imputation type      iterative
15  Iterative imputation iterations              5
16        Numeric iterative imputer       lightgbm
17    Categorical iterative imputer       lightgbm
18         Maximum one-hot encoding             25
19                  Encoding method           None
20              Polynomial features           True
21                Polynomial degree              2
22         Remove multicollinearity           True
23      Multicollinearity threshold           0.55
24                        Normalize           True
25                 Normalize method         zscore
26                   Fold Generator          KFold
27                      Fold Number             10
28                         CPU Jobs             -1
29                          Use GPU          False
30                   Log Experiment   MlflowLogger
31                  Experiment Name  house_pricing
32                              USI           8bbd
2025-02-25 23:29:59,380:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:29:59,381:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:29:59,444:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:29:59,445:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:29:59,446:INFO:Logging experiment in loggers
2025-02-25 23:29:59,572:INFO:SubProcess save_model() called ==================================
2025-02-25 23:29:59,657:INFO:Initializing save_model()
2025-02-25 23:29:59,657:INFO:save_model(model=Pipeline(memory=FastMemory(location=/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/joblib),
         steps=[('date_feature_extractor',
                 TransformerWrapper(include=['Date'],
                                    transformer=ExtractDateTimeFeatures())),
                ('iterative_imputer',
                 TransformerWrapper(transformer=IterativeImputer(cat_estimator=LGBMClassifier(n_jobs=-1,
                                                                                              random_state=123),
                                                                 cat_estimator_prepare_...
                                    transformer=TargetEncoder(cols=['Suburb',
                                                                    'Seller',
                                                                    'CouncilArea'],
                                                              handle_missing='return_nan'))),
                ('polynomial_features',
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('remove_multicollinearity',
                 TransformerWrapper(exclude=[],
                                    transformer=RemoveMulticollinearity(threshold=0.55))),
                ('normalize',
                 TransformerWrapper(transformer=StandardScaler()))]), model_name=/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/tmpgoe0b9pe/Transformation Pipeline, prep_pipe_=Pipeline(memory=FastMemory(location=/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/joblib),
         steps=[('date_feature_extractor',
                 TransformerWrapper(include=['Date'],
                                    transformer=ExtractDateTimeFeatures())),
                ('iterative_imputer',
                 TransformerWrapper(transformer=IterativeImputer(cat_estimator=LGBMClassifier(n_jobs=-1,
                                                                                              random_state=123),
                                                                 cat_estimator_prepare_...
                                    transformer=TargetEncoder(cols=['Suburb',
                                                                    'Seller',
                                                                    'CouncilArea'],
                                                              handle_missing='return_nan'))),
                ('polynomial_features',
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('remove_multicollinearity',
                 TransformerWrapper(exclude=[],
                                    transformer=RemoveMulticollinearity(threshold=0.55))),
                ('normalize',
                 TransformerWrapper(transformer=StandardScaler()))]), verbose=False, use_case=MLUsecase.REGRESSION, kwargs={})
2025-02-25 23:29:59,657:INFO:Adding model into prep_pipe
2025-02-25 23:29:59,657:WARNING:Only Model saved as it was a pipeline.
2025-02-25 23:29:59,744:INFO:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/tmpgoe0b9pe/Transformation Pipeline.pkl saved in current working directory
2025-02-25 23:29:59,784:INFO:Pipeline(memory=FastMemory(location=/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/joblib),
         steps=[('date_feature_extractor',
                 TransformerWrapper(include=['Date'],
                                    transformer=ExtractDateTimeFeatures())),
                ('iterative_imputer',
                 TransformerWrapper(transformer=IterativeImputer(cat_estimator=LGBMClassifier(n_jobs=-1,
                                                                                              random_state=123),
                                                                 cat_estimator_prepare_...
                                    transformer=TargetEncoder(cols=['Suburb',
                                                                    'Seller',
                                                                    'CouncilArea'],
                                                              handle_missing='return_nan'))),
                ('polynomial_features',
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('remove_multicollinearity',
                 TransformerWrapper(exclude=[],
                                    transformer=RemoveMulticollinearity(threshold=0.55))),
                ('normalize',
                 TransformerWrapper(transformer=StandardScaler()))])
2025-02-25 23:29:59,785:INFO:save_model() successfully completed......................................
2025-02-25 23:29:59,920:INFO:SubProcess save_model() end ==================================
2025-02-25 23:29:59,925:INFO:setup() successfully completed in 3.08s...............
2025-02-25 23:30:11,137:INFO:PyCaret RegressionExperiment
2025-02-25 23:30:11,137:INFO:Logging name: house_pricing
2025-02-25 23:30:11,137:INFO:ML Usecase: MLUsecase.REGRESSION
2025-02-25 23:30:11,137:INFO:version 3.3.2
2025-02-25 23:30:11,137:INFO:Initializing setup()
2025-02-25 23:30:11,137:INFO:self.USI: 3cbe
2025-02-25 23:30:11,137:INFO:self._variable_keys: {'target_param', 'fold_groups_param', 'log_plots_param', 'fold_shuffle_param', 'y', 'X_train', 'USI', 'y_train', '_available_plots', 'X_test', 'fold_generator', 'logging_param', 'n_jobs_param', 'data', 'idx', 'y_test', 'X', 'memory', '_ml_usecase', 'transform_target_param', 'gpu_n_jobs_param', 'exp_name_log', 'exp_id', 'pipeline', 'html_param', 'seed', 'gpu_param'}
2025-02-25 23:30:11,137:INFO:Checking environment
2025-02-25 23:30:11,137:INFO:python_version: 3.10.15
2025-02-25 23:30:11,137:INFO:python_build: ('main', 'Oct  3 2024 02:24:49')
2025-02-25 23:30:11,137:INFO:machine: arm64
2025-02-25 23:30:11,137:INFO:platform: macOS-14.1-arm64-arm-64bit
2025-02-25 23:30:11,137:INFO:Memory: svmem(total=17179869184, available=4689199104, percent=72.7, used=6354698240, free=322945024, active=4426366976, inactive=4350820352, wired=1928331264)
2025-02-25 23:30:11,137:INFO:Physical Core: 8
2025-02-25 23:30:11,137:INFO:Logical Core: 8
2025-02-25 23:30:11,137:INFO:Checking libraries
2025-02-25 23:30:11,137:INFO:System:
2025-02-25 23:30:11,137:INFO:    python: 3.10.15 (main, Oct  3 2024, 02:24:49) [Clang 14.0.6 ]
2025-02-25 23:30:11,137:INFO:executable: /Users/daaa/opt/miniconda3/envs/mlops/bin/python
2025-02-25 23:30:11,137:INFO:   machine: macOS-14.1-arm64-arm-64bit
2025-02-25 23:30:11,137:INFO:PyCaret required dependencies:
2025-02-25 23:30:11,137:INFO:                 pip: 24.2
2025-02-25 23:30:11,137:INFO:          setuptools: 75.1.0
2025-02-25 23:30:11,137:INFO:             pycaret: 3.3.2
2025-02-25 23:30:11,137:INFO:             IPython: 8.30.0
2025-02-25 23:30:11,137:INFO:          ipywidgets: 8.1.5
2025-02-25 23:30:11,137:INFO:                tqdm: 4.67.1
2025-02-25 23:30:11,137:INFO:               numpy: 1.26.4
2025-02-25 23:30:11,137:INFO:              pandas: 2.1.4
2025-02-25 23:30:11,137:INFO:              jinja2: 3.1.4
2025-02-25 23:30:11,137:INFO:               scipy: 1.11.4
2025-02-25 23:30:11,137:INFO:              joblib: 1.3.2
2025-02-25 23:30:11,137:INFO:             sklearn: 1.4.2
2025-02-25 23:30:11,137:INFO:                pyod: 2.0.2
2025-02-25 23:30:11,137:INFO:            imblearn: 0.12.4
2025-02-25 23:30:11,137:INFO:   category_encoders: 2.6.4
2025-02-25 23:30:11,137:INFO:            lightgbm: 4.5.0
2025-02-25 23:30:11,137:INFO:               numba: 0.60.0
2025-02-25 23:30:11,137:INFO:            requests: 2.32.3
2025-02-25 23:30:11,137:INFO:          matplotlib: 3.10.0
2025-02-25 23:30:11,137:INFO:          scikitplot: 0.3.7
2025-02-25 23:30:11,138:INFO:         yellowbrick: 1.5
2025-02-25 23:30:11,138:INFO:              plotly: 5.24.1
2025-02-25 23:30:11,138:INFO:    plotly-resampler: Not installed
2025-02-25 23:30:11,138:INFO:             kaleido: 0.2.1
2025-02-25 23:30:11,138:INFO:           schemdraw: 0.15
2025-02-25 23:30:11,138:INFO:         statsmodels: 0.14.4
2025-02-25 23:30:11,138:INFO:              sktime: 0.26.0
2025-02-25 23:30:11,138:INFO:               tbats: 1.1.3
2025-02-25 23:30:11,138:INFO:            pmdarima: 2.0.4
2025-02-25 23:30:11,138:INFO:              psutil: 6.1.0
2025-02-25 23:30:11,138:INFO:          markupsafe: 2.1.5
2025-02-25 23:30:11,138:INFO:             pickle5: Not installed
2025-02-25 23:30:11,138:INFO:         cloudpickle: 3.1.0
2025-02-25 23:30:11,138:INFO:         deprecation: 2.1.0
2025-02-25 23:30:11,138:INFO:              xxhash: 3.5.0
2025-02-25 23:30:11,138:INFO:           wurlitzer: 3.1.1
2025-02-25 23:30:11,138:INFO:PyCaret optional dependencies:
2025-02-25 23:30:11,138:INFO:                shap: 0.44.1
2025-02-25 23:30:11,138:INFO:           interpret: 0.6.9
2025-02-25 23:30:11,138:INFO:                umap: 0.5.7
2025-02-25 23:30:11,138:INFO:     ydata_profiling: 4.12.1
2025-02-25 23:30:11,138:INFO:  explainerdashboard: 0.4.8
2025-02-25 23:30:11,138:INFO:             autoviz: Not installed
2025-02-25 23:30:11,138:INFO:           fairlearn: 0.7.0
2025-02-25 23:30:11,138:INFO:          deepchecks: Not installed
2025-02-25 23:30:11,139:INFO:             xgboost: 2.1.3
2025-02-25 23:30:11,139:INFO:            catboost: 1.1.1
2025-02-25 23:30:11,139:INFO:              kmodes: 0.12.2
2025-02-25 23:30:11,139:INFO:             mlxtend: 0.23.3
2025-02-25 23:30:11,139:INFO:       statsforecast: 1.5.0
2025-02-25 23:30:11,139:INFO:        tune_sklearn: 0.5.0
2025-02-25 23:30:11,139:INFO:                 ray: 2.40.0
2025-02-25 23:30:11,139:INFO:            hyperopt: 0.2.7
2025-02-25 23:30:11,139:INFO:              optuna: 4.1.0
2025-02-25 23:30:11,139:INFO:               skopt: 0.10.2
2025-02-25 23:30:11,139:INFO:              mlflow: 2.16.0
2025-02-25 23:30:11,140:INFO:              gradio: 5.12.0
2025-02-25 23:30:11,140:INFO:             fastapi: 0.115.6
2025-02-25 23:30:11,140:INFO:             uvicorn: 0.34.0
2025-02-25 23:30:11,140:INFO:              m2cgen: 0.10.0
2025-02-25 23:30:11,140:INFO:           evidently: 0.4.40
2025-02-25 23:30:11,140:INFO:               fugue: 0.8.7
2025-02-25 23:30:11,140:INFO:           streamlit: Not installed
2025-02-25 23:30:11,140:INFO:             prophet: Not installed
2025-02-25 23:30:11,140:INFO:None
2025-02-25 23:30:11,140:INFO:Set up data.
2025-02-25 23:30:11,229:INFO:Set up folding strategy.
2025-02-25 23:30:11,229:INFO:Set up train/test split.
2025-02-25 23:30:11,237:INFO:Set up index.
2025-02-25 23:30:11,237:INFO:Assigning column types.
2025-02-25 23:30:11,241:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-02-25 23:30:11,241:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2025-02-25 23:30:11,244:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-25 23:30:11,246:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-25 23:30:11,279:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:30:11,303:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-25 23:30:11,304:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:30:11,305:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:30:11,306:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2025-02-25 23:30:11,308:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-25 23:30:11,311:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-25 23:30:11,344:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:30:11,368:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-25 23:30:11,368:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:30:11,369:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:30:11,370:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2025-02-25 23:30:11,372:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-25 23:30:11,375:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-25 23:30:11,408:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:30:11,433:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-25 23:30:11,433:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:30:11,434:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:30:11,437:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-25 23:30:11,439:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-25 23:30:11,472:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:30:11,496:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-25 23:30:11,496:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:30:11,497:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:30:11,498:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2025-02-25 23:30:11,503:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-25 23:30:11,536:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:30:11,559:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-25 23:30:11,560:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:30:11,561:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:30:11,566:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-25 23:30:11,599:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:30:11,623:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-25 23:30:11,624:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:30:11,625:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:30:11,625:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2025-02-25 23:30:11,664:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:30:11,688:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-25 23:30:11,688:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:30:11,689:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:30:11,728:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:30:11,752:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-25 23:30:11,752:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:30:11,753:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:30:11,754:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-02-25 23:30:11,792:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:30:11,817:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:30:11,818:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:30:11,856:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:30:11,881:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:30:11,882:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:30:11,882:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2025-02-25 23:30:11,947:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:30:11,949:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:30:12,011:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:30:12,013:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:30:12,014:INFO:Preparing preprocessing pipeline...
2025-02-25 23:30:12,014:INFO:Set up date feature engineering.
2025-02-25 23:30:12,014:INFO:Set up iterative imputation.
2025-02-25 23:30:12,076:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:30:12,078:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:30:12,102:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-25 23:30:12,117:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:30:12,119:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:30:12,136:INFO:Set up encoding of ordinal features.
2025-02-25 23:30:12,140:WARNING:The number of classes passed to feature Method in the ordinal_features parameter (11) don't match with the number of classes in the data (5).
2025-02-25 23:30:12,141:INFO:Set up encoding of categorical features.
2025-02-25 23:30:12,141:INFO:Set up polynomial features.
2025-02-25 23:30:12,141:INFO:Set up removing multicollinearity.
2025-02-25 23:30:12,141:INFO:Set up feature normalization.
2025-02-25 23:30:12,567:INFO:Finished creating preprocessing pipeline.
2025-02-25 23:30:12,610:INFO:Pipeline: Pipeline(memory=FastMemory(location=/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/joblib),
         steps=[('date_feature_extractor',
                 TransformerWrapper(include=['Date'],
                                    transformer=ExtractDateTimeFeatures())),
                ('iterative_imputer',
                 TransformerWrapper(transformer=IterativeImputer(cat_estimator=LGBMClassifier(n_jobs=-1,
                                                                                              random_state=123),
                                                                 cat_estimator_prepare_...
                                    transformer=TargetEncoder(cols=['Suburb',
                                                                    'Seller',
                                                                    'CouncilArea'],
                                                              handle_missing='return_nan'))),
                ('polynomial_features',
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('remove_multicollinearity',
                 TransformerWrapper(exclude=[],
                                    transformer=RemoveMulticollinearity(threshold=0.55))),
                ('normalize',
                 TransformerWrapper(transformer=StandardScaler()))])
2025-02-25 23:30:12,610:INFO:Creating final display dataframe.
2025-02-25 23:30:13,084:INFO:Setup _display_container:                         Description          Value
0                        Session id            123
1                            Target          Price
2                       Target type     Regression
3               Original data shape    (13580, 21)
4            Transformed data shape   (13580, 134)
5       Transformed train set shape    (9506, 134)
6        Transformed test set shape    (4074, 134)
7                   Ignore features              4
8                  Ordinal features              3
9                  Numeric features              8
10                    Date features              1
11             Categorical features              6
12         Rows with missing values          54.4%
13                       Preprocess           True
14                  Imputation type      iterative
15  Iterative imputation iterations              5
16        Numeric iterative imputer       lightgbm
17    Categorical iterative imputer       lightgbm
18         Maximum one-hot encoding             25
19                  Encoding method           None
20              Polynomial features           True
21                Polynomial degree              2
22         Remove multicollinearity           True
23      Multicollinearity threshold           0.55
24                        Normalize           True
25                 Normalize method         zscore
26                   Fold Generator          KFold
27                      Fold Number             10
28                         CPU Jobs             -1
29                          Use GPU          False
30                   Log Experiment   MlflowLogger
31                  Experiment Name  house_pricing
32                              USI           3cbe
2025-02-25 23:30:13,151:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:30:13,152:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:30:13,213:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:30:13,215:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:30:13,215:INFO:Logging experiment in loggers
2025-02-25 23:30:13,233:INFO:SubProcess save_model() called ==================================
2025-02-25 23:30:13,315:INFO:Initializing save_model()
2025-02-25 23:30:13,315:INFO:save_model(model=Pipeline(memory=FastMemory(location=/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/joblib),
         steps=[('date_feature_extractor',
                 TransformerWrapper(include=['Date'],
                                    transformer=ExtractDateTimeFeatures())),
                ('iterative_imputer',
                 TransformerWrapper(transformer=IterativeImputer(cat_estimator=LGBMClassifier(n_jobs=-1,
                                                                                              random_state=123),
                                                                 cat_estimator_prepare_...
                                    transformer=TargetEncoder(cols=['Suburb',
                                                                    'Seller',
                                                                    'CouncilArea'],
                                                              handle_missing='return_nan'))),
                ('polynomial_features',
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('remove_multicollinearity',
                 TransformerWrapper(exclude=[],
                                    transformer=RemoveMulticollinearity(threshold=0.55))),
                ('normalize',
                 TransformerWrapper(transformer=StandardScaler()))]), model_name=/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/tmpm_jqf4ty/Transformation Pipeline, prep_pipe_=Pipeline(memory=FastMemory(location=/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/joblib),
         steps=[('date_feature_extractor',
                 TransformerWrapper(include=['Date'],
                                    transformer=ExtractDateTimeFeatures())),
                ('iterative_imputer',
                 TransformerWrapper(transformer=IterativeImputer(cat_estimator=LGBMClassifier(n_jobs=-1,
                                                                                              random_state=123),
                                                                 cat_estimator_prepare_...
                                    transformer=TargetEncoder(cols=['Suburb',
                                                                    'Seller',
                                                                    'CouncilArea'],
                                                              handle_missing='return_nan'))),
                ('polynomial_features',
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('remove_multicollinearity',
                 TransformerWrapper(exclude=[],
                                    transformer=RemoveMulticollinearity(threshold=0.55))),
                ('normalize',
                 TransformerWrapper(transformer=StandardScaler()))]), verbose=False, use_case=MLUsecase.REGRESSION, kwargs={})
2025-02-25 23:30:13,315:INFO:Adding model into prep_pipe
2025-02-25 23:30:13,315:WARNING:Only Model saved as it was a pipeline.
2025-02-25 23:30:13,396:INFO:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/tmpm_jqf4ty/Transformation Pipeline.pkl saved in current working directory
2025-02-25 23:30:13,438:INFO:Pipeline(memory=FastMemory(location=/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/joblib),
         steps=[('date_feature_extractor',
                 TransformerWrapper(include=['Date'],
                                    transformer=ExtractDateTimeFeatures())),
                ('iterative_imputer',
                 TransformerWrapper(transformer=IterativeImputer(cat_estimator=LGBMClassifier(n_jobs=-1,
                                                                                              random_state=123),
                                                                 cat_estimator_prepare_...
                                    transformer=TargetEncoder(cols=['Suburb',
                                                                    'Seller',
                                                                    'CouncilArea'],
                                                              handle_missing='return_nan'))),
                ('polynomial_features',
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('remove_multicollinearity',
                 TransformerWrapper(exclude=[],
                                    transformer=RemoveMulticollinearity(threshold=0.55))),
                ('normalize',
                 TransformerWrapper(transformer=StandardScaler()))])
2025-02-25 23:30:13,438:INFO:save_model() successfully completed......................................
2025-02-25 23:30:13,546:INFO:SubProcess save_model() end ==================================
2025-02-25 23:30:13,551:INFO:setup() successfully completed in 2.08s...............
2025-02-25 23:30:15,077:INFO:Initializing compare_models()
2025-02-25 23:30:15,078:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x29312bfa0>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x29312bfa0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2025-02-25 23:30:15,078:INFO:Checking exceptions
2025-02-25 23:30:15,081:INFO:Preparing display monitor
2025-02-25 23:30:15,131:INFO:Initializing Linear Regression
2025-02-25 23:30:15,131:INFO:Total runtime is 4.470348358154297e-06 minutes
2025-02-25 23:30:15,133:INFO:SubProcess create_model() called ==================================
2025-02-25 23:30:15,134:INFO:Initializing create_model()
2025-02-25 23:30:15,134:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x29312bfa0>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x2a751dae0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-25 23:30:15,134:INFO:Checking exceptions
2025-02-25 23:30:15,134:INFO:Importing libraries
2025-02-25 23:30:15,134:INFO:Copying training dataset
2025-02-25 23:30:15,143:INFO:Defining folds
2025-02-25 23:30:15,143:INFO:Declaring metric variables
2025-02-25 23:30:15,146:INFO:Importing untrained model
2025-02-25 23:30:15,148:INFO:Linear Regression Imported successfully
2025-02-25 23:30:15,152:INFO:Starting cross validation
2025-02-25 23:30:15,229:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-25 23:30:19,003:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:19,005:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:19,005:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:19,005:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:19,013:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:19,013:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:19,013:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:19,013:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:19,013:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:19,013:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:19,021:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:19,021:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:19,021:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:19,021:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:19,027:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003493 seconds.
2025-02-25 23:30:19,027:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:19,027:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002997 seconds.
2025-02-25 23:30:19,027:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:19,027:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003478 seconds.
2025-02-25 23:30:19,027:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:19,027:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003108 seconds.
2025-02-25 23:30:19,027:INFO:You can set `force_col_wise=true` to remove the overhead.[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003106 seconds.
2025-02-25 23:30:19,027:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:19,027:INFO:
2025-02-25 23:30:19,027:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002998 seconds.
2025-02-25 23:30:19,027:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:19,027:INFO:[LightGBM] [Info] Total Bins 1844
2025-02-25 23:30:19,027:INFO:[LightGBM] [Info] Total Bins 1837[LightGBM] [Info] Total Bins 1846
2025-02-25 23:30:19,027:INFO:
2025-02-25 23:30:19,028:INFO:[LightGBM] [Info] Total Bins 1839[LightGBM] [Info] Total Bins 1836
2025-02-25 23:30:19,028:INFO:
2025-02-25 23:30:19,028:INFO:[LightGBM] [Info] Total Bins 1842
2025-02-25 23:30:19,028:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:19,028:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:19,028:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001387 seconds.
2025-02-25 23:30:19,028:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-25 23:30:19,028:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-25 23:30:19,028:INFO:[LightGBM] [Info] Total Bins 1843
2025-02-25 23:30:19,028:INFO:[LightGBM] [Info] Number of data points in the train set: 8518, number of used features: 17
2025-02-25 23:30:19,028:INFO:[LightGBM] [Info] Number of data points in the train set: 8520, number of used features: 17
2025-02-25 23:30:19,028:INFO:[LightGBM] [Info] Number of data points in the train set: 8520, number of used features: 17[LightGBM] [Info] Number of data points in the train set: 8525, number of used features: 17
2025-02-25 23:30:19,029:INFO:
2025-02-25 23:30:19,029:INFO:[LightGBM] [Info] Number of data points in the train set: 8520, number of used features: 17
2025-02-25 23:30:19,029:INFO:[LightGBM] [Info] Number of data points in the train set: 8519, number of used features: 17
2025-02-25 23:30:19,029:INFO:[LightGBM] [Info] Number of data points in the train set: 8516, number of used features: 17
2025-02-25 23:30:19,030:INFO:[LightGBM] [Info] Start training from score 1.609038
2025-02-25 23:30:19,030:INFO:[LightGBM] [Info] Start training from score 1.608287
2025-02-25 23:30:19,030:INFO:[LightGBM] [Info] Start training from score 1.610263
2025-02-25 23:30:19,030:INFO:[LightGBM] [Info] Start training from score 1.611059
2025-02-25 23:30:19,030:INFO:[LightGBM] [Info] Start training from score 1.606921
2025-02-25 23:30:19,030:INFO:[LightGBM] [Info] Start training from score 1.608099
2025-02-25 23:30:19,030:INFO:[LightGBM] [Info] Start training from score 1.614906
2025-02-25 23:30:19,032:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001371 seconds.
2025-02-25 23:30:19,033:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:19,033:INFO:[LightGBM] [Info] Total Bins 1845
2025-02-25 23:30:19,033:INFO:[LightGBM] [Info] Number of data points in the train set: 8523, number of used features: 17
2025-02-25 23:30:19,034:INFO:[LightGBM] [Info] Start training from score 1.615394
2025-02-25 23:30:22,071:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:22,071:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:22,073:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000653 seconds.
2025-02-25 23:30:22,073:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:22,074:INFO:[LightGBM] [Info] Total Bins 1770
2025-02-25 23:30:22,074:INFO:[LightGBM] [Info] Number of data points in the train set: 7677, number of used features: 17
2025-02-25 23:30:22,074:INFO:[LightGBM] [Info] Start training from score 14.874951
2025-02-25 23:30:22,110:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:22,110:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:22,113:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000426 seconds.
2025-02-25 23:30:22,113:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-25 23:30:22,113:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-25 23:30:22,113:INFO:[LightGBM] [Info] Total Bins 1759
2025-02-25 23:30:22,113:INFO:[LightGBM] [Info] Number of data points in the train set: 7685, number of used features: 17
2025-02-25 23:30:22,114:INFO:[LightGBM] [Info] Start training from score 14.835003
2025-02-25 23:30:22,120:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:22,120:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:22,123:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000770 seconds.
2025-02-25 23:30:22,123:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:22,123:INFO:[LightGBM] [Info] Total Bins 1764
2025-02-25 23:30:22,123:INFO:[LightGBM] [Info] Number of data points in the train set: 7690, number of used features: 17
2025-02-25 23:30:22,124:INFO:[LightGBM] [Info] Start training from score 14.915475
2025-02-25 23:30:22,159:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:22,159:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:22,162:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000889 seconds.
2025-02-25 23:30:22,162:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:22,162:INFO:[LightGBM] [Info] Total Bins 1773
2025-02-25 23:30:22,163:INFO:[LightGBM] [Info] Number of data points in the train set: 7691, number of used features: 17
2025-02-25 23:30:22,163:INFO:[LightGBM] [Info] Start training from score 14.927188
2025-02-25 23:30:22,192:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:22,192:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:22,195:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000443 seconds.
2025-02-25 23:30:22,195:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-25 23:30:22,195:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-25 23:30:22,195:INFO:[LightGBM] [Info] Total Bins 1761
2025-02-25 23:30:22,195:INFO:[LightGBM] [Info] Number of data points in the train set: 7687, number of used features: 17
2025-02-25 23:30:22,196:INFO:[LightGBM] [Info] Start training from score 14.905945
2025-02-25 23:30:22,198:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:22,198:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:22,201:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000751 seconds.
2025-02-25 23:30:22,201:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:22,201:INFO:[LightGBM] [Info] Total Bins 1767
2025-02-25 23:30:22,201:INFO:[LightGBM] [Info] Number of data points in the train set: 7681, number of used features: 17
2025-02-25 23:30:22,202:INFO:[LightGBM] [Info] Start training from score 14.834266
2025-02-25 23:30:22,209:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:22,209:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:22,212:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000905 seconds.
2025-02-25 23:30:22,212:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:22,212:INFO:[LightGBM] [Info] Total Bins 1772
2025-02-25 23:30:22,212:INFO:[LightGBM] [Info] Number of data points in the train set: 7714, number of used features: 17
2025-02-25 23:30:22,213:INFO:[LightGBM] [Info] Start training from score 14.845735
2025-02-25 23:30:23,150:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:23,151:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:23,168:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008756 seconds.
2025-02-25 23:30:23,168:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:23,168:INFO:[LightGBM] [Info] Total Bins 1766
2025-02-25 23:30:23,168:INFO:[LightGBM] [Info] Number of data points in the train set: 7690, number of used features: 17
2025-02-25 23:30:23,172:INFO:[LightGBM] [Info] Start training from score 14.825618
2025-02-25 23:30:25,235:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:25,235:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:25,237:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000955 seconds.
2025-02-25 23:30:25,237:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:25,237:INFO:[LightGBM] [Info] Total Bins 1769
2025-02-25 23:30:25,237:INFO:[LightGBM] [Info] Number of data points in the train set: 5167, number of used features: 17
2025-02-25 23:30:25,238:INFO:[LightGBM] [Info] Start training from score 1965.395394
2025-02-25 23:30:25,332:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:25,332:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:25,335:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000659 seconds.
2025-02-25 23:30:25,335:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:25,335:INFO:[LightGBM] [Info] Total Bins 1769
2025-02-25 23:30:25,335:INFO:[LightGBM] [Info] Number of data points in the train set: 5163, number of used features: 17
2025-02-25 23:30:25,336:INFO:[LightGBM] [Info] Start training from score 1965.293628
2025-02-25 23:30:25,348:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:25,348:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:25,352:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000710 seconds.
2025-02-25 23:30:25,352:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:25,352:INFO:[LightGBM] [Info] Total Bins 1770
2025-02-25 23:30:25,352:INFO:[LightGBM] [Info] Number of data points in the train set: 5153, number of used features: 17
2025-02-25 23:30:25,353:INFO:[LightGBM] [Info] Start training from score 1965.193091
2025-02-25 23:30:25,355:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:25,355:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:25,357:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000694 seconds.
2025-02-25 23:30:25,357:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:25,357:INFO:[LightGBM] [Info] Total Bins 1766
2025-02-25 23:30:25,357:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-25 23:30:25,358:INFO:[LightGBM] [Info] Start training from score 1965.418992
2025-02-25 23:30:25,446:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:25,446:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:25,448:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000595 seconds.
2025-02-25 23:30:25,449:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:25,449:INFO:[LightGBM] [Info] Total Bins 1771
2025-02-25 23:30:25,449:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-25 23:30:25,450:INFO:[LightGBM] [Info] Start training from score 1965.251163
2025-02-25 23:30:25,773:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:25,773:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:25,776:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000365 seconds.
2025-02-25 23:30:25,776:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-25 23:30:25,776:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-25 23:30:25,776:INFO:[LightGBM] [Info] Total Bins 1759
2025-02-25 23:30:25,776:INFO:[LightGBM] [Info] Number of data points in the train set: 5176, number of used features: 17
2025-02-25 23:30:25,777:INFO:[LightGBM] [Info] Start training from score 1965.290958
2025-02-25 23:30:25,880:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:25,880:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:25,882:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000658 seconds.
2025-02-25 23:30:25,883:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:25,883:INFO:[LightGBM] [Info] Total Bins 1760
2025-02-25 23:30:25,883:INFO:[LightGBM] [Info] Number of data points in the train set: 5140, number of used features: 17
2025-02-25 23:30:25,883:INFO:[LightGBM] [Info] Start training from score 1965.178599
2025-02-25 23:30:26,089:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:26,089:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:26,092:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001315 seconds.
2025-02-25 23:30:26,092:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:26,092:INFO:[LightGBM] [Info] Total Bins 1757
2025-02-25 23:30:26,093:INFO:[LightGBM] [Info] Number of data points in the train set: 5152, number of used features: 17
2025-02-25 23:30:26,094:INFO:[LightGBM] [Info] Start training from score 1965.258152
2025-02-25 23:30:27,789:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:27,789:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:27,792:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000814 seconds.
2025-02-25 23:30:27,793:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:27,793:INFO:[LightGBM] [Info] Total Bins 2210
2025-02-25 23:30:27,793:INFO:[LightGBM] [Info] Number of data points in the train set: 8525, number of used features: 17
2025-02-25 23:30:27,793:INFO:[LightGBM] [Info] Start training from score 1.606921
2025-02-25 23:30:27,949:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:27,949:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:27,952:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000810 seconds.
2025-02-25 23:30:27,952:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:27,952:INFO:[LightGBM] [Info] Total Bins 2212
2025-02-25 23:30:27,952:INFO:[LightGBM] [Info] Number of data points in the train set: 8519, number of used features: 17
2025-02-25 23:30:27,953:INFO:[LightGBM] [Info] Start training from score 1.608287
2025-02-25 23:30:27,989:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:27,989:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:27,998:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002995 seconds.
2025-02-25 23:30:27,998:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:27,998:INFO:[LightGBM] [Info] Total Bins 2215
2025-02-25 23:30:28,000:INFO:[LightGBM] [Info] Number of data points in the train set: 8520, number of used features: 17
2025-02-25 23:30:28,003:INFO:[LightGBM] [Info] Start training from score 1.609038
2025-02-25 23:30:28,024:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:28,024:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:28,045:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012881 seconds.
2025-02-25 23:30:28,045:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:28,045:INFO:[LightGBM] [Info] Total Bins 2216
2025-02-25 23:30:28,046:INFO:[LightGBM] [Info] Number of data points in the train set: 8523, number of used features: 17
2025-02-25 23:30:28,047:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:28,047:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:28,049:INFO:[LightGBM] [Info] Start training from score 1.615394
2025-02-25 23:30:28,051:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000868 seconds.
2025-02-25 23:30:28,051:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:28,051:INFO:[LightGBM] [Info] Total Bins 2216
2025-02-25 23:30:28,051:INFO:[LightGBM] [Info] Number of data points in the train set: 8518, number of used features: 17
2025-02-25 23:30:28,051:INFO:[LightGBM] [Info] Start training from score 1.611059
2025-02-25 23:30:28,587:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:28,588:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:28,591:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001244 seconds.
2025-02-25 23:30:28,591:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:28,591:INFO:[LightGBM] [Info] Total Bins 2207
2025-02-25 23:30:28,592:INFO:[LightGBM] [Info] Number of data points in the train set: 8516, number of used features: 17
2025-02-25 23:30:28,595:INFO:[LightGBM] [Info] Start training from score 1.610263
2025-02-25 23:30:28,724:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:28,724:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:28,728:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001172 seconds.
2025-02-25 23:30:28,728:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:28,728:INFO:[LightGBM] [Info] Total Bins 2212
2025-02-25 23:30:28,728:INFO:[LightGBM] [Info] Number of data points in the train set: 8520, number of used features: 17
2025-02-25 23:30:28,729:INFO:[LightGBM] [Info] Start training from score 1.608099
2025-02-25 23:30:28,982:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:28,983:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:29,014:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021921 seconds.
2025-02-25 23:30:29,017:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:29,017:INFO:[LightGBM] [Info] Total Bins 2206
2025-02-25 23:30:29,022:INFO:[LightGBM] [Info] Number of data points in the train set: 8520, number of used features: 17
2025-02-25 23:30:29,046:INFO:[LightGBM] [Info] Start training from score 1.614906
2025-02-25 23:30:30,479:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:30,479:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:30,482:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001134 seconds.
2025-02-25 23:30:30,483:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:30,483:INFO:[LightGBM] [Info] Total Bins 1919
2025-02-25 23:30:30,483:INFO:[LightGBM] [Info] Number of data points in the train set: 7690, number of used features: 17
2025-02-25 23:30:30,484:INFO:[LightGBM] [Info] Start training from score 14.915475
2025-02-25 23:30:30,560:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:30,561:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:30,582:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006592 seconds.
2025-02-25 23:30:30,582:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:30,582:INFO:[LightGBM] [Info] Total Bins 1926
2025-02-25 23:30:30,583:INFO:[LightGBM] [Info] Number of data points in the train set: 7691, number of used features: 17
2025-02-25 23:30:30,593:INFO:[LightGBM] [Info] Start training from score 14.927188
2025-02-25 23:30:30,625:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:30,625:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:30,627:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:30,628:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:30,629:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001240 seconds.
2025-02-25 23:30:30,629:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:30,629:INFO:[LightGBM] [Info] Total Bins 1923
2025-02-25 23:30:30,629:INFO:[LightGBM] [Info] Number of data points in the train set: 7677, number of used features: 17
2025-02-25 23:30:30,630:INFO:[LightGBM] [Info] Start training from score 14.874951
2025-02-25 23:30:30,631:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001044 seconds.
2025-02-25 23:30:30,631:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:30,631:INFO:[LightGBM] [Info] Total Bins 1920
2025-02-25 23:30:30,631:INFO:[LightGBM] [Info] Number of data points in the train set: 7681, number of used features: 17
2025-02-25 23:30:30,631:INFO:[LightGBM] [Info] Start training from score 14.834266
2025-02-25 23:30:30,705:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:30,705:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:30,708:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000796 seconds.
2025-02-25 23:30:30,708:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:30,709:INFO:[LightGBM] [Info] Total Bins 1926
2025-02-25 23:30:30,709:INFO:[LightGBM] [Info] Number of data points in the train set: 7714, number of used features: 17
2025-02-25 23:30:30,709:INFO:[LightGBM] [Info] Start training from score 14.845735
2025-02-25 23:30:31,164:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:31,164:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:31,167:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000790 seconds.
2025-02-25 23:30:31,167:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:31,167:INFO:[LightGBM] [Info] Total Bins 1914
2025-02-25 23:30:31,167:INFO:[LightGBM] [Info] Number of data points in the train set: 7685, number of used features: 17
2025-02-25 23:30:31,168:INFO:[LightGBM] [Info] Start training from score 14.835003
2025-02-25 23:30:31,448:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:31,449:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:31,452:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000960 seconds.
2025-02-25 23:30:31,452:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:31,452:INFO:[LightGBM] [Info] Total Bins 1919
2025-02-25 23:30:31,452:INFO:[LightGBM] [Info] Number of data points in the train set: 7690, number of used features: 17
2025-02-25 23:30:31,452:INFO:[LightGBM] [Info] Start training from score 14.825618
2025-02-25 23:30:31,662:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:31,663:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:31,666:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001079 seconds.
2025-02-25 23:30:31,666:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:31,666:INFO:[LightGBM] [Info] Total Bins 1916
2025-02-25 23:30:31,666:INFO:[LightGBM] [Info] Number of data points in the train set: 7687, number of used features: 17
2025-02-25 23:30:31,667:INFO:[LightGBM] [Info] Start training from score 14.905945
2025-02-25 23:30:33,098:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:33,099:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:33,101:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000728 seconds.
2025-02-25 23:30:33,101:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:33,101:INFO:[LightGBM] [Info] Total Bins 1769
2025-02-25 23:30:33,101:INFO:[LightGBM] [Info] Number of data points in the train set: 5167, number of used features: 17
2025-02-25 23:30:33,102:INFO:[LightGBM] [Info] Start training from score 1965.395394
2025-02-25 23:30:33,298:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:33,298:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:33,312:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010071 seconds.
2025-02-25 23:30:33,313:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:33,313:INFO:[LightGBM] [Info] Total Bins 1767
2025-02-25 23:30:33,313:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-25 23:30:33,317:INFO:[LightGBM] [Info] Start training from score 1965.418992
2025-02-25 23:30:33,386:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:33,386:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:33,388:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001111 seconds.
2025-02-25 23:30:33,388:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:33,388:INFO:[LightGBM] [Info] Total Bins 1770
2025-02-25 23:30:33,389:INFO:[LightGBM] [Info] Number of data points in the train set: 5153, number of used features: 17
2025-02-25 23:30:33,390:INFO:[LightGBM] [Info] Start training from score 1965.193091
2025-02-25 23:30:33,439:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:33,439:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:33,442:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000988 seconds.
2025-02-25 23:30:33,442:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:33,442:INFO:[LightGBM] [Info] Total Bins 1771
2025-02-25 23:30:33,442:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-25 23:30:33,442:INFO:[LightGBM] [Info] Start training from score 1965.251163
2025-02-25 23:30:33,447:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:33,447:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:33,449:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000751 seconds.
2025-02-25 23:30:33,449:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:33,450:INFO:[LightGBM] [Info] Total Bins 1771
2025-02-25 23:30:33,450:INFO:[LightGBM] [Info] Number of data points in the train set: 5163, number of used features: 17
2025-02-25 23:30:33,450:INFO:[LightGBM] [Info] Start training from score 1965.293628
2025-02-25 23:30:33,817:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:33,817:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:33,820:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001710 seconds.
2025-02-25 23:30:33,820:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:33,820:INFO:[LightGBM] [Info] Total Bins 1760
2025-02-25 23:30:33,820:INFO:[LightGBM] [Info] Number of data points in the train set: 5140, number of used features: 17
2025-02-25 23:30:33,821:INFO:[LightGBM] [Info] Start training from score 1965.178599
2025-02-25 23:30:34,152:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:34,152:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:34,154:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000657 seconds.
2025-02-25 23:30:34,154:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:34,154:INFO:[LightGBM] [Info] Total Bins 1758
2025-02-25 23:30:34,154:INFO:[LightGBM] [Info] Number of data points in the train set: 5176, number of used features: 17
2025-02-25 23:30:34,155:INFO:[LightGBM] [Info] Start training from score 1965.290958
2025-02-25 23:30:34,224:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:34,224:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:34,227:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000867 seconds.
2025-02-25 23:30:34,227:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:34,227:INFO:[LightGBM] [Info] Total Bins 1758
2025-02-25 23:30:34,227:INFO:[LightGBM] [Info] Number of data points in the train set: 5152, number of used features: 17
2025-02-25 23:30:34,228:INFO:[LightGBM] [Info] Start training from score 1965.258152
2025-02-25 23:30:35,703:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:35,703:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:35,707:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000795 seconds.
2025-02-25 23:30:35,707:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:35,707:INFO:[LightGBM] [Info] Total Bins 2209
2025-02-25 23:30:35,707:INFO:[LightGBM] [Info] Number of data points in the train set: 8525, number of used features: 17
2025-02-25 23:30:35,708:INFO:[LightGBM] [Info] Start training from score 1.606921
2025-02-25 23:30:35,965:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:35,965:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:35,988:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012428 seconds.
2025-02-25 23:30:35,988:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:35,988:INFO:[LightGBM] [Info] Total Bins 2215
2025-02-25 23:30:35,990:INFO:[LightGBM] [Info] Number of data points in the train set: 8523, number of used features: 17
2025-02-25 23:30:35,994:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:35,994:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:35,996:INFO:[LightGBM] [Info] Start training from score 1.615394
2025-02-25 23:30:35,997:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000790 seconds.
2025-02-25 23:30:35,997:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:35,997:INFO:[LightGBM] [Info] Total Bins 2215
2025-02-25 23:30:35,997:INFO:[LightGBM] [Info] Number of data points in the train set: 8520, number of used features: 17
2025-02-25 23:30:35,998:INFO:[LightGBM] [Info] Start training from score 1.609038
2025-02-25 23:30:36,091:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:36,091:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:36,094:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000697 seconds.
2025-02-25 23:30:36,094:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:36,094:INFO:[LightGBM] [Info] Total Bins 2216
2025-02-25 23:30:36,094:INFO:[LightGBM] [Info] Number of data points in the train set: 8518, number of used features: 17
2025-02-25 23:30:36,095:INFO:[LightGBM] [Info] Start training from score 1.611059
2025-02-25 23:30:36,110:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:36,110:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:36,114:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000899 seconds.
2025-02-25 23:30:36,114:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:36,114:INFO:[LightGBM] [Info] Total Bins 2213
2025-02-25 23:30:36,114:INFO:[LightGBM] [Info] Number of data points in the train set: 8519, number of used features: 17
2025-02-25 23:30:36,114:INFO:[LightGBM] [Info] Start training from score 1.608287
2025-02-25 23:30:36,498:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:36,499:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:36,502:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000952 seconds.
2025-02-25 23:30:36,502:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:36,502:INFO:[LightGBM] [Info] Total Bins 2206
2025-02-25 23:30:36,502:INFO:[LightGBM] [Info] Number of data points in the train set: 8516, number of used features: 17
2025-02-25 23:30:36,503:INFO:[LightGBM] [Info] Start training from score 1.610263
2025-02-25 23:30:36,829:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:36,829:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:36,832:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000833 seconds.
2025-02-25 23:30:36,832:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:36,832:INFO:[LightGBM] [Info] Total Bins 2206
2025-02-25 23:30:36,832:INFO:[LightGBM] [Info] Number of data points in the train set: 8520, number of used features: 17
2025-02-25 23:30:36,833:INFO:[LightGBM] [Info] Start training from score 1.614906
2025-02-25 23:30:36,969:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:36,969:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:36,972:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000906 seconds.
2025-02-25 23:30:36,972:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:36,972:INFO:[LightGBM] [Info] Total Bins 2212
2025-02-25 23:30:36,972:INFO:[LightGBM] [Info] Number of data points in the train set: 8520, number of used features: 17
2025-02-25 23:30:36,972:INFO:[LightGBM] [Info] Start training from score 1.608099
2025-02-25 23:30:38,498:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:38,498:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:38,501:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001018 seconds.
2025-02-25 23:30:38,501:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:38,501:INFO:[LightGBM] [Info] Total Bins 1921
2025-02-25 23:30:38,501:INFO:[LightGBM] [Info] Number of data points in the train set: 7690, number of used features: 17
2025-02-25 23:30:38,502:INFO:[LightGBM] [Info] Start training from score 14.915475
2025-02-25 23:30:38,763:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:38,763:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:38,767:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000941 seconds.
2025-02-25 23:30:38,767:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:38,767:INFO:[LightGBM] [Info] Total Bins 1921
2025-02-25 23:30:38,767:INFO:[LightGBM] [Info] Number of data points in the train set: 7681, number of used features: 17
2025-02-25 23:30:38,768:INFO:[LightGBM] [Info] Start training from score 14.834266
2025-02-25 23:30:38,806:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:38,806:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:38,809:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000454 seconds.
2025-02-25 23:30:38,809:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-25 23:30:38,809:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-25 23:30:38,809:INFO:[LightGBM] [Info] Total Bins 1925
2025-02-25 23:30:38,809:INFO:[LightGBM] [Info] Number of data points in the train set: 7714, number of used features: 17
2025-02-25 23:30:38,810:INFO:[LightGBM] [Info] Start training from score 14.845735
2025-02-25 23:30:38,890:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:38,890:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:38,893:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000842 seconds.
2025-02-25 23:30:38,893:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:38,893:INFO:[LightGBM] [Info] Total Bins 1924
2025-02-25 23:30:38,893:INFO:[LightGBM] [Info] Number of data points in the train set: 7677, number of used features: 17
2025-02-25 23:30:38,894:INFO:[LightGBM] [Info] Start training from score 14.874951
2025-02-25 23:30:38,940:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:38,940:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:38,943:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000772 seconds.
2025-02-25 23:30:38,943:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:38,943:INFO:[LightGBM] [Info] Total Bins 1927
2025-02-25 23:30:38,943:INFO:[LightGBM] [Info] Number of data points in the train set: 7691, number of used features: 17
2025-02-25 23:30:38,944:INFO:[LightGBM] [Info] Start training from score 14.927188
2025-02-25 23:30:39,334:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:39,334:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:39,336:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000599 seconds.
2025-02-25 23:30:39,336:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:39,336:INFO:[LightGBM] [Info] Total Bins 1914
2025-02-25 23:30:39,337:INFO:[LightGBM] [Info] Number of data points in the train set: 7685, number of used features: 17
2025-02-25 23:30:39,337:INFO:[LightGBM] [Info] Start training from score 14.835003
2025-02-25 23:30:39,435:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:39,436:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:39,440:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001920 seconds.
2025-02-25 23:30:39,440:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:39,440:INFO:[LightGBM] [Info] Total Bins 1916
2025-02-25 23:30:39,441:INFO:[LightGBM] [Info] Number of data points in the train set: 7687, number of used features: 17
2025-02-25 23:30:39,442:INFO:[LightGBM] [Info] Start training from score 14.905945
2025-02-25 23:30:39,732:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:39,732:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:39,736:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001251 seconds.
2025-02-25 23:30:39,736:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:39,736:INFO:[LightGBM] [Info] Total Bins 1919
2025-02-25 23:30:39,736:INFO:[LightGBM] [Info] Number of data points in the train set: 7690, number of used features: 17
2025-02-25 23:30:39,737:INFO:[LightGBM] [Info] Start training from score 14.825618
2025-02-25 23:30:41,241:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:41,241:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:41,243:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000783 seconds.
2025-02-25 23:30:41,243:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:41,243:INFO:[LightGBM] [Info] Total Bins 1770
2025-02-25 23:30:41,243:INFO:[LightGBM] [Info] Number of data points in the train set: 5167, number of used features: 17
2025-02-25 23:30:41,244:INFO:[LightGBM] [Info] Start training from score 1965.395394
2025-02-25 23:30:41,331:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:41,331:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:41,333:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000593 seconds.
2025-02-25 23:30:41,333:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:41,334:INFO:[LightGBM] [Info] Total Bins 1771
2025-02-25 23:30:41,334:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-25 23:30:41,334:INFO:[LightGBM] [Info] Start training from score 1965.251163
2025-02-25 23:30:41,536:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:41,536:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:41,538:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000647 seconds.
2025-02-25 23:30:41,538:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:41,538:INFO:[LightGBM] [Info] Total Bins 1770
2025-02-25 23:30:41,538:INFO:[LightGBM] [Info] Number of data points in the train set: 5153, number of used features: 17
2025-02-25 23:30:41,539:INFO:[LightGBM] [Info] Start training from score 1965.193091
2025-02-25 23:30:41,720:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:41,720:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:41,723:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001336 seconds.
2025-02-25 23:30:41,723:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:41,723:INFO:[LightGBM] [Info] Total Bins 1770
2025-02-25 23:30:41,724:INFO:[LightGBM] [Info] Number of data points in the train set: 5163, number of used features: 17
2025-02-25 23:30:41,725:INFO:[LightGBM] [Info] Start training from score 1965.293628
2025-02-25 23:30:41,935:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:41,935:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:41,943:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002792 seconds.
2025-02-25 23:30:41,943:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:41,943:INFO:[LightGBM] [Info] Total Bins 1759
2025-02-25 23:30:41,944:INFO:[LightGBM] [Info] Number of data points in the train set: 5140, number of used features: 17
2025-02-25 23:30:41,945:INFO:[LightGBM] [Info] Start training from score 1965.178599
2025-02-25 23:30:41,955:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:41,955:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:41,995:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.026591 seconds.
2025-02-25 23:30:41,995:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:41,995:INFO:[LightGBM] [Info] Total Bins 1766
2025-02-25 23:30:41,996:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-25 23:30:42,012:INFO:[LightGBM] [Info] Start training from score 1965.418992
2025-02-25 23:30:42,112:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:42,112:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:42,123:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007714 seconds.
2025-02-25 23:30:42,123:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-25 23:30:42,123:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-25 23:30:42,126:INFO:[LightGBM] [Info] Total Bins 1759
2025-02-25 23:30:42,128:INFO:[LightGBM] [Info] Number of data points in the train set: 5176, number of used features: 17
2025-02-25 23:30:42,131:INFO:[LightGBM] [Info] Start training from score 1965.290958
2025-02-25 23:30:42,601:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:42,601:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:42,604:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000692 seconds.
2025-02-25 23:30:42,604:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:42,604:INFO:[LightGBM] [Info] Total Bins 1758
2025-02-25 23:30:42,604:INFO:[LightGBM] [Info] Number of data points in the train set: 5152, number of used features: 17
2025-02-25 23:30:42,605:INFO:[LightGBM] [Info] Start training from score 1965.258152
2025-02-25 23:30:44,149:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:44,149:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:44,153:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000902 seconds.
2025-02-25 23:30:44,153:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:44,153:INFO:[LightGBM] [Info] Total Bins 2215
2025-02-25 23:30:44,153:INFO:[LightGBM] [Info] Number of data points in the train set: 8520, number of used features: 17
2025-02-25 23:30:44,154:INFO:[LightGBM] [Info] Start training from score 1.609038
2025-02-25 23:30:44,236:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:44,236:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:44,239:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000873 seconds.
2025-02-25 23:30:44,239:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:44,239:INFO:[LightGBM] [Info] Total Bins 2211
2025-02-25 23:30:44,239:INFO:[LightGBM] [Info] Number of data points in the train set: 8525, number of used features: 17
2025-02-25 23:30:44,240:INFO:[LightGBM] [Info] Start training from score 1.606921
2025-02-25 23:30:44,531:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:44,531:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:44,534:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000821 seconds.
2025-02-25 23:30:44,534:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:44,534:INFO:[LightGBM] [Info] Total Bins 2216
2025-02-25 23:30:44,534:INFO:[LightGBM] [Info] Number of data points in the train set: 8518, number of used features: 17
2025-02-25 23:30:44,535:INFO:[LightGBM] [Info] Start training from score 1.611059
2025-02-25 23:30:44,742:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:44,742:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:44,745:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000816 seconds.
2025-02-25 23:30:44,745:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:44,745:INFO:[LightGBM] [Info] Total Bins 2213
2025-02-25 23:30:44,745:INFO:[LightGBM] [Info] Number of data points in the train set: 8519, number of used features: 17
2025-02-25 23:30:44,746:INFO:[LightGBM] [Info] Start training from score 1.608287
2025-02-25 23:30:44,856:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:44,856:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:44,859:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001000 seconds.
2025-02-25 23:30:44,859:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:44,859:INFO:[LightGBM] [Info] Total Bins 2208
2025-02-25 23:30:44,859:INFO:[LightGBM] [Info] Number of data points in the train set: 8516, number of used features: 17
2025-02-25 23:30:44,860:INFO:[LightGBM] [Info] Start training from score 1.610263
2025-02-25 23:30:44,897:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:44,897:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:44,901:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001134 seconds.
2025-02-25 23:30:44,901:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:44,901:INFO:[LightGBM] [Info] Total Bins 2215
2025-02-25 23:30:44,901:INFO:[LightGBM] [Info] Number of data points in the train set: 8523, number of used features: 17
2025-02-25 23:30:44,902:INFO:[LightGBM] [Info] Start training from score 1.615394
2025-02-25 23:30:45,433:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:45,433:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:45,439:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002573 seconds.
2025-02-25 23:30:45,439:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:45,439:INFO:[LightGBM] [Info] Total Bins 2212
2025-02-25 23:30:45,440:INFO:[LightGBM] [Info] Number of data points in the train set: 8520, number of used features: 17
2025-02-25 23:30:45,442:INFO:[LightGBM] [Info] Start training from score 1.608099
2025-02-25 23:30:45,708:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:45,708:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:45,712:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000875 seconds.
2025-02-25 23:30:45,712:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:45,712:INFO:[LightGBM] [Info] Total Bins 2206
2025-02-25 23:30:45,712:INFO:[LightGBM] [Info] Number of data points in the train set: 8520, number of used features: 17
2025-02-25 23:30:45,712:INFO:[LightGBM] [Info] Start training from score 1.614906
2025-02-25 23:30:47,137:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:47,137:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:47,139:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000732 seconds.
2025-02-25 23:30:47,139:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:47,139:INFO:[LightGBM] [Info] Total Bins 1920
2025-02-25 23:30:47,140:INFO:[LightGBM] [Info] Number of data points in the train set: 7681, number of used features: 17
2025-02-25 23:30:47,140:INFO:[LightGBM] [Info] Start training from score 14.834266
2025-02-25 23:30:47,466:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:47,466:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:47,471:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001800 seconds.
2025-02-25 23:30:47,471:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:47,471:INFO:[LightGBM] [Info] Total Bins 1921
2025-02-25 23:30:47,471:INFO:[LightGBM] [Info] Number of data points in the train set: 7690, number of used features: 17
2025-02-25 23:30:47,472:INFO:[LightGBM] [Info] Start training from score 14.915475
2025-02-25 23:30:47,817:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:47,817:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:47,821:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001042 seconds.
2025-02-25 23:30:47,821:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:47,821:INFO:[LightGBM] [Info] Total Bins 1923
2025-02-25 23:30:47,821:INFO:[LightGBM] [Info] Number of data points in the train set: 7677, number of used features: 17
2025-02-25 23:30:47,821:INFO:[LightGBM] [Info] Start training from score 14.874951
2025-02-25 23:30:47,937:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:47,937:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:47,941:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000966 seconds.
2025-02-25 23:30:47,941:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:47,941:INFO:[LightGBM] [Info] Total Bins 1927
2025-02-25 23:30:47,941:INFO:[LightGBM] [Info] Number of data points in the train set: 7691, number of used features: 17
2025-02-25 23:30:47,942:INFO:[LightGBM] [Info] Start training from score 14.927188
2025-02-25 23:30:48,173:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:48,173:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:48,198:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015864 seconds.
2025-02-25 23:30:48,198:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:48,198:INFO:[LightGBM] [Info] Total Bins 1926
2025-02-25 23:30:48,230:INFO:[LightGBM] [Info] Number of data points in the train set: 7714, number of used features: 17
2025-02-25 23:30:48,232:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:48,232:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:48,236:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001550 seconds.
2025-02-25 23:30:48,237:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:48,237:INFO:[LightGBM] [Info] Total Bins 1915
2025-02-25 23:30:48,237:INFO:[LightGBM] [Info] Number of data points in the train set: 7685, number of used features: 17
2025-02-25 23:30:48,237:INFO:[LightGBM] [Info] Start training from score 14.835003
2025-02-25 23:30:48,263:INFO:[LightGBM] [Info] Start training from score 14.845735
2025-02-25 23:30:48,778:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:48,778:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:48,781:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000826 seconds.
2025-02-25 23:30:48,781:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:48,781:INFO:[LightGBM] [Info] Total Bins 1919
2025-02-25 23:30:48,782:INFO:[LightGBM] [Info] Number of data points in the train set: 7690, number of used features: 17
2025-02-25 23:30:48,782:INFO:[LightGBM] [Info] Start training from score 14.825618
2025-02-25 23:30:48,913:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:48,914:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:48,917:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001361 seconds.
2025-02-25 23:30:48,917:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:48,917:INFO:[LightGBM] [Info] Total Bins 1914
2025-02-25 23:30:48,917:INFO:[LightGBM] [Info] Number of data points in the train set: 7687, number of used features: 17
2025-02-25 23:30:48,918:INFO:[LightGBM] [Info] Start training from score 14.905945
2025-02-25 23:30:50,198:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:50,198:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:50,202:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001269 seconds.
2025-02-25 23:30:50,202:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:50,202:INFO:[LightGBM] [Info] Total Bins 1771
2025-02-25 23:30:50,202:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-25 23:30:50,203:INFO:[LightGBM] [Info] Start training from score 1965.251163
2025-02-25 23:30:50,448:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:50,448:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:50,451:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000874 seconds.
2025-02-25 23:30:50,451:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:50,451:INFO:[LightGBM] [Info] Total Bins 1769
2025-02-25 23:30:50,452:INFO:[LightGBM] [Info] Number of data points in the train set: 5167, number of used features: 17
2025-02-25 23:30:50,452:INFO:[LightGBM] [Info] Start training from score 1965.395394
2025-02-25 23:30:50,745:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:50,745:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:50,748:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001116 seconds.
2025-02-25 23:30:50,748:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:50,748:INFO:[LightGBM] [Info] Total Bins 1770
2025-02-25 23:30:50,748:INFO:[LightGBM] [Info] Number of data points in the train set: 5153, number of used features: 17
2025-02-25 23:30:50,750:INFO:[LightGBM] [Info] Start training from score 1965.193091
2025-02-25 23:30:50,790:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:50,790:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:50,792:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000641 seconds.
2025-02-25 23:30:50,792:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:50,792:INFO:[LightGBM] [Info] Total Bins 1770
2025-02-25 23:30:50,792:INFO:[LightGBM] [Info] Number of data points in the train set: 5163, number of used features: 17
2025-02-25 23:30:50,793:INFO:[LightGBM] [Info] Start training from score 1965.293628
2025-02-25 23:30:51,216:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:51,216:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:51,218:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000724 seconds.
2025-02-25 23:30:51,218:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:51,219:INFO:[LightGBM] [Info] Total Bins 1760
2025-02-25 23:30:51,219:INFO:[LightGBM] [Info] Number of data points in the train set: 5140, number of used features: 17
2025-02-25 23:30:51,219:INFO:[LightGBM] [Info] Start training from score 1965.178599
2025-02-25 23:30:51,314:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:51,314:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:51,317:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001011 seconds.
2025-02-25 23:30:51,317:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:51,317:INFO:[LightGBM] [Info] Total Bins 1766
2025-02-25 23:30:51,317:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-25 23:30:51,318:INFO:[LightGBM] [Info] Start training from score 1965.418992
2025-02-25 23:30:51,836:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:51,836:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:51,839:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000871 seconds.
2025-02-25 23:30:51,839:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:51,839:INFO:[LightGBM] [Info] Total Bins 1757
2025-02-25 23:30:51,839:INFO:[LightGBM] [Info] Number of data points in the train set: 5152, number of used features: 17
2025-02-25 23:30:51,840:INFO:[LightGBM] [Info] Start training from score 1965.258152
2025-02-25 23:30:51,999:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:51,999:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:52,002:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000974 seconds.
2025-02-25 23:30:52,002:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:52,002:INFO:[LightGBM] [Info] Total Bins 1758
2025-02-25 23:30:52,002:INFO:[LightGBM] [Info] Number of data points in the train set: 5176, number of used features: 17
2025-02-25 23:30:52,003:INFO:[LightGBM] [Info] Start training from score 1965.290958
2025-02-25 23:30:53,258:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:53,258:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:53,262:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000872 seconds.
2025-02-25 23:30:53,262:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:53,262:INFO:[LightGBM] [Info] Total Bins 2215
2025-02-25 23:30:53,262:INFO:[LightGBM] [Info] Number of data points in the train set: 8520, number of used features: 17
2025-02-25 23:30:53,263:INFO:[LightGBM] [Info] Start training from score 1.609038
2025-02-25 23:30:53,522:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:53,522:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:53,550:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003811 seconds.
2025-02-25 23:30:53,551:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-25 23:30:53,551:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-25 23:30:53,551:INFO:[LightGBM] [Info] Total Bins 2215
2025-02-25 23:30:53,552:INFO:[LightGBM] [Info] Number of data points in the train set: 8518, number of used features: 17
2025-02-25 23:30:53,556:INFO:[LightGBM] [Info] Start training from score 1.611059
2025-02-25 23:30:53,576:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:53,576:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:53,581:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002005 seconds.
2025-02-25 23:30:53,581:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:53,581:INFO:[LightGBM] [Info] Total Bins 2210
2025-02-25 23:30:53,582:INFO:[LightGBM] [Info] Number of data points in the train set: 8525, number of used features: 17
2025-02-25 23:30:53,584:INFO:[LightGBM] [Info] Start training from score 1.606921
2025-02-25 23:30:53,889:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:53,890:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:53,927:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002095 seconds.
2025-02-25 23:30:53,927:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:53,927:INFO:[LightGBM] [Info] Total Bins 2213
2025-02-25 23:30:53,927:INFO:[LightGBM] [Info] Number of data points in the train set: 8519, number of used features: 17
2025-02-25 23:30:53,929:INFO:[LightGBM] [Info] Start training from score 1.608287
2025-02-25 23:30:54,171:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:54,171:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:54,177:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003651 seconds.
2025-02-25 23:30:54,177:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:54,177:INFO:[LightGBM] [Info] Total Bins 2205
2025-02-25 23:30:54,179:INFO:[LightGBM] [Info] Number of data points in the train set: 8516, number of used features: 17
2025-02-25 23:30:54,181:INFO:[LightGBM] [Info] Start training from score 1.610263
2025-02-25 23:30:54,215:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:54,215:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:54,220:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002285 seconds.
2025-02-25 23:30:54,220:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:54,220:INFO:[LightGBM] [Info] Total Bins 2214
2025-02-25 23:30:54,221:INFO:[LightGBM] [Info] Number of data points in the train set: 8523, number of used features: 17
2025-02-25 23:30:54,222:INFO:[LightGBM] [Info] Start training from score 1.615394
2025-02-25 23:30:54,660:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:54,660:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:54,663:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001198 seconds.
2025-02-25 23:30:54,664:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:54,664:INFO:[LightGBM] [Info] Total Bins 2212
2025-02-25 23:30:54,664:INFO:[LightGBM] [Info] Number of data points in the train set: 8520, number of used features: 17
2025-02-25 23:30:54,665:INFO:[LightGBM] [Info] Start training from score 1.608099
2025-02-25 23:30:54,916:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:54,916:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:54,919:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000852 seconds.
2025-02-25 23:30:54,920:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:54,920:INFO:[LightGBM] [Info] Total Bins 2206
2025-02-25 23:30:54,920:INFO:[LightGBM] [Info] Number of data points in the train set: 8520, number of used features: 17
2025-02-25 23:30:54,921:INFO:[LightGBM] [Info] Start training from score 1.614906
2025-02-25 23:30:56,089:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:56,089:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:56,093:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000873 seconds.
2025-02-25 23:30:56,093:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-25 23:30:56,093:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-25 23:30:56,093:INFO:[LightGBM] [Info] Total Bins 1920
2025-02-25 23:30:56,094:INFO:[LightGBM] [Info] Number of data points in the train set: 7681, number of used features: 17
2025-02-25 23:30:56,095:INFO:[LightGBM] [Info] Start training from score 14.834266
2025-02-25 23:30:56,468:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-25 23:30:56,468:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-25 23:30:56,471:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000606 seconds.
2025-02-25 23:30:56,471:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-25 23:30:56,471:INFO:[LightGBM] [Info] Total Bins 1921
2025-02-25 23:30:56,471:INFO:[LightGBM] [Info] Number of data points in the train set: 7690, number of used features: 17
2025-02-25 23:30:56,472:INFO:[LightGBM] [Info] Start training from score 14.915475
2025-02-25 23:31:50,942:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-25 23:31:50,942:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-25 23:31:50,942:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-25 23:31:50,942:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-25 23:31:52,275:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_23854/2838843215.py:2: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.boxplot(data=df, x="Method", y="Price", palette="viridis")

2025-02-25 23:31:52,391:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_23854/2297183518.py:4: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.barplot(x=avg_price_by_seller.index, y=avg_price_by_seller.values, palette="viridis")

2025-02-25 23:31:53,515:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_23854/1639960328.py:2: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.boxplot(

2025-02-25 23:31:54,128:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_23854/2157425422.py:3: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.violinplot(data=df, x="Bedroom2", y="Price", palette="viridis", ax=axes[0])

2025-02-25 23:31:54,247:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_23854/2157425422.py:8: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.violinplot(data=df, x="Bathroom", y="Price", palette="viridis", ax=axes[1])

2025-02-25 23:31:54,343:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_23854/2157425422.py:13: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.violinplot(data=df, x="Car", y="Price", palette="viridis", ax=axes[2])

2025-02-25 23:31:55,547:INFO:PyCaret RegressionExperiment
2025-02-25 23:31:55,547:INFO:Logging name: house_pricing
2025-02-25 23:31:55,547:INFO:ML Usecase: MLUsecase.REGRESSION
2025-02-25 23:31:55,547:INFO:version 3.3.2
2025-02-25 23:31:55,547:INFO:Initializing setup()
2025-02-25 23:31:55,547:INFO:self.USI: f8d7
2025-02-25 23:31:55,547:INFO:self._variable_keys: {'X', 'n_jobs_param', 'gpu_param', 'html_param', 'X_test', 'seed', 'y', 'exp_id', 'fold_groups_param', 'pipeline', 'y_train', 'X_train', 'logging_param', 'USI', 'fold_generator', 'memory', 'y_test', 'idx', 'log_plots_param', '_ml_usecase', 'transform_target_param', 'gpu_n_jobs_param', 'exp_name_log', '_available_plots', 'data', 'target_param', 'fold_shuffle_param'}
2025-02-25 23:31:55,547:INFO:Checking environment
2025-02-25 23:31:55,547:INFO:python_version: 3.10.15
2025-02-25 23:31:55,547:INFO:python_build: ('main', 'Oct  3 2024 02:24:49')
2025-02-25 23:31:55,547:INFO:machine: arm64
2025-02-25 23:31:55,547:INFO:platform: macOS-14.1-arm64-arm-64bit
2025-02-25 23:31:55,547:INFO:Memory: svmem(total=17179869184, available=6381977600, percent=62.9, used=5696307200, free=2246311936, active=3700670464, inactive=3780427776, wired=1995636736)
2025-02-25 23:31:55,547:INFO:Physical Core: 8
2025-02-25 23:31:55,547:INFO:Logical Core: 8
2025-02-25 23:31:55,547:INFO:Checking libraries
2025-02-25 23:31:55,547:INFO:System:
2025-02-25 23:31:55,547:INFO:    python: 3.10.15 (main, Oct  3 2024, 02:24:49) [Clang 14.0.6 ]
2025-02-25 23:31:55,547:INFO:executable: /Users/daaa/opt/miniconda3/envs/mlops/bin/python
2025-02-25 23:31:55,547:INFO:   machine: macOS-14.1-arm64-arm-64bit
2025-02-25 23:31:55,547:INFO:PyCaret required dependencies:
2025-02-25 23:31:55,949:INFO:                 pip: 24.2
2025-02-25 23:31:55,949:INFO:          setuptools: 75.1.0
2025-02-25 23:31:55,949:INFO:             pycaret: 3.3.2
2025-02-25 23:31:55,950:INFO:             IPython: 8.30.0
2025-02-25 23:31:55,950:INFO:          ipywidgets: 8.1.5
2025-02-25 23:31:55,950:INFO:                tqdm: 4.67.1
2025-02-25 23:31:55,950:INFO:               numpy: 1.26.4
2025-02-25 23:31:55,950:INFO:              pandas: 2.1.4
2025-02-25 23:31:55,950:INFO:              jinja2: 3.1.4
2025-02-25 23:31:55,950:INFO:               scipy: 1.11.4
2025-02-25 23:31:55,950:INFO:              joblib: 1.3.2
2025-02-25 23:31:55,950:INFO:             sklearn: 1.4.2
2025-02-25 23:31:55,950:INFO:                pyod: 2.0.2
2025-02-25 23:31:55,950:INFO:            imblearn: 0.12.4
2025-02-25 23:31:55,950:INFO:   category_encoders: 2.6.4
2025-02-25 23:31:55,950:INFO:            lightgbm: 4.5.0
2025-02-25 23:31:55,950:INFO:               numba: 0.60.0
2025-02-25 23:31:55,950:INFO:            requests: 2.32.3
2025-02-25 23:31:55,950:INFO:          matplotlib: 3.10.0
2025-02-25 23:31:55,950:INFO:          scikitplot: 0.3.7
2025-02-25 23:31:55,950:INFO:         yellowbrick: 1.5
2025-02-25 23:31:55,950:INFO:              plotly: 5.24.1
2025-02-25 23:31:55,950:INFO:    plotly-resampler: Not installed
2025-02-25 23:31:55,950:INFO:             kaleido: 0.2.1
2025-02-25 23:31:55,950:INFO:           schemdraw: 0.15
2025-02-25 23:31:55,950:INFO:         statsmodels: 0.14.4
2025-02-25 23:31:55,950:INFO:              sktime: 0.26.0
2025-02-25 23:31:55,950:INFO:               tbats: 1.1.3
2025-02-25 23:31:55,950:INFO:            pmdarima: 2.0.4
2025-02-25 23:31:55,950:INFO:              psutil: 6.1.0
2025-02-25 23:31:55,950:INFO:          markupsafe: 2.1.5
2025-02-25 23:31:55,950:INFO:             pickle5: Not installed
2025-02-25 23:31:55,950:INFO:         cloudpickle: 3.1.0
2025-02-25 23:31:55,950:INFO:         deprecation: 2.1.0
2025-02-25 23:31:55,950:INFO:              xxhash: 3.5.0
2025-02-25 23:31:55,950:INFO:           wurlitzer: 3.1.1
2025-02-25 23:31:55,950:INFO:PyCaret optional dependencies:
2025-02-25 23:31:57,335:INFO:                shap: 0.44.1
2025-02-25 23:31:57,335:INFO:           interpret: 0.6.9
2025-02-25 23:31:57,335:INFO:                umap: 0.5.7
2025-02-25 23:31:57,335:INFO:     ydata_profiling: 4.12.1
2025-02-25 23:31:57,335:INFO:  explainerdashboard: 0.4.8
2025-02-25 23:31:57,335:INFO:             autoviz: Not installed
2025-02-25 23:31:57,335:INFO:           fairlearn: 0.7.0
2025-02-25 23:31:57,335:INFO:          deepchecks: Not installed
2025-02-25 23:31:57,335:INFO:             xgboost: 2.1.3
2025-02-25 23:31:57,335:INFO:            catboost: 1.1.1
2025-02-25 23:31:57,335:INFO:              kmodes: 0.12.2
2025-02-25 23:31:57,335:INFO:             mlxtend: 0.23.3
2025-02-25 23:31:57,335:INFO:       statsforecast: 1.5.0
2025-02-25 23:31:57,335:INFO:        tune_sklearn: 0.5.0
2025-02-25 23:31:57,335:INFO:                 ray: 2.40.0
2025-02-25 23:31:57,335:INFO:            hyperopt: 0.2.7
2025-02-25 23:31:57,335:INFO:              optuna: 4.1.0
2025-02-25 23:31:57,335:INFO:               skopt: 0.10.2
2025-02-25 23:31:57,335:INFO:              mlflow: 2.16.0
2025-02-25 23:31:57,335:INFO:              gradio: 5.12.0
2025-02-25 23:31:57,335:INFO:             fastapi: 0.115.6
2025-02-25 23:31:57,335:INFO:             uvicorn: 0.34.0
2025-02-25 23:31:57,335:INFO:              m2cgen: 0.10.0
2025-02-25 23:31:57,335:INFO:           evidently: 0.4.40
2025-02-25 23:31:57,335:INFO:               fugue: 0.8.7
2025-02-25 23:31:57,335:INFO:           streamlit: Not installed
2025-02-25 23:31:57,335:INFO:             prophet: Not installed
2025-02-25 23:31:57,335:INFO:None
2025-02-25 23:31:57,335:INFO:Set up data.
2025-02-25 23:31:57,356:INFO:Set up folding strategy.
2025-02-25 23:31:57,356:INFO:Set up train/test split.
2025-02-25 23:31:57,364:INFO:Set up index.
2025-02-25 23:31:57,364:INFO:Assigning column types.
2025-02-25 23:31:57,367:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-02-25 23:31:57,367:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2025-02-25 23:31:57,370:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-25 23:31:57,372:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-25 23:31:57,405:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:31:57,428:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-25 23:31:57,429:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:31:57,431:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:31:57,540:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2025-02-25 23:31:57,542:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-25 23:31:57,545:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-25 23:31:57,577:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:31:57,601:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-25 23:31:57,601:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:31:57,602:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:31:57,603:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2025-02-25 23:31:57,605:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-25 23:31:57,607:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-25 23:31:57,640:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:31:57,663:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-25 23:31:57,663:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:31:57,664:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:31:57,667:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-25 23:31:57,669:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-25 23:31:57,701:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:31:57,725:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-25 23:31:57,725:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:31:57,727:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:31:57,727:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2025-02-25 23:31:57,732:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-25 23:31:57,765:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:31:57,789:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-25 23:31:57,789:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:31:57,791:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:31:57,796:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-25 23:31:57,829:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:31:57,853:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-25 23:31:57,853:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:31:57,855:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:31:57,855:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2025-02-25 23:31:57,893:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:31:57,917:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-25 23:31:57,917:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:31:57,918:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:31:57,956:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:31:57,980:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-25 23:31:57,980:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:31:57,981:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:31:57,982:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-02-25 23:31:58,018:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:31:58,042:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:31:58,043:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:31:58,080:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:31:58,103:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:31:58,104:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:31:58,105:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2025-02-25 23:31:58,165:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:31:58,167:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:31:58,228:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:31:58,229:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:31:58,232:INFO:Preparing preprocessing pipeline...
2025-02-25 23:31:58,232:INFO:Set up date feature engineering.
2025-02-25 23:31:58,232:INFO:Set up iterative imputation.
2025-02-25 23:31:58,293:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:31:58,294:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:31:58,321:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-25 23:31:58,339:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:31:58,340:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:31:58,357:INFO:Set up encoding of ordinal features.
2025-02-25 23:31:58,360:WARNING:The number of classes passed to feature Method in the ordinal_features parameter (11) don't match with the number of classes in the data (5).
2025-02-25 23:31:58,361:INFO:Set up encoding of categorical features.
2025-02-25 23:31:58,361:INFO:Set up polynomial features.
2025-02-25 23:31:58,361:INFO:Set up removing multicollinearity.
2025-02-25 23:31:58,361:INFO:Set up feature normalization.
2025-02-25 23:31:58,702:INFO:Finished creating preprocessing pipeline.
2025-02-25 23:31:58,744:INFO:Pipeline: Pipeline(memory=FastMemory(location=/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/joblib),
         steps=[('date_feature_extractor',
                 TransformerWrapper(include=['Date'],
                                    transformer=ExtractDateTimeFeatures())),
                ('iterative_imputer',
                 TransformerWrapper(transformer=IterativeImputer(cat_estimator=LGBMClassifier(n_jobs=-1,
                                                                                              random_state=123),
                                                                 cat_estimator_prepare_...
                                    transformer=TargetEncoder(cols=['Suburb',
                                                                    'Seller',
                                                                    'CouncilArea'],
                                                              handle_missing='return_nan'))),
                ('polynomial_features',
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('remove_multicollinearity',
                 TransformerWrapper(exclude=[],
                                    transformer=RemoveMulticollinearity(threshold=0.55))),
                ('normalize',
                 TransformerWrapper(transformer=StandardScaler()))])
2025-02-25 23:31:58,745:INFO:Creating final display dataframe.
2025-02-25 23:31:59,198:INFO:Setup _display_container:                         Description          Value
0                        Session id            123
1                            Target          Price
2                       Target type     Regression
3               Original data shape    (13580, 21)
4            Transformed data shape   (13580, 134)
5       Transformed train set shape    (9506, 134)
6        Transformed test set shape    (4074, 134)
7                   Ignore features              4
8                  Ordinal features              3
9                  Numeric features              8
10                    Date features              1
11             Categorical features              6
12         Rows with missing values          54.4%
13                       Preprocess           True
14                  Imputation type      iterative
15  Iterative imputation iterations              5
16        Numeric iterative imputer       lightgbm
17    Categorical iterative imputer       lightgbm
18         Maximum one-hot encoding             25
19                  Encoding method           None
20              Polynomial features           True
21                Polynomial degree              2
22         Remove multicollinearity           True
23      Multicollinearity threshold           0.55
24                        Normalize           True
25                 Normalize method         zscore
26                   Fold Generator          KFold
27                      Fold Number             10
28                         CPU Jobs             -1
29                          Use GPU          False
30                   Log Experiment   MlflowLogger
31                  Experiment Name  house_pricing
32                              USI           f8d7
2025-02-25 23:31:59,264:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:31:59,266:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:31:59,327:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:31:59,328:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:31:59,329:INFO:Logging experiment in loggers
2025-02-25 23:31:59,434:INFO:SubProcess save_model() called ==================================
2025-02-25 23:31:59,517:INFO:Initializing save_model()
2025-02-25 23:31:59,517:INFO:save_model(model=Pipeline(memory=FastMemory(location=/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/joblib),
         steps=[('date_feature_extractor',
                 TransformerWrapper(include=['Date'],
                                    transformer=ExtractDateTimeFeatures())),
                ('iterative_imputer',
                 TransformerWrapper(transformer=IterativeImputer(cat_estimator=LGBMClassifier(n_jobs=-1,
                                                                                              random_state=123),
                                                                 cat_estimator_prepare_...
                                    transformer=TargetEncoder(cols=['Suburb',
                                                                    'Seller',
                                                                    'CouncilArea'],
                                                              handle_missing='return_nan'))),
                ('polynomial_features',
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('remove_multicollinearity',
                 TransformerWrapper(exclude=[],
                                    transformer=RemoveMulticollinearity(threshold=0.55))),
                ('normalize',
                 TransformerWrapper(transformer=StandardScaler()))]), model_name=/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/tmpsj4prmn6/Transformation Pipeline, prep_pipe_=Pipeline(memory=FastMemory(location=/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/joblib),
         steps=[('date_feature_extractor',
                 TransformerWrapper(include=['Date'],
                                    transformer=ExtractDateTimeFeatures())),
                ('iterative_imputer',
                 TransformerWrapper(transformer=IterativeImputer(cat_estimator=LGBMClassifier(n_jobs=-1,
                                                                                              random_state=123),
                                                                 cat_estimator_prepare_...
                                    transformer=TargetEncoder(cols=['Suburb',
                                                                    'Seller',
                                                                    'CouncilArea'],
                                                              handle_missing='return_nan'))),
                ('polynomial_features',
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('remove_multicollinearity',
                 TransformerWrapper(exclude=[],
                                    transformer=RemoveMulticollinearity(threshold=0.55))),
                ('normalize',
                 TransformerWrapper(transformer=StandardScaler()))]), verbose=False, use_case=MLUsecase.REGRESSION, kwargs={})
2025-02-25 23:31:59,517:INFO:Adding model into prep_pipe
2025-02-25 23:31:59,517:WARNING:Only Model saved as it was a pipeline.
2025-02-25 23:31:59,602:INFO:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/tmpsj4prmn6/Transformation Pipeline.pkl saved in current working directory
2025-02-25 23:31:59,643:INFO:Pipeline(memory=FastMemory(location=/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/joblib),
         steps=[('date_feature_extractor',
                 TransformerWrapper(include=['Date'],
                                    transformer=ExtractDateTimeFeatures())),
                ('iterative_imputer',
                 TransformerWrapper(transformer=IterativeImputer(cat_estimator=LGBMClassifier(n_jobs=-1,
                                                                                              random_state=123),
                                                                 cat_estimator_prepare_...
                                    transformer=TargetEncoder(cols=['Suburb',
                                                                    'Seller',
                                                                    'CouncilArea'],
                                                              handle_missing='return_nan'))),
                ('polynomial_features',
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('remove_multicollinearity',
                 TransformerWrapper(exclude=[],
                                    transformer=RemoveMulticollinearity(threshold=0.55))),
                ('normalize',
                 TransformerWrapper(transformer=StandardScaler()))])
2025-02-25 23:31:59,643:INFO:save_model() successfully completed......................................
2025-02-25 23:31:59,744:INFO:SubProcess save_model() end ==================================
2025-02-25 23:31:59,749:INFO:setup() successfully completed in 3.79s...............
2025-02-25 23:32:00,017:INFO:Initializing compare_models()
2025-02-25 23:32:00,017:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x2924569e0>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x2924569e0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2025-02-25 23:32:00,017:INFO:Checking exceptions
2025-02-25 23:32:00,022:INFO:Preparing display monitor
2025-02-25 23:32:00,055:INFO:Initializing Linear Regression
2025-02-25 23:32:00,055:INFO:Total runtime is 4.6173731486002605e-06 minutes
2025-02-25 23:32:00,058:INFO:SubProcess create_model() called ==================================
2025-02-25 23:32:00,059:INFO:Initializing create_model()
2025-02-25 23:32:00,059:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x2924569e0>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x290f924d0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-25 23:32:00,059:INFO:Checking exceptions
2025-02-25 23:32:00,059:INFO:Importing libraries
2025-02-25 23:32:00,059:INFO:Copying training dataset
2025-02-25 23:32:00,069:INFO:Defining folds
2025-02-25 23:32:00,069:INFO:Declaring metric variables
2025-02-25 23:32:00,071:INFO:Importing untrained model
2025-02-25 23:32:00,073:INFO:Linear Regression Imported successfully
2025-02-25 23:32:00,076:INFO:Starting cross validation
2025-02-25 23:32:00,150:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-25 23:32:42,545:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-25 23:32:42,891:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-25 23:32:43,061:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-25 23:32:43,112:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-25 23:32:43,326:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-25 23:32:43,326:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-25 23:32:43,733:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-25 23:32:43,782:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-25 23:36:09,968:INFO:PyCaret RegressionExperiment
2025-02-25 23:36:09,968:INFO:Logging name: house_pricing
2025-02-25 23:36:09,968:INFO:ML Usecase: MLUsecase.REGRESSION
2025-02-25 23:36:09,968:INFO:version 3.3.2
2025-02-25 23:36:09,968:INFO:Initializing setup()
2025-02-25 23:36:09,968:INFO:self.USI: 1bbb
2025-02-25 23:36:09,968:INFO:self._variable_keys: {'X', 'n_jobs_param', 'gpu_param', 'html_param', 'X_test', 'seed', 'y', 'exp_id', 'fold_groups_param', 'pipeline', 'y_train', 'X_train', 'logging_param', 'USI', 'fold_generator', 'memory', 'y_test', 'idx', 'log_plots_param', '_ml_usecase', 'transform_target_param', 'gpu_n_jobs_param', 'exp_name_log', '_available_plots', 'data', 'target_param', 'fold_shuffle_param'}
2025-02-25 23:36:09,968:INFO:Checking environment
2025-02-25 23:36:09,968:INFO:python_version: 3.10.15
2025-02-25 23:36:09,968:INFO:python_build: ('main', 'Oct  3 2024 02:24:49')
2025-02-25 23:36:09,968:INFO:machine: arm64
2025-02-25 23:36:09,968:INFO:platform: macOS-14.1-arm64-arm-64bit
2025-02-25 23:36:09,968:INFO:Memory: svmem(total=17179869184, available=6217711616, percent=63.8, used=6278234112, free=1859436544, active=4354506752, inactive=4329291776, wired=1923727360)
2025-02-25 23:36:09,968:INFO:Physical Core: 8
2025-02-25 23:36:09,968:INFO:Logical Core: 8
2025-02-25 23:36:09,968:INFO:Checking libraries
2025-02-25 23:36:09,968:INFO:System:
2025-02-25 23:36:09,968:INFO:    python: 3.10.15 (main, Oct  3 2024, 02:24:49) [Clang 14.0.6 ]
2025-02-25 23:36:09,969:INFO:executable: /Users/daaa/opt/miniconda3/envs/mlops/bin/python
2025-02-25 23:36:09,969:INFO:   machine: macOS-14.1-arm64-arm-64bit
2025-02-25 23:36:09,969:INFO:PyCaret required dependencies:
2025-02-25 23:36:09,969:INFO:                 pip: 24.2
2025-02-25 23:36:09,969:INFO:          setuptools: 75.1.0
2025-02-25 23:36:09,969:INFO:             pycaret: 3.3.2
2025-02-25 23:36:09,969:INFO:             IPython: 8.30.0
2025-02-25 23:36:09,969:INFO:          ipywidgets: 8.1.5
2025-02-25 23:36:09,969:INFO:                tqdm: 4.67.1
2025-02-25 23:36:09,969:INFO:               numpy: 1.26.4
2025-02-25 23:36:09,969:INFO:              pandas: 2.1.4
2025-02-25 23:36:09,969:INFO:              jinja2: 3.1.4
2025-02-25 23:36:09,969:INFO:               scipy: 1.11.4
2025-02-25 23:36:09,969:INFO:              joblib: 1.3.2
2025-02-25 23:36:09,969:INFO:             sklearn: 1.4.2
2025-02-25 23:36:09,969:INFO:                pyod: 2.0.2
2025-02-25 23:36:09,969:INFO:            imblearn: 0.12.4
2025-02-25 23:36:09,969:INFO:   category_encoders: 2.6.4
2025-02-25 23:36:09,969:INFO:            lightgbm: 4.5.0
2025-02-25 23:36:09,969:INFO:               numba: 0.60.0
2025-02-25 23:36:09,969:INFO:            requests: 2.32.3
2025-02-25 23:36:09,969:INFO:          matplotlib: 3.10.0
2025-02-25 23:36:09,969:INFO:          scikitplot: 0.3.7
2025-02-25 23:36:09,969:INFO:         yellowbrick: 1.5
2025-02-25 23:36:09,969:INFO:              plotly: 5.24.1
2025-02-25 23:36:09,969:INFO:    plotly-resampler: Not installed
2025-02-25 23:36:09,969:INFO:             kaleido: 0.2.1
2025-02-25 23:36:09,969:INFO:           schemdraw: 0.15
2025-02-25 23:36:09,969:INFO:         statsmodels: 0.14.4
2025-02-25 23:36:09,969:INFO:              sktime: 0.26.0
2025-02-25 23:36:09,969:INFO:               tbats: 1.1.3
2025-02-25 23:36:09,969:INFO:            pmdarima: 2.0.4
2025-02-25 23:36:09,969:INFO:              psutil: 6.1.0
2025-02-25 23:36:09,969:INFO:          markupsafe: 2.1.5
2025-02-25 23:36:09,969:INFO:             pickle5: Not installed
2025-02-25 23:36:09,969:INFO:         cloudpickle: 3.1.0
2025-02-25 23:36:09,969:INFO:         deprecation: 2.1.0
2025-02-25 23:36:09,969:INFO:              xxhash: 3.5.0
2025-02-25 23:36:09,969:INFO:           wurlitzer: 3.1.1
2025-02-25 23:36:09,969:INFO:PyCaret optional dependencies:
2025-02-25 23:36:09,969:INFO:                shap: 0.44.1
2025-02-25 23:36:09,969:INFO:           interpret: 0.6.9
2025-02-25 23:36:09,969:INFO:                umap: 0.5.7
2025-02-25 23:36:09,969:INFO:     ydata_profiling: 4.12.1
2025-02-25 23:36:09,969:INFO:  explainerdashboard: 0.4.8
2025-02-25 23:36:09,969:INFO:             autoviz: Not installed
2025-02-25 23:36:09,969:INFO:           fairlearn: 0.7.0
2025-02-25 23:36:09,969:INFO:          deepchecks: Not installed
2025-02-25 23:36:09,970:INFO:             xgboost: 2.1.3
2025-02-25 23:36:09,970:INFO:            catboost: 1.1.1
2025-02-25 23:36:09,970:INFO:              kmodes: 0.12.2
2025-02-25 23:36:09,970:INFO:             mlxtend: 0.23.3
2025-02-25 23:36:09,970:INFO:       statsforecast: 1.5.0
2025-02-25 23:36:09,970:INFO:        tune_sklearn: 0.5.0
2025-02-25 23:36:09,970:INFO:                 ray: 2.40.0
2025-02-25 23:36:09,970:INFO:            hyperopt: 0.2.7
2025-02-25 23:36:09,970:INFO:              optuna: 4.1.0
2025-02-25 23:36:09,970:INFO:               skopt: 0.10.2
2025-02-25 23:36:09,970:INFO:              mlflow: 2.16.0
2025-02-25 23:36:09,970:INFO:              gradio: 5.12.0
2025-02-25 23:36:09,970:INFO:             fastapi: 0.115.6
2025-02-25 23:36:09,970:INFO:             uvicorn: 0.34.0
2025-02-25 23:36:09,970:INFO:              m2cgen: 0.10.0
2025-02-25 23:36:09,970:INFO:           evidently: 0.4.40
2025-02-25 23:36:09,970:INFO:               fugue: 0.8.7
2025-02-25 23:36:09,970:INFO:           streamlit: Not installed
2025-02-25 23:36:09,970:INFO:             prophet: Not installed
2025-02-25 23:36:09,970:INFO:None
2025-02-25 23:36:09,970:INFO:Set up data.
2025-02-25 23:36:09,996:INFO:Set up folding strategy.
2025-02-25 23:36:09,996:INFO:Set up train/test split.
2025-02-25 23:36:10,006:INFO:Set up index.
2025-02-25 23:36:10,006:INFO:Assigning column types.
2025-02-25 23:36:10,011:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-02-25 23:36:10,011:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2025-02-25 23:36:10,013:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-25 23:36:10,016:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-25 23:36:10,049:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:36:10,074:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-25 23:36:10,074:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:36:10,076:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:36:10,076:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2025-02-25 23:36:10,079:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-25 23:36:10,081:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-25 23:36:10,114:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:36:10,137:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-25 23:36:10,137:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:36:10,139:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:36:10,139:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2025-02-25 23:36:10,142:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-25 23:36:10,144:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-25 23:36:10,177:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:36:10,200:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-25 23:36:10,200:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:36:10,202:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:36:10,204:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-25 23:36:10,207:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-25 23:36:10,239:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:36:10,262:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-25 23:36:10,262:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:36:10,263:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:36:10,264:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2025-02-25 23:36:10,268:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-25 23:36:10,300:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:36:10,323:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-25 23:36:10,323:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:36:10,325:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:36:10,330:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-25 23:36:10,361:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:36:10,385:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-25 23:36:10,385:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:36:10,386:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:36:10,386:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2025-02-25 23:36:10,424:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:36:10,447:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-25 23:36:10,448:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:36:10,449:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:36:10,487:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:36:10,511:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-25 23:36:10,512:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:36:10,513:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:36:10,513:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-02-25 23:36:10,551:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:36:10,575:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:36:10,577:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:36:10,615:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:36:10,640:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:36:10,641:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:36:10,642:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2025-02-25 23:36:10,704:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:36:10,705:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:36:10,767:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:36:10,768:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:36:10,769:INFO:Preparing preprocessing pipeline...
2025-02-25 23:36:10,769:INFO:Set up date feature engineering.
2025-02-25 23:36:10,769:INFO:Set up iterative imputation.
2025-02-25 23:36:10,832:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:36:10,833:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:36:10,858:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-25 23:36:10,873:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:36:10,874:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:36:10,893:INFO:Set up encoding of ordinal features.
2025-02-25 23:36:10,897:WARNING:The number of classes passed to feature Method in the ordinal_features parameter (11) don't match with the number of classes in the data (5).
2025-02-25 23:36:10,897:INFO:Set up encoding of categorical features.
2025-02-25 23:36:10,897:INFO:Set up polynomial features.
2025-02-25 23:36:10,897:INFO:Set up removing multicollinearity.
2025-02-25 23:36:10,897:INFO:Set up feature normalization.
2025-02-25 23:36:10,897:INFO:Set up feature selection.
2025-02-25 23:36:10,962:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:36:10,963:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:36:11,326:INFO:Finished creating preprocessing pipeline.
2025-02-25 23:36:11,368:INFO:Pipeline: Pipeline(memory=FastMemory(location=/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/joblib),
         steps=[('date_feature_extractor',
                 TransformerWrapper(include=['Date'],
                                    transformer=ExtractDateTimeFeatures())),
                ('iterative_imputer',
                 TransformerWrapper(transformer=IterativeImputer(cat_estimator=LGBMClassifier(n_jobs=-1,
                                                                                              random_state=123),
                                                                 cat_estimator_prepare_...
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('remove_multicollinearity',
                 TransformerWrapper(exclude=[],
                                    transformer=RemoveMulticollinearity(threshold=0.55))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('feature_selection',
                 TransformerWrapper(exclude=[],
                                    transformer=SelectFromModel(estimator=LGBMRegressor(),
                                                                max_features=12,
                                                                threshold=-inf)))])
2025-02-25 23:36:11,368:INFO:Creating final display dataframe.
2025-02-25 23:40:29,225:INFO:PyCaret RegressionExperiment
2025-02-25 23:40:29,225:INFO:Logging name: house_pricing
2025-02-25 23:40:29,225:INFO:ML Usecase: MLUsecase.REGRESSION
2025-02-25 23:40:29,225:INFO:version 3.3.2
2025-02-25 23:40:29,225:INFO:Initializing setup()
2025-02-25 23:40:29,225:INFO:self.USI: 6d2d
2025-02-25 23:40:29,225:INFO:self._variable_keys: {'X', 'n_jobs_param', 'gpu_param', 'html_param', 'X_test', 'seed', 'y', 'exp_id', 'fold_groups_param', 'pipeline', 'y_train', 'X_train', 'logging_param', 'USI', 'fold_generator', 'memory', 'y_test', 'idx', 'log_plots_param', '_ml_usecase', 'transform_target_param', 'gpu_n_jobs_param', 'exp_name_log', '_available_plots', 'data', 'target_param', 'fold_shuffle_param'}
2025-02-25 23:40:29,225:INFO:Checking environment
2025-02-25 23:40:29,225:INFO:python_version: 3.10.15
2025-02-25 23:40:29,225:INFO:python_build: ('main', 'Oct  3 2024 02:24:49')
2025-02-25 23:40:29,225:INFO:machine: arm64
2025-02-25 23:40:29,226:INFO:platform: macOS-14.1-arm64-arm-64bit
2025-02-25 23:40:29,226:INFO:Memory: svmem(total=17179869184, available=5829099520, percent=66.1, used=6915375104, free=844791808, active=5002690560, inactive=4893917184, wired=1912684544)
2025-02-25 23:40:29,226:INFO:Physical Core: 8
2025-02-25 23:40:29,226:INFO:Logical Core: 8
2025-02-25 23:40:29,226:INFO:Checking libraries
2025-02-25 23:40:29,226:INFO:System:
2025-02-25 23:40:29,226:INFO:    python: 3.10.15 (main, Oct  3 2024, 02:24:49) [Clang 14.0.6 ]
2025-02-25 23:40:29,226:INFO:executable: /Users/daaa/opt/miniconda3/envs/mlops/bin/python
2025-02-25 23:40:29,226:INFO:   machine: macOS-14.1-arm64-arm-64bit
2025-02-25 23:40:29,226:INFO:PyCaret required dependencies:
2025-02-25 23:40:29,226:INFO:                 pip: 24.2
2025-02-25 23:40:29,226:INFO:          setuptools: 75.1.0
2025-02-25 23:40:29,226:INFO:             pycaret: 3.3.2
2025-02-25 23:40:29,226:INFO:             IPython: 8.30.0
2025-02-25 23:40:29,226:INFO:          ipywidgets: 8.1.5
2025-02-25 23:40:29,226:INFO:                tqdm: 4.67.1
2025-02-25 23:40:29,226:INFO:               numpy: 1.26.4
2025-02-25 23:40:29,226:INFO:              pandas: 2.1.4
2025-02-25 23:40:29,226:INFO:              jinja2: 3.1.4
2025-02-25 23:40:29,226:INFO:               scipy: 1.11.4
2025-02-25 23:40:29,226:INFO:              joblib: 1.3.2
2025-02-25 23:40:29,226:INFO:             sklearn: 1.4.2
2025-02-25 23:40:29,226:INFO:                pyod: 2.0.2
2025-02-25 23:40:29,226:INFO:            imblearn: 0.12.4
2025-02-25 23:40:29,226:INFO:   category_encoders: 2.6.4
2025-02-25 23:40:29,226:INFO:            lightgbm: 4.5.0
2025-02-25 23:40:29,226:INFO:               numba: 0.60.0
2025-02-25 23:40:29,226:INFO:            requests: 2.32.3
2025-02-25 23:40:29,226:INFO:          matplotlib: 3.10.0
2025-02-25 23:40:29,227:INFO:          scikitplot: 0.3.7
2025-02-25 23:40:29,227:INFO:         yellowbrick: 1.5
2025-02-25 23:40:29,227:INFO:              plotly: 5.24.1
2025-02-25 23:40:29,227:INFO:    plotly-resampler: Not installed
2025-02-25 23:40:29,227:INFO:             kaleido: 0.2.1
2025-02-25 23:40:29,227:INFO:           schemdraw: 0.15
2025-02-25 23:40:29,227:INFO:         statsmodels: 0.14.4
2025-02-25 23:40:29,227:INFO:              sktime: 0.26.0
2025-02-25 23:40:29,227:INFO:               tbats: 1.1.3
2025-02-25 23:40:29,227:INFO:            pmdarima: 2.0.4
2025-02-25 23:40:29,227:INFO:              psutil: 6.1.0
2025-02-25 23:40:29,227:INFO:          markupsafe: 2.1.5
2025-02-25 23:40:29,227:INFO:             pickle5: Not installed
2025-02-25 23:40:29,227:INFO:         cloudpickle: 3.1.0
2025-02-25 23:40:29,227:INFO:         deprecation: 2.1.0
2025-02-25 23:40:29,227:INFO:              xxhash: 3.5.0
2025-02-25 23:40:29,227:INFO:           wurlitzer: 3.1.1
2025-02-25 23:40:29,227:INFO:PyCaret optional dependencies:
2025-02-25 23:40:29,227:INFO:                shap: 0.44.1
2025-02-25 23:40:29,227:INFO:           interpret: 0.6.9
2025-02-25 23:40:29,227:INFO:                umap: 0.5.7
2025-02-25 23:40:29,227:INFO:     ydata_profiling: 4.12.1
2025-02-25 23:40:29,227:INFO:  explainerdashboard: 0.4.8
2025-02-25 23:40:29,227:INFO:             autoviz: Not installed
2025-02-25 23:40:29,227:INFO:           fairlearn: 0.7.0
2025-02-25 23:40:29,227:INFO:          deepchecks: Not installed
2025-02-25 23:40:29,227:INFO:             xgboost: 2.1.3
2025-02-25 23:40:29,228:INFO:            catboost: 1.1.1
2025-02-25 23:40:29,228:INFO:              kmodes: 0.12.2
2025-02-25 23:40:29,228:INFO:             mlxtend: 0.23.3
2025-02-25 23:40:29,228:INFO:       statsforecast: 1.5.0
2025-02-25 23:40:29,228:INFO:        tune_sklearn: 0.5.0
2025-02-25 23:40:29,228:INFO:                 ray: 2.40.0
2025-02-25 23:40:29,228:INFO:            hyperopt: 0.2.7
2025-02-25 23:40:29,228:INFO:              optuna: 4.1.0
2025-02-25 23:40:29,228:INFO:               skopt: 0.10.2
2025-02-25 23:40:29,228:INFO:              mlflow: 2.16.0
2025-02-25 23:40:29,228:INFO:              gradio: 5.12.0
2025-02-25 23:40:29,228:INFO:             fastapi: 0.115.6
2025-02-25 23:40:29,228:INFO:             uvicorn: 0.34.0
2025-02-25 23:40:29,228:INFO:              m2cgen: 0.10.0
2025-02-25 23:40:29,228:INFO:           evidently: 0.4.40
2025-02-25 23:40:29,228:INFO:               fugue: 0.8.7
2025-02-25 23:40:29,228:INFO:           streamlit: Not installed
2025-02-25 23:40:29,228:INFO:             prophet: Not installed
2025-02-25 23:40:29,228:INFO:None
2025-02-25 23:40:29,228:INFO:Set up data.
2025-02-25 23:40:29,251:INFO:Set up folding strategy.
2025-02-25 23:40:29,251:INFO:Set up train/test split.
2025-02-25 23:40:29,260:INFO:Set up index.
2025-02-25 23:40:29,260:INFO:Assigning column types.
2025-02-25 23:40:29,265:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-02-25 23:40:29,265:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2025-02-25 23:40:29,268:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-25 23:40:29,270:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-25 23:40:29,304:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:40:29,327:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-25 23:40:29,327:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:40:29,329:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:40:29,329:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2025-02-25 23:40:29,332:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-25 23:40:29,334:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-25 23:40:29,367:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:40:29,390:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-25 23:40:29,391:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:40:29,392:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:40:29,392:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2025-02-25 23:40:29,395:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-25 23:40:29,397:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-25 23:40:29,430:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:40:29,454:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-25 23:40:29,454:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:40:29,455:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:40:29,458:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-25 23:40:29,460:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-25 23:40:29,492:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:40:29,515:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-25 23:40:29,515:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:40:29,517:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:40:29,517:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2025-02-25 23:40:29,522:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-25 23:40:29,553:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:40:29,576:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-25 23:40:29,576:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:40:29,577:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:40:29,582:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-25 23:40:29,614:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:40:29,637:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-25 23:40:29,637:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:40:29,638:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:40:29,638:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2025-02-25 23:40:29,675:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:40:29,697:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-25 23:40:29,697:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:40:29,699:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:40:29,735:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:40:29,758:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-25 23:40:29,758:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:40:29,759:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:40:29,760:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-02-25 23:40:29,797:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:40:29,821:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:40:29,822:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:40:29,860:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:40:29,883:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:40:29,885:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:40:29,885:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2025-02-25 23:40:29,946:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:40:29,947:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:40:30,008:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:40:30,009:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:40:30,010:INFO:Preparing preprocessing pipeline...
2025-02-25 23:40:30,010:INFO:Set up date feature engineering.
2025-02-25 23:40:30,010:INFO:Set up iterative imputation.
2025-02-25 23:40:30,070:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:40:30,072:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:40:30,095:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-25 23:40:30,109:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:40:30,111:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:40:30,127:INFO:Set up encoding of ordinal features.
2025-02-25 23:40:30,134:WARNING:The number of classes passed to feature Method in the ordinal_features parameter (11) don't match with the number of classes in the data (5).
2025-02-25 23:40:30,134:INFO:Set up encoding of categorical features.
2025-02-25 23:40:30,134:INFO:Set up polynomial features.
2025-02-25 23:40:30,134:INFO:Set up removing multicollinearity.
2025-02-25 23:40:30,134:INFO:Set up feature normalization.
2025-02-25 23:40:30,134:INFO:Set up feature selection.
2025-02-25 23:40:30,193:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:40:30,195:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:40:44,833:INFO:Finished creating preprocessing pipeline.
2025-02-25 23:40:44,876:INFO:Pipeline: Pipeline(memory=FastMemory(location=/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/joblib),
         steps=[('date_feature_extractor',
                 TransformerWrapper(include=['Date'],
                                    transformer=ExtractDateTimeFeatures())),
                ('iterative_imputer',
                 TransformerWrapper(transformer=IterativeImputer(cat_estimator=LGBMClassifier(n_jobs=-1,
                                                                                              random_state=123),
                                                                 cat_estimator_prepare_...
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('remove_multicollinearity',
                 TransformerWrapper(exclude=[],
                                    transformer=RemoveMulticollinearity(threshold=0.55))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('feature_selection',
                 TransformerWrapper(exclude=[],
                                    transformer=SelectFromModel(estimator=RandomForestRegressor(),
                                                                max_features=12,
                                                                threshold=-inf)))])
2025-02-25 23:40:44,876:INFO:Creating final display dataframe.
2025-02-25 23:40:46,345:INFO:Setup _display_container:                         Description          Value
0                        Session id            123
1                            Target          Price
2                       Target type     Regression
3               Original data shape    (13580, 21)
4            Transformed data shape    (13580, 13)
5       Transformed train set shape     (9506, 13)
6        Transformed test set shape     (4074, 13)
7                   Ignore features              4
8                  Ordinal features              3
9                  Numeric features              8
10                    Date features              1
11             Categorical features              6
12         Rows with missing values          54.4%
13                       Preprocess           True
14                  Imputation type      iterative
15  Iterative imputation iterations              5
16        Numeric iterative imputer       lightgbm
17    Categorical iterative imputer       lightgbm
18         Maximum one-hot encoding             25
19                  Encoding method           None
20              Polynomial features           True
21                Polynomial degree              2
22         Remove multicollinearity           True
23      Multicollinearity threshold           0.55
24                        Normalize           True
25                 Normalize method         zscore
26                Feature selection           True
27         Feature selection method        classic
28      Feature selection estimator             rf
29      Number of features selected            0.8
30                   Fold Generator          KFold
31                      Fold Number             10
32                         CPU Jobs             -1
33                          Use GPU          False
34                   Log Experiment   MlflowLogger
35                  Experiment Name  house_pricing
36                              USI           6d2d
2025-02-25 23:40:46,411:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:40:46,413:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:40:46,473:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:40:46,474:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:40:46,475:INFO:Logging experiment in loggers
2025-02-25 23:40:46,492:INFO:SubProcess save_model() called ==================================
2025-02-25 23:40:46,576:INFO:Initializing save_model()
2025-02-25 23:40:46,576:INFO:save_model(model=Pipeline(memory=FastMemory(location=/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/joblib),
         steps=[('date_feature_extractor',
                 TransformerWrapper(include=['Date'],
                                    transformer=ExtractDateTimeFeatures())),
                ('iterative_imputer',
                 TransformerWrapper(transformer=IterativeImputer(cat_estimator=LGBMClassifier(n_jobs=-1,
                                                                                              random_state=123),
                                                                 cat_estimator_prepare_...
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('remove_multicollinearity',
                 TransformerWrapper(exclude=[],
                                    transformer=RemoveMulticollinearity(threshold=0.55))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('feature_selection',
                 TransformerWrapper(exclude=[],
                                    transformer=SelectFromModel(estimator=RandomForestRegressor(),
                                                                max_features=12,
                                                                threshold=-inf)))]), model_name=/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/tmpeutct6jf/Transformation Pipeline, prep_pipe_=Pipeline(memory=FastMemory(location=/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/joblib),
         steps=[('date_feature_extractor',
                 TransformerWrapper(include=['Date'],
                                    transformer=ExtractDateTimeFeatures())),
                ('iterative_imputer',
                 TransformerWrapper(transformer=IterativeImputer(cat_estimator=LGBMClassifier(n_jobs=-1,
                                                                                              random_state=123),
                                                                 cat_estimator_prepare_...
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('remove_multicollinearity',
                 TransformerWrapper(exclude=[],
                                    transformer=RemoveMulticollinearity(threshold=0.55))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('feature_selection',
                 TransformerWrapper(exclude=[],
                                    transformer=SelectFromModel(estimator=RandomForestRegressor(),
                                                                max_features=12,
                                                                threshold=-inf)))]), verbose=False, use_case=MLUsecase.REGRESSION, kwargs={})
2025-02-25 23:40:46,576:INFO:Adding model into prep_pipe
2025-02-25 23:40:46,576:WARNING:Only Model saved as it was a pipeline.
2025-02-25 23:40:46,692:INFO:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/tmpeutct6jf/Transformation Pipeline.pkl saved in current working directory
2025-02-25 23:40:46,735:INFO:Pipeline(memory=FastMemory(location=/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/joblib),
         steps=[('date_feature_extractor',
                 TransformerWrapper(include=['Date'],
                                    transformer=ExtractDateTimeFeatures())),
                ('iterative_imputer',
                 TransformerWrapper(transformer=IterativeImputer(cat_estimator=LGBMClassifier(n_jobs=-1,
                                                                                              random_state=123),
                                                                 cat_estimator_prepare_...
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('remove_multicollinearity',
                 TransformerWrapper(exclude=[],
                                    transformer=RemoveMulticollinearity(threshold=0.55))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('feature_selection',
                 TransformerWrapper(exclude=[],
                                    transformer=SelectFromModel(estimator=RandomForestRegressor(),
                                                                max_features=12,
                                                                threshold=-inf)))])
2025-02-25 23:40:46,735:INFO:save_model() successfully completed......................................
2025-02-25 23:40:46,885:INFO:SubProcess save_model() end ==================================
2025-02-25 23:40:46,917:INFO:setup() successfully completed in 17.26s...............
2025-02-25 23:41:09,848:INFO:PyCaret RegressionExperiment
2025-02-25 23:41:09,849:INFO:Logging name: house_pricing
2025-02-25 23:41:09,849:INFO:ML Usecase: MLUsecase.REGRESSION
2025-02-25 23:41:09,849:INFO:version 3.3.2
2025-02-25 23:41:09,849:INFO:Initializing setup()
2025-02-25 23:41:09,849:INFO:self.USI: 0812
2025-02-25 23:41:09,849:INFO:self._variable_keys: {'X', 'n_jobs_param', 'gpu_param', 'html_param', 'X_test', 'seed', 'y', 'exp_id', 'fold_groups_param', 'pipeline', 'y_train', 'X_train', 'logging_param', 'USI', 'fold_generator', 'memory', 'y_test', 'idx', 'log_plots_param', '_ml_usecase', 'transform_target_param', 'gpu_n_jobs_param', 'exp_name_log', '_available_plots', 'data', 'target_param', 'fold_shuffle_param'}
2025-02-25 23:41:09,849:INFO:Checking environment
2025-02-25 23:41:09,849:INFO:python_version: 3.10.15
2025-02-25 23:41:09,849:INFO:python_build: ('main', 'Oct  3 2024 02:24:49')
2025-02-25 23:41:09,849:INFO:machine: arm64
2025-02-25 23:41:09,849:INFO:platform: macOS-14.1-arm64-arm-64bit
2025-02-25 23:41:09,849:INFO:Memory: svmem(total=17179869184, available=5576048640, percent=67.5, used=7171342336, free=336822272, active=5249171456, inactive=4971970560, wired=1922170880)
2025-02-25 23:41:09,849:INFO:Physical Core: 8
2025-02-25 23:41:09,849:INFO:Logical Core: 8
2025-02-25 23:41:09,849:INFO:Checking libraries
2025-02-25 23:41:09,849:INFO:System:
2025-02-25 23:41:09,850:INFO:    python: 3.10.15 (main, Oct  3 2024, 02:24:49) [Clang 14.0.6 ]
2025-02-25 23:41:09,850:INFO:executable: /Users/daaa/opt/miniconda3/envs/mlops/bin/python
2025-02-25 23:41:09,850:INFO:   machine: macOS-14.1-arm64-arm-64bit
2025-02-25 23:41:09,850:INFO:PyCaret required dependencies:
2025-02-25 23:41:09,850:INFO:                 pip: 24.2
2025-02-25 23:41:09,850:INFO:          setuptools: 75.1.0
2025-02-25 23:41:09,850:INFO:             pycaret: 3.3.2
2025-02-25 23:41:09,850:INFO:             IPython: 8.30.0
2025-02-25 23:41:09,850:INFO:          ipywidgets: 8.1.5
2025-02-25 23:41:09,850:INFO:                tqdm: 4.67.1
2025-02-25 23:41:09,850:INFO:               numpy: 1.26.4
2025-02-25 23:41:09,850:INFO:              pandas: 2.1.4
2025-02-25 23:41:09,850:INFO:              jinja2: 3.1.4
2025-02-25 23:41:09,850:INFO:               scipy: 1.11.4
2025-02-25 23:41:09,851:INFO:              joblib: 1.3.2
2025-02-25 23:41:09,851:INFO:             sklearn: 1.4.2
2025-02-25 23:41:09,851:INFO:                pyod: 2.0.2
2025-02-25 23:41:09,851:INFO:            imblearn: 0.12.4
2025-02-25 23:41:09,851:INFO:   category_encoders: 2.6.4
2025-02-25 23:41:09,851:INFO:            lightgbm: 4.5.0
2025-02-25 23:41:09,851:INFO:               numba: 0.60.0
2025-02-25 23:41:09,851:INFO:            requests: 2.32.3
2025-02-25 23:41:09,851:INFO:          matplotlib: 3.10.0
2025-02-25 23:41:09,851:INFO:          scikitplot: 0.3.7
2025-02-25 23:41:09,851:INFO:         yellowbrick: 1.5
2025-02-25 23:41:09,851:INFO:              plotly: 5.24.1
2025-02-25 23:41:09,851:INFO:    plotly-resampler: Not installed
2025-02-25 23:41:09,851:INFO:             kaleido: 0.2.1
2025-02-25 23:41:09,851:INFO:           schemdraw: 0.15
2025-02-25 23:41:09,851:INFO:         statsmodels: 0.14.4
2025-02-25 23:41:09,851:INFO:              sktime: 0.26.0
2025-02-25 23:41:09,851:INFO:               tbats: 1.1.3
2025-02-25 23:41:09,851:INFO:            pmdarima: 2.0.4
2025-02-25 23:41:09,851:INFO:              psutil: 6.1.0
2025-02-25 23:41:09,851:INFO:          markupsafe: 2.1.5
2025-02-25 23:41:09,851:INFO:             pickle5: Not installed
2025-02-25 23:41:09,851:INFO:         cloudpickle: 3.1.0
2025-02-25 23:41:09,851:INFO:         deprecation: 2.1.0
2025-02-25 23:41:09,851:INFO:              xxhash: 3.5.0
2025-02-25 23:41:09,851:INFO:           wurlitzer: 3.1.1
2025-02-25 23:41:09,851:INFO:PyCaret optional dependencies:
2025-02-25 23:41:09,851:INFO:                shap: 0.44.1
2025-02-25 23:41:09,851:INFO:           interpret: 0.6.9
2025-02-25 23:41:09,851:INFO:                umap: 0.5.7
2025-02-25 23:41:09,851:INFO:     ydata_profiling: 4.12.1
2025-02-25 23:41:09,851:INFO:  explainerdashboard: 0.4.8
2025-02-25 23:41:09,851:INFO:             autoviz: Not installed
2025-02-25 23:41:09,851:INFO:           fairlearn: 0.7.0
2025-02-25 23:41:09,851:INFO:          deepchecks: Not installed
2025-02-25 23:41:09,851:INFO:             xgboost: 2.1.3
2025-02-25 23:41:09,851:INFO:            catboost: 1.1.1
2025-02-25 23:41:09,851:INFO:              kmodes: 0.12.2
2025-02-25 23:41:09,851:INFO:             mlxtend: 0.23.3
2025-02-25 23:41:09,851:INFO:       statsforecast: 1.5.0
2025-02-25 23:41:09,851:INFO:        tune_sklearn: 0.5.0
2025-02-25 23:41:09,851:INFO:                 ray: 2.40.0
2025-02-25 23:41:09,851:INFO:            hyperopt: 0.2.7
2025-02-25 23:41:09,851:INFO:              optuna: 4.1.0
2025-02-25 23:41:09,851:INFO:               skopt: 0.10.2
2025-02-25 23:41:09,851:INFO:              mlflow: 2.16.0
2025-02-25 23:41:09,851:INFO:              gradio: 5.12.0
2025-02-25 23:41:09,851:INFO:             fastapi: 0.115.6
2025-02-25 23:41:09,851:INFO:             uvicorn: 0.34.0
2025-02-25 23:41:09,851:INFO:              m2cgen: 0.10.0
2025-02-25 23:41:09,851:INFO:           evidently: 0.4.40
2025-02-25 23:41:09,851:INFO:               fugue: 0.8.7
2025-02-25 23:41:09,851:INFO:           streamlit: Not installed
2025-02-25 23:41:09,851:INFO:             prophet: Not installed
2025-02-25 23:41:09,851:INFO:None
2025-02-25 23:41:09,851:INFO:Set up data.
2025-02-25 23:41:09,873:INFO:Set up folding strategy.
2025-02-25 23:41:09,873:INFO:Set up train/test split.
2025-02-25 23:41:09,881:INFO:Set up index.
2025-02-25 23:41:09,882:INFO:Assigning column types.
2025-02-25 23:41:09,886:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-02-25 23:41:09,886:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2025-02-25 23:41:09,889:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-25 23:41:09,891:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-25 23:41:09,924:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:41:09,947:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-25 23:41:09,947:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:41:09,949:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:41:09,949:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2025-02-25 23:41:09,952:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-25 23:41:09,954:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-25 23:41:09,987:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:41:10,011:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-25 23:41:10,011:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:41:10,012:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:41:10,013:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2025-02-25 23:41:10,015:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-25 23:41:10,017:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-25 23:41:10,050:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:41:10,074:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-25 23:41:10,074:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:41:10,075:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:41:10,078:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-25 23:41:10,080:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-25 23:41:10,112:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:41:10,135:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-25 23:41:10,136:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:41:10,137:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:41:10,137:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2025-02-25 23:41:10,142:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-25 23:41:10,173:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:41:10,196:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-25 23:41:10,196:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:41:10,198:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:41:10,203:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-25 23:41:10,236:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:41:10,260:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-25 23:41:10,260:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:41:10,262:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:41:10,262:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2025-02-25 23:41:10,300:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:41:10,324:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-25 23:41:10,324:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:41:10,326:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:41:10,363:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:41:10,387:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-25 23:41:10,387:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:41:10,389:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:41:10,389:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-02-25 23:41:10,427:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:41:10,450:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:41:10,452:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:41:10,490:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:41:10,516:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:41:10,518:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:41:10,518:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2025-02-25 23:41:10,584:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:41:10,585:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:41:10,648:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:41:10,649:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:41:10,650:INFO:Preparing preprocessing pipeline...
2025-02-25 23:41:10,650:INFO:Set up date feature engineering.
2025-02-25 23:41:10,650:INFO:Set up iterative imputation.
2025-02-25 23:41:10,711:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:41:10,713:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:41:10,737:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-25 23:41:10,752:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:41:10,753:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:41:10,772:INFO:Set up encoding of ordinal features.
2025-02-25 23:41:10,777:WARNING:The number of classes passed to feature Method in the ordinal_features parameter (11) don't match with the number of classes in the data (5).
2025-02-25 23:41:10,777:INFO:Set up encoding of categorical features.
2025-02-25 23:41:10,777:INFO:Set up polynomial features.
2025-02-25 23:41:10,777:INFO:Set up removing multicollinearity.
2025-02-25 23:41:10,777:INFO:Set up feature normalization.
2025-02-25 23:41:10,777:INFO:Set up feature selection.
2025-02-25 23:41:10,839:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:41:10,840:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:41:11,243:INFO:Finished creating preprocessing pipeline.
2025-02-25 23:41:11,288:INFO:Pipeline: Pipeline(memory=FastMemory(location=/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/joblib),
         steps=[('date_feature_extractor',
                 TransformerWrapper(include=['Date'],
                                    transformer=ExtractDateTimeFeatures())),
                ('iterative_imputer',
                 TransformerWrapper(transformer=IterativeImputer(cat_estimator=LGBMClassifier(n_jobs=-1,
                                                                                              random_state=123),
                                                                 cat_estimator_prepare_...
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('remove_multicollinearity',
                 TransformerWrapper(exclude=[],
                                    transformer=RemoveMulticollinearity(threshold=0.55))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('feature_selection',
                 TransformerWrapper(exclude=[],
                                    transformer=SelectFromModel(estimator=RandomForestRegressor(),
                                                                max_features=12,
                                                                threshold=-inf)))])
2025-02-25 23:41:11,288:INFO:Creating final display dataframe.
2025-02-25 23:41:12,814:INFO:Setup _display_container:                         Description          Value
0                        Session id            123
1                            Target          Price
2                       Target type     Regression
3               Original data shape    (13580, 21)
4            Transformed data shape    (13580, 13)
5       Transformed train set shape     (9506, 13)
6        Transformed test set shape     (4074, 13)
7                   Ignore features              4
8                  Ordinal features              3
9                  Numeric features              8
10                    Date features              1
11             Categorical features              6
12         Rows with missing values          54.4%
13                       Preprocess           True
14                  Imputation type      iterative
15  Iterative imputation iterations              5
16        Numeric iterative imputer       lightgbm
17    Categorical iterative imputer       lightgbm
18         Maximum one-hot encoding             25
19                  Encoding method           None
20              Polynomial features           True
21                Polynomial degree              2
22         Remove multicollinearity           True
23      Multicollinearity threshold           0.55
24                        Normalize           True
25                 Normalize method         zscore
26                Feature selection           True
27         Feature selection method        classic
28      Feature selection estimator             rf
29      Number of features selected            0.8
30                   Fold Generator          KFold
31                      Fold Number             10
32                         CPU Jobs             -1
33                          Use GPU          False
34                   Log Experiment   MlflowLogger
35                  Experiment Name  house_pricing
36                              USI           0812
2025-02-25 23:41:12,887:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:41:12,888:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:41:19,537:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-25 23:41:19,537:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-25 23:41:19,537:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-25 23:41:19,537:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-25 23:41:20,799:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_23970/2838843215.py:2: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.boxplot(data=df, x="Method", y="Price", palette="viridis")

2025-02-25 23:41:20,910:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_23970/2297183518.py:4: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.barplot(x=avg_price_by_seller.index, y=avg_price_by_seller.values, palette="viridis")

2025-02-25 23:41:22,126:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_23970/1639960328.py:2: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.boxplot(

2025-02-25 23:41:22,626:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_23970/2157425422.py:3: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.violinplot(data=df, x="Bedroom2", y="Price", palette="viridis", ax=axes[0])

2025-02-25 23:41:22,746:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_23970/2157425422.py:8: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.violinplot(data=df, x="Bathroom", y="Price", palette="viridis", ax=axes[1])

2025-02-25 23:41:22,846:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_23970/2157425422.py:13: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.violinplot(data=df, x="Car", y="Price", palette="viridis", ax=axes[2])

2025-02-25 23:41:24,101:INFO:PyCaret RegressionExperiment
2025-02-25 23:41:24,101:INFO:Logging name: house_pricing
2025-02-25 23:41:24,101:INFO:ML Usecase: MLUsecase.REGRESSION
2025-02-25 23:41:24,101:INFO:version 3.3.2
2025-02-25 23:41:24,101:INFO:Initializing setup()
2025-02-25 23:41:24,101:INFO:self.USI: e0f6
2025-02-25 23:41:24,101:INFO:self._variable_keys: {'X_test', 'USI', 'memory', 'y', 'log_plots_param', 'idx', 'transform_target_param', 'fold_generator', 'X_train', 'data', 'target_param', 'y_test', 'n_jobs_param', 'gpu_param', 'logging_param', 'y_train', 'gpu_n_jobs_param', 'pipeline', 'html_param', 'fold_groups_param', 'seed', 'fold_shuffle_param', 'X', 'exp_name_log', '_ml_usecase', '_available_plots', 'exp_id'}
2025-02-25 23:41:24,101:INFO:Checking environment
2025-02-25 23:41:24,101:INFO:python_version: 3.10.15
2025-02-25 23:41:24,101:INFO:python_build: ('main', 'Oct  3 2024 02:24:49')
2025-02-25 23:41:24,101:INFO:machine: arm64
2025-02-25 23:41:24,101:INFO:platform: macOS-14.1-arm64-arm-64bit
2025-02-25 23:41:24,101:INFO:Memory: svmem(total=17179869184, available=5787926528, percent=66.3, used=6971588608, free=785858560, active=5015732224, inactive=4699799552, wired=1955856384)
2025-02-25 23:41:24,101:INFO:Physical Core: 8
2025-02-25 23:41:24,101:INFO:Logical Core: 8
2025-02-25 23:41:24,101:INFO:Checking libraries
2025-02-25 23:41:24,101:INFO:System:
2025-02-25 23:41:24,101:INFO:    python: 3.10.15 (main, Oct  3 2024, 02:24:49) [Clang 14.0.6 ]
2025-02-25 23:41:24,101:INFO:executable: /Users/daaa/opt/miniconda3/envs/mlops/bin/python
2025-02-25 23:41:24,101:INFO:   machine: macOS-14.1-arm64-arm-64bit
2025-02-25 23:41:24,101:INFO:PyCaret required dependencies:
2025-02-25 23:41:24,511:INFO:                 pip: 24.2
2025-02-25 23:41:24,511:INFO:          setuptools: 75.1.0
2025-02-25 23:41:24,511:INFO:             pycaret: 3.3.2
2025-02-25 23:41:24,511:INFO:             IPython: 8.30.0
2025-02-25 23:41:24,511:INFO:          ipywidgets: 8.1.5
2025-02-25 23:41:24,511:INFO:                tqdm: 4.67.1
2025-02-25 23:41:24,511:INFO:               numpy: 1.26.4
2025-02-25 23:41:24,511:INFO:              pandas: 2.1.4
2025-02-25 23:41:24,511:INFO:              jinja2: 3.1.4
2025-02-25 23:41:24,511:INFO:               scipy: 1.11.4
2025-02-25 23:41:24,511:INFO:              joblib: 1.3.2
2025-02-25 23:41:24,511:INFO:             sklearn: 1.4.2
2025-02-25 23:41:24,511:INFO:                pyod: 2.0.2
2025-02-25 23:41:24,511:INFO:            imblearn: 0.12.4
2025-02-25 23:41:24,511:INFO:   category_encoders: 2.6.4
2025-02-25 23:41:24,511:INFO:            lightgbm: 4.5.0
2025-02-25 23:41:24,511:INFO:               numba: 0.60.0
2025-02-25 23:41:24,511:INFO:            requests: 2.32.3
2025-02-25 23:41:24,511:INFO:          matplotlib: 3.10.0
2025-02-25 23:41:24,511:INFO:          scikitplot: 0.3.7
2025-02-25 23:41:24,511:INFO:         yellowbrick: 1.5
2025-02-25 23:41:24,511:INFO:              plotly: 5.24.1
2025-02-25 23:41:24,511:INFO:    plotly-resampler: Not installed
2025-02-25 23:41:24,511:INFO:             kaleido: 0.2.1
2025-02-25 23:41:24,511:INFO:           schemdraw: 0.15
2025-02-25 23:41:24,511:INFO:         statsmodels: 0.14.4
2025-02-25 23:41:24,511:INFO:              sktime: 0.26.0
2025-02-25 23:41:24,511:INFO:               tbats: 1.1.3
2025-02-25 23:41:24,511:INFO:            pmdarima: 2.0.4
2025-02-25 23:41:24,511:INFO:              psutil: 6.1.0
2025-02-25 23:41:24,511:INFO:          markupsafe: 2.1.5
2025-02-25 23:41:24,511:INFO:             pickle5: Not installed
2025-02-25 23:41:24,511:INFO:         cloudpickle: 3.1.0
2025-02-25 23:41:24,511:INFO:         deprecation: 2.1.0
2025-02-25 23:41:24,511:INFO:              xxhash: 3.5.0
2025-02-25 23:41:24,512:INFO:           wurlitzer: 3.1.1
2025-02-25 23:41:24,512:INFO:PyCaret optional dependencies:
2025-02-25 23:41:25,891:INFO:                shap: 0.44.1
2025-02-25 23:41:25,891:INFO:           interpret: 0.6.9
2025-02-25 23:41:25,891:INFO:                umap: 0.5.7
2025-02-25 23:41:25,891:INFO:     ydata_profiling: 4.12.1
2025-02-25 23:41:25,891:INFO:  explainerdashboard: 0.4.8
2025-02-25 23:41:25,891:INFO:             autoviz: Not installed
2025-02-25 23:41:25,891:INFO:           fairlearn: 0.7.0
2025-02-25 23:41:25,891:INFO:          deepchecks: Not installed
2025-02-25 23:41:25,891:INFO:             xgboost: 2.1.3
2025-02-25 23:41:25,891:INFO:            catboost: 1.1.1
2025-02-25 23:41:25,891:INFO:              kmodes: 0.12.2
2025-02-25 23:41:25,891:INFO:             mlxtend: 0.23.3
2025-02-25 23:41:25,891:INFO:       statsforecast: 1.5.0
2025-02-25 23:41:25,891:INFO:        tune_sklearn: 0.5.0
2025-02-25 23:41:25,892:INFO:                 ray: 2.40.0
2025-02-25 23:41:25,892:INFO:            hyperopt: 0.2.7
2025-02-25 23:41:25,892:INFO:              optuna: 4.1.0
2025-02-25 23:41:25,892:INFO:               skopt: 0.10.2
2025-02-25 23:41:25,892:INFO:              mlflow: 2.16.0
2025-02-25 23:41:25,892:INFO:              gradio: 5.12.0
2025-02-25 23:41:25,892:INFO:             fastapi: 0.115.6
2025-02-25 23:41:25,892:INFO:             uvicorn: 0.34.0
2025-02-25 23:41:25,892:INFO:              m2cgen: 0.10.0
2025-02-25 23:41:25,892:INFO:           evidently: 0.4.40
2025-02-25 23:41:25,892:INFO:               fugue: 0.8.7
2025-02-25 23:41:25,892:INFO:           streamlit: Not installed
2025-02-25 23:41:25,892:INFO:             prophet: Not installed
2025-02-25 23:41:25,892:INFO:None
2025-02-25 23:41:25,892:INFO:Set up data.
2025-02-25 23:41:25,912:INFO:Set up folding strategy.
2025-02-25 23:41:25,912:INFO:Set up train/test split.
2025-02-25 23:41:25,919:INFO:Set up index.
2025-02-25 23:41:25,919:INFO:Assigning column types.
2025-02-25 23:41:25,923:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-02-25 23:41:25,923:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2025-02-25 23:41:25,925:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-25 23:41:25,928:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-25 23:41:25,960:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:41:25,984:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-25 23:41:25,984:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:41:25,985:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:41:26,000:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2025-02-25 23:41:26,003:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-25 23:41:26,005:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-25 23:41:26,038:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:41:26,061:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-25 23:41:26,061:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:41:26,062:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:41:26,063:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2025-02-25 23:41:26,065:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-25 23:41:26,067:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-25 23:41:26,099:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:41:26,123:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-25 23:41:26,123:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:41:26,124:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:41:26,127:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-25 23:41:26,129:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-25 23:41:26,161:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:41:26,184:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-25 23:41:26,185:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:41:26,186:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:41:26,186:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2025-02-25 23:41:26,191:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-25 23:41:26,222:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:41:26,247:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-25 23:41:26,247:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:41:26,248:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:41:26,253:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-25 23:41:26,286:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:41:26,309:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-25 23:41:26,310:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:41:26,311:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:41:26,311:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2025-02-25 23:41:26,349:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:41:26,373:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-25 23:41:26,373:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:41:26,375:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:41:26,412:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:41:26,435:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-25 23:41:26,436:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:41:26,437:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:41:26,437:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-02-25 23:41:26,475:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:41:26,499:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:41:26,500:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:41:26,538:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-25 23:41:26,561:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:41:26,563:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:41:26,563:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2025-02-25 23:41:26,624:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:41:26,625:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:41:26,690:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:41:26,691:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:41:26,693:INFO:Preparing preprocessing pipeline...
2025-02-25 23:41:26,693:INFO:Set up date feature engineering.
2025-02-25 23:41:26,693:INFO:Set up iterative imputation.
2025-02-25 23:41:26,754:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:41:26,755:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:41:26,782:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-25 23:41:26,800:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:41:26,801:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:41:26,817:INFO:Set up encoding of ordinal features.
2025-02-25 23:41:26,821:WARNING:The number of classes passed to feature Method in the ordinal_features parameter (11) don't match with the number of classes in the data (5).
2025-02-25 23:41:26,821:INFO:Set up encoding of categorical features.
2025-02-25 23:41:26,821:INFO:Set up polynomial features.
2025-02-25 23:41:26,821:INFO:Set up removing multicollinearity.
2025-02-25 23:41:26,821:INFO:Set up feature normalization.
2025-02-25 23:41:26,821:INFO:Set up feature selection.
2025-02-25 23:41:26,881:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:41:26,882:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:41:27,278:INFO:Finished creating preprocessing pipeline.
2025-02-25 23:41:27,322:INFO:Pipeline: Pipeline(memory=FastMemory(location=/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/joblib),
         steps=[('date_feature_extractor',
                 TransformerWrapper(include=['Date'],
                                    transformer=ExtractDateTimeFeatures())),
                ('iterative_imputer',
                 TransformerWrapper(transformer=IterativeImputer(cat_estimator=LGBMClassifier(n_jobs=-1,
                                                                                              random_state=123),
                                                                 cat_estimator_prepare_...
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('remove_multicollinearity',
                 TransformerWrapper(exclude=[],
                                    transformer=RemoveMulticollinearity(threshold=0.55))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('feature_selection',
                 TransformerWrapper(exclude=[],
                                    transformer=SelectFromModel(estimator=RandomForestRegressor(),
                                                                max_features=12,
                                                                threshold=-inf)))])
2025-02-25 23:41:27,322:INFO:Creating final display dataframe.
2025-02-25 23:41:28,888:INFO:Setup _display_container:                         Description          Value
0                        Session id            123
1                            Target          Price
2                       Target type     Regression
3               Original data shape    (13580, 21)
4            Transformed data shape    (13580, 13)
5       Transformed train set shape     (9506, 13)
6        Transformed test set shape     (4074, 13)
7                   Ignore features              4
8                  Ordinal features              3
9                  Numeric features              8
10                    Date features              1
11             Categorical features              6
12         Rows with missing values          54.4%
13                       Preprocess           True
14                  Imputation type      iterative
15  Iterative imputation iterations              5
16        Numeric iterative imputer       lightgbm
17    Categorical iterative imputer       lightgbm
18         Maximum one-hot encoding             25
19                  Encoding method           None
20              Polynomial features           True
21                Polynomial degree              2
22         Remove multicollinearity           True
23      Multicollinearity threshold           0.55
24                        Normalize           True
25                 Normalize method         zscore
26                Feature selection           True
27         Feature selection method        classic
28      Feature selection estimator             rf
29      Number of features selected            0.8
30                   Fold Generator          KFold
31                      Fold Number             10
32                         CPU Jobs             -1
33                          Use GPU          False
34                   Log Experiment   MlflowLogger
35                  Experiment Name  house_pricing
36                              USI           e0f6
2025-02-25 23:41:28,954:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:41:28,956:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:41:29,018:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-25 23:41:29,019:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-25 23:41:29,020:INFO:Logging experiment in loggers
2025-02-25 23:41:29,137:INFO:SubProcess save_model() called ==================================
2025-02-25 23:41:29,222:INFO:Initializing save_model()
2025-02-25 23:41:29,222:INFO:save_model(model=Pipeline(memory=FastMemory(location=/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/joblib),
         steps=[('date_feature_extractor',
                 TransformerWrapper(include=['Date'],
                                    transformer=ExtractDateTimeFeatures())),
                ('iterative_imputer',
                 TransformerWrapper(transformer=IterativeImputer(cat_estimator=LGBMClassifier(n_jobs=-1,
                                                                                              random_state=123),
                                                                 cat_estimator_prepare_...
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('remove_multicollinearity',
                 TransformerWrapper(exclude=[],
                                    transformer=RemoveMulticollinearity(threshold=0.55))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('feature_selection',
                 TransformerWrapper(exclude=[],
                                    transformer=SelectFromModel(estimator=RandomForestRegressor(),
                                                                max_features=12,
                                                                threshold=-inf)))]), model_name=/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/tmpe_z2x017/Transformation Pipeline, prep_pipe_=Pipeline(memory=FastMemory(location=/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/joblib),
         steps=[('date_feature_extractor',
                 TransformerWrapper(include=['Date'],
                                    transformer=ExtractDateTimeFeatures())),
                ('iterative_imputer',
                 TransformerWrapper(transformer=IterativeImputer(cat_estimator=LGBMClassifier(n_jobs=-1,
                                                                                              random_state=123),
                                                                 cat_estimator_prepare_...
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('remove_multicollinearity',
                 TransformerWrapper(exclude=[],
                                    transformer=RemoveMulticollinearity(threshold=0.55))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('feature_selection',
                 TransformerWrapper(exclude=[],
                                    transformer=SelectFromModel(estimator=RandomForestRegressor(),
                                                                max_features=12,
                                                                threshold=-inf)))]), verbose=False, use_case=MLUsecase.REGRESSION, kwargs={})
2025-02-25 23:41:29,222:INFO:Adding model into prep_pipe
2025-02-25 23:41:29,222:WARNING:Only Model saved as it was a pipeline.
2025-02-25 23:41:29,346:INFO:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/tmpe_z2x017/Transformation Pipeline.pkl saved in current working directory
2025-02-25 23:41:29,388:INFO:Pipeline(memory=FastMemory(location=/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/joblib),
         steps=[('date_feature_extractor',
                 TransformerWrapper(include=['Date'],
                                    transformer=ExtractDateTimeFeatures())),
                ('iterative_imputer',
                 TransformerWrapper(transformer=IterativeImputer(cat_estimator=LGBMClassifier(n_jobs=-1,
                                                                                              random_state=123),
                                                                 cat_estimator_prepare_...
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('remove_multicollinearity',
                 TransformerWrapper(exclude=[],
                                    transformer=RemoveMulticollinearity(threshold=0.55))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('feature_selection',
                 TransformerWrapper(exclude=[],
                                    transformer=SelectFromModel(estimator=RandomForestRegressor(),
                                                                max_features=12,
                                                                threshold=-inf)))])
2025-02-25 23:41:29,388:INFO:save_model() successfully completed......................................
2025-02-25 23:41:29,483:INFO:SubProcess save_model() end ==================================
2025-02-25 23:41:29,515:INFO:setup() successfully completed in 4.92s...............
2025-02-25 23:41:29,820:INFO:Initializing compare_models()
2025-02-25 23:41:29,820:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x2959dbd30>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x2959dbd30>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2025-02-25 23:41:29,820:INFO:Checking exceptions
2025-02-25 23:41:29,825:INFO:Preparing display monitor
2025-02-25 23:41:29,863:INFO:Initializing Linear Regression
2025-02-25 23:41:29,863:INFO:Total runtime is 7.2836875915527345e-06 minutes
2025-02-25 23:41:29,865:INFO:SubProcess create_model() called ==================================
2025-02-25 23:41:29,865:INFO:Initializing create_model()
2025-02-25 23:41:29,865:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x2959dbd30>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x29561a650>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-25 23:41:29,866:INFO:Checking exceptions
2025-02-25 23:41:29,866:INFO:Importing libraries
2025-02-25 23:41:29,866:INFO:Copying training dataset
2025-02-25 23:41:29,876:INFO:Defining folds
2025-02-25 23:41:29,877:INFO:Declaring metric variables
2025-02-25 23:41:29,879:INFO:Importing untrained model
2025-02-25 23:41:29,881:INFO:Linear Regression Imported successfully
2025-02-25 23:41:29,884:INFO:Starting cross validation
2025-02-25 23:41:30,014:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-25 23:42:12,587:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-25 23:42:12,839:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-25 23:42:13,389:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-25 23:42:13,666:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-25 23:42:13,778:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-25 23:42:13,868:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-25 23:42:14,000:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-25 23:42:14,004:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-25 23:54:45,590:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-25 23:54:45,590:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-25 23:54:45,590:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-25 23:54:45,590:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-25 23:58:06,047:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-25 23:58:06,047:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-25 23:58:06,047:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-25 23:58:06,047:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-26 11:12:50,239:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_24436/2838843215.py:2: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.boxplot(data=df, x="Method", y="Price", palette="viridis")

2025-02-26 11:12:50,336:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_24436/2297183518.py:4: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.barplot(x=avg_price_by_seller.index, y=avg_price_by_seller.values, palette="viridis")

2025-02-26 11:12:51,512:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_24436/1639960328.py:2: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.boxplot(

2025-02-26 11:12:51,924:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_24436/2157425422.py:3: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.violinplot(data=df, x="Bedroom2", y="Price", palette="viridis", ax=axes[0])

2025-02-26 11:12:52,056:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_24436/2157425422.py:8: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.violinplot(data=df, x="Bathroom", y="Price", palette="viridis", ax=axes[1])

2025-02-26 11:12:52,131:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_24436/2157425422.py:13: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.violinplot(data=df, x="Car", y="Price", palette="viridis", ax=axes[2])

2025-02-26 11:23:25,762:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-26 11:23:25,762:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-26 11:23:25,762:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-26 11:23:25,762:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-26 11:23:26,831:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_28746/2838843215.py:2: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.boxplot(data=df, x="Method", y="Price", palette="viridis")

2025-02-26 11:23:26,922:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_28746/2297183518.py:4: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.barplot(x=avg_price_by_seller.index, y=avg_price_by_seller.values, palette="viridis")

2025-02-26 11:23:27,857:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_28746/1639960328.py:2: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.boxplot(

2025-02-26 11:23:28,250:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_28746/2157425422.py:3: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.violinplot(data=df, x="Bedroom2", y="Price", palette="viridis", ax=axes[0])

2025-02-26 11:23:28,341:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_28746/2157425422.py:8: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.violinplot(data=df, x="Bathroom", y="Price", palette="viridis", ax=axes[1])

2025-02-26 11:23:28,412:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_28746/2157425422.py:13: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.violinplot(data=df, x="Car", y="Price", palette="viridis", ax=axes[2])

2025-02-26 11:23:29,430:INFO:PyCaret RegressionExperiment
2025-02-26 11:23:29,430:INFO:Logging name: house_pricing
2025-02-26 11:23:29,430:INFO:ML Usecase: MLUsecase.REGRESSION
2025-02-26 11:23:29,431:INFO:version 3.3.2
2025-02-26 11:23:29,431:INFO:Initializing setup()
2025-02-26 11:23:29,431:INFO:self.USI: f9a0
2025-02-26 11:23:29,431:INFO:self._variable_keys: {'_ml_usecase', 'fold_shuffle_param', 'exp_id', 'log_plots_param', 'X_test', 'pipeline', 'fold_groups_param', 'data', 'exp_name_log', 'X_train', '_available_plots', 'n_jobs_param', 'gpu_param', 'fold_generator', 'transform_target_param', 'html_param', 'y_test', 'gpu_n_jobs_param', 'memory', 'y', 'seed', 'target_param', 'idx', 'logging_param', 'USI', 'X', 'y_train'}
2025-02-26 11:23:29,431:INFO:Checking environment
2025-02-26 11:23:29,431:INFO:python_version: 3.10.15
2025-02-26 11:23:29,431:INFO:python_build: ('main', 'Oct  3 2024 02:24:49')
2025-02-26 11:23:29,431:INFO:machine: arm64
2025-02-26 11:23:29,431:INFO:platform: macOS-14.1-arm64-arm-64bit
2025-02-26 11:23:29,431:INFO:Memory: svmem(total=17179869184, available=5832359936, percent=66.1, used=7276380160, free=498417664, active=5362499584, inactive=5184995328, wired=1913880576)
2025-02-26 11:23:29,431:INFO:Physical Core: 8
2025-02-26 11:23:29,431:INFO:Logical Core: 8
2025-02-26 11:23:29,431:INFO:Checking libraries
2025-02-26 11:23:29,431:INFO:System:
2025-02-26 11:23:29,431:INFO:    python: 3.10.15 (main, Oct  3 2024, 02:24:49) [Clang 14.0.6 ]
2025-02-26 11:23:29,431:INFO:executable: /Users/daaa/opt/miniconda3/envs/mlops/bin/python
2025-02-26 11:23:29,431:INFO:   machine: macOS-14.1-arm64-arm-64bit
2025-02-26 11:23:29,431:INFO:PyCaret required dependencies:
2025-02-26 11:23:29,982:INFO:                 pip: 24.2
2025-02-26 11:23:29,982:INFO:          setuptools: 75.1.0
2025-02-26 11:23:29,982:INFO:             pycaret: 3.3.2
2025-02-26 11:23:29,982:INFO:             IPython: 8.30.0
2025-02-26 11:23:29,982:INFO:          ipywidgets: 8.1.5
2025-02-26 11:23:29,982:INFO:                tqdm: 4.67.1
2025-02-26 11:23:29,982:INFO:               numpy: 1.26.4
2025-02-26 11:23:29,982:INFO:              pandas: 2.1.4
2025-02-26 11:23:29,982:INFO:              jinja2: 3.1.4
2025-02-26 11:23:29,982:INFO:               scipy: 1.11.4
2025-02-26 11:23:29,982:INFO:              joblib: 1.3.2
2025-02-26 11:23:29,982:INFO:             sklearn: 1.4.2
2025-02-26 11:23:29,982:INFO:                pyod: 2.0.2
2025-02-26 11:23:29,982:INFO:            imblearn: 0.12.4
2025-02-26 11:23:29,982:INFO:   category_encoders: 2.6.4
2025-02-26 11:23:29,982:INFO:            lightgbm: 4.5.0
2025-02-26 11:23:29,982:INFO:               numba: 0.60.0
2025-02-26 11:23:29,982:INFO:            requests: 2.32.3
2025-02-26 11:23:29,982:INFO:          matplotlib: 3.10.0
2025-02-26 11:23:29,982:INFO:          scikitplot: 0.3.7
2025-02-26 11:23:29,982:INFO:         yellowbrick: 1.5
2025-02-26 11:23:29,982:INFO:              plotly: 5.24.1
2025-02-26 11:23:29,982:INFO:    plotly-resampler: Not installed
2025-02-26 11:23:29,982:INFO:             kaleido: 0.2.1
2025-02-26 11:23:29,982:INFO:           schemdraw: 0.15
2025-02-26 11:23:29,982:INFO:         statsmodels: 0.14.4
2025-02-26 11:23:29,982:INFO:              sktime: 0.26.0
2025-02-26 11:23:29,982:INFO:               tbats: 1.1.3
2025-02-26 11:23:29,982:INFO:            pmdarima: 2.0.4
2025-02-26 11:23:29,982:INFO:              psutil: 6.1.0
2025-02-26 11:23:29,982:INFO:          markupsafe: 2.1.5
2025-02-26 11:23:29,982:INFO:             pickle5: Not installed
2025-02-26 11:23:29,982:INFO:         cloudpickle: 3.1.0
2025-02-26 11:23:29,982:INFO:         deprecation: 2.1.0
2025-02-26 11:23:29,982:INFO:              xxhash: 3.5.0
2025-02-26 11:23:29,982:INFO:           wurlitzer: 3.1.1
2025-02-26 11:23:29,982:INFO:PyCaret optional dependencies:
2025-02-26 11:23:31,291:INFO:                shap: 0.44.1
2025-02-26 11:23:31,291:INFO:           interpret: 0.6.9
2025-02-26 11:23:31,291:INFO:                umap: 0.5.7
2025-02-26 11:23:31,291:INFO:     ydata_profiling: 4.12.1
2025-02-26 11:23:31,292:INFO:  explainerdashboard: 0.4.8
2025-02-26 11:23:31,292:INFO:             autoviz: Not installed
2025-02-26 11:23:31,292:INFO:           fairlearn: 0.7.0
2025-02-26 11:23:31,292:INFO:          deepchecks: Not installed
2025-02-26 11:23:31,292:INFO:             xgboost: 2.1.3
2025-02-26 11:23:31,292:INFO:            catboost: 1.1.1
2025-02-26 11:23:31,292:INFO:              kmodes: 0.12.2
2025-02-26 11:23:31,292:INFO:             mlxtend: 0.23.3
2025-02-26 11:23:31,292:INFO:       statsforecast: 1.5.0
2025-02-26 11:23:31,292:INFO:        tune_sklearn: 0.5.0
2025-02-26 11:23:31,292:INFO:                 ray: 2.40.0
2025-02-26 11:23:31,292:INFO:            hyperopt: 0.2.7
2025-02-26 11:23:31,292:INFO:              optuna: 4.1.0
2025-02-26 11:23:31,292:INFO:               skopt: 0.10.2
2025-02-26 11:23:31,292:INFO:              mlflow: 2.16.0
2025-02-26 11:23:31,292:INFO:              gradio: 5.12.0
2025-02-26 11:23:31,292:INFO:             fastapi: 0.115.6
2025-02-26 11:23:31,292:INFO:             uvicorn: 0.34.0
2025-02-26 11:23:31,292:INFO:              m2cgen: 0.10.0
2025-02-26 11:23:31,292:INFO:           evidently: 0.4.40
2025-02-26 11:23:31,292:INFO:               fugue: 0.8.7
2025-02-26 11:23:31,292:INFO:           streamlit: Not installed
2025-02-26 11:23:31,292:INFO:             prophet: Not installed
2025-02-26 11:23:31,292:INFO:None
2025-02-26 11:23:31,292:INFO:Set up data.
2025-02-26 11:23:31,311:INFO:Set up folding strategy.
2025-02-26 11:23:31,311:INFO:Set up train/test split.
2025-02-26 11:23:31,317:INFO:Set up index.
2025-02-26 11:23:31,317:INFO:Assigning column types.
2025-02-26 11:23:31,320:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-02-26 11:23:31,320:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2025-02-26 11:23:31,322:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-26 11:23:31,323:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-26 11:23:31,349:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-26 11:23:31,367:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-26 11:23:31,367:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-26 11:23:31,368:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-26 11:23:31,401:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2025-02-26 11:23:31,403:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-26 11:23:31,404:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-26 11:23:31,429:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-26 11:23:31,447:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-26 11:23:31,447:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-26 11:23:31,448:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-26 11:23:31,448:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2025-02-26 11:23:31,450:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-26 11:23:31,452:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-26 11:23:31,476:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-26 11:23:31,494:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-26 11:23:31,494:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-26 11:23:31,495:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-26 11:23:31,497:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-26 11:23:31,499:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-26 11:23:31,522:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-26 11:23:31,541:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-26 11:23:31,541:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-26 11:23:31,542:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-26 11:23:31,542:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2025-02-26 11:23:31,546:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-26 11:23:31,573:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-26 11:23:31,591:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-26 11:23:31,591:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-26 11:23:31,592:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-26 11:23:31,596:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-26 11:23:31,622:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-26 11:23:31,639:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-26 11:23:31,640:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-26 11:23:31,641:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-26 11:23:31,641:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2025-02-26 11:23:31,669:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-26 11:23:31,687:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-26 11:23:31,687:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-26 11:23:31,688:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-26 11:23:31,716:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-26 11:23:31,733:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-26 11:23:31,734:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-26 11:23:31,735:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-26 11:23:31,735:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-02-26 11:23:31,763:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-26 11:23:31,781:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-26 11:23:31,782:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-26 11:23:31,810:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-26 11:23:31,828:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-26 11:23:31,829:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-26 11:23:31,830:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2025-02-26 11:23:31,876:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-26 11:23:31,877:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-26 11:23:31,923:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-26 11:23:31,924:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-26 11:23:31,926:INFO:Preparing preprocessing pipeline...
2025-02-26 11:23:31,926:INFO:Set up date feature engineering.
2025-02-26 11:23:31,926:INFO:Set up iterative imputation.
2025-02-26 11:23:31,973:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-26 11:23:31,974:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-26 11:23:31,994:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-26 11:23:32,008:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-26 11:23:32,009:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-26 11:23:32,023:INFO:Set up encoding of ordinal features.
2025-02-26 11:23:32,026:WARNING:The number of classes passed to feature Method in the ordinal_features parameter (11) don't match with the number of classes in the data (5).
2025-02-26 11:23:32,026:INFO:Set up encoding of categorical features.
2025-02-26 11:23:32,026:INFO:Set up polynomial features.
2025-02-26 11:23:32,026:INFO:Set up removing multicollinearity.
2025-02-26 11:23:32,026:INFO:Set up feature normalization.
2025-02-26 11:23:32,026:INFO:Set up feature selection.
2025-02-26 11:23:32,075:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-26 11:23:32,076:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-26 11:23:32,442:INFO:Finished creating preprocessing pipeline.
2025-02-26 11:23:32,481:INFO:Pipeline: Pipeline(memory=FastMemory(location=/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/joblib),
         steps=[('date_feature_extractor',
                 TransformerWrapper(include=['Date'],
                                    transformer=ExtractDateTimeFeatures())),
                ('iterative_imputer',
                 TransformerWrapper(transformer=IterativeImputer(cat_estimator=LGBMClassifier(n_jobs=-1,
                                                                                              random_state=123),
                                                                 cat_estimator_prepare_...
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('remove_multicollinearity',
                 TransformerWrapper(exclude=[],
                                    transformer=RemoveMulticollinearity(threshold=0.55))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('feature_selection',
                 TransformerWrapper(exclude=[],
                                    transformer=SelectFromModel(estimator=RandomForestRegressor(),
                                                                max_features=12,
                                                                threshold=-inf)))])
2025-02-26 11:23:32,481:INFO:Creating final display dataframe.
2025-02-26 11:23:33,687:INFO:Setup _display_container:                         Description          Value
0                        Session id            123
1                            Target          Price
2                       Target type     Regression
3               Original data shape    (13580, 21)
4            Transformed data shape    (13580, 13)
5       Transformed train set shape     (9506, 13)
6        Transformed test set shape     (4074, 13)
7                   Ignore features              4
8                  Ordinal features              3
9                  Numeric features              8
10                    Date features              1
11             Categorical features              6
12         Rows with missing values          54.4%
13                       Preprocess           True
14                  Imputation type      iterative
15  Iterative imputation iterations              5
16        Numeric iterative imputer       lightgbm
17    Categorical iterative imputer       lightgbm
18         Maximum one-hot encoding             25
19                  Encoding method           None
20              Polynomial features           True
21                Polynomial degree              2
22         Remove multicollinearity           True
23      Multicollinearity threshold           0.55
24                        Normalize           True
25                 Normalize method         zscore
26                Feature selection           True
27         Feature selection method        classic
28      Feature selection estimator             rf
29      Number of features selected            0.8
30                   Fold Generator          KFold
31                      Fold Number             10
32                         CPU Jobs             -1
33                          Use GPU          False
34                   Log Experiment   MlflowLogger
35                  Experiment Name  house_pricing
36                              USI           f9a0
2025-02-26 11:23:33,738:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-26 11:23:33,739:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-26 11:23:33,786:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-26 11:23:33,788:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-26 11:23:33,788:INFO:Logging experiment in loggers
2025-02-26 11:23:33,917:INFO:SubProcess save_model() called ==================================
2025-02-26 11:23:33,982:INFO:Initializing save_model()
2025-02-26 11:23:33,982:INFO:save_model(model=Pipeline(memory=FastMemory(location=/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/joblib),
         steps=[('date_feature_extractor',
                 TransformerWrapper(include=['Date'],
                                    transformer=ExtractDateTimeFeatures())),
                ('iterative_imputer',
                 TransformerWrapper(transformer=IterativeImputer(cat_estimator=LGBMClassifier(n_jobs=-1,
                                                                                              random_state=123),
                                                                 cat_estimator_prepare_...
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('remove_multicollinearity',
                 TransformerWrapper(exclude=[],
                                    transformer=RemoveMulticollinearity(threshold=0.55))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('feature_selection',
                 TransformerWrapper(exclude=[],
                                    transformer=SelectFromModel(estimator=RandomForestRegressor(),
                                                                max_features=12,
                                                                threshold=-inf)))]), model_name=/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/tmpltw7wx0f/Transformation Pipeline, prep_pipe_=Pipeline(memory=FastMemory(location=/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/joblib),
         steps=[('date_feature_extractor',
                 TransformerWrapper(include=['Date'],
                                    transformer=ExtractDateTimeFeatures())),
                ('iterative_imputer',
                 TransformerWrapper(transformer=IterativeImputer(cat_estimator=LGBMClassifier(n_jobs=-1,
                                                                                              random_state=123),
                                                                 cat_estimator_prepare_...
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('remove_multicollinearity',
                 TransformerWrapper(exclude=[],
                                    transformer=RemoveMulticollinearity(threshold=0.55))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('feature_selection',
                 TransformerWrapper(exclude=[],
                                    transformer=SelectFromModel(estimator=RandomForestRegressor(),
                                                                max_features=12,
                                                                threshold=-inf)))]), verbose=False, use_case=MLUsecase.REGRESSION, kwargs={})
2025-02-26 11:23:33,982:INFO:Adding model into prep_pipe
2025-02-26 11:23:33,982:WARNING:Only Model saved as it was a pipeline.
2025-02-26 11:23:34,128:INFO:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/tmpltw7wx0f/Transformation Pipeline.pkl saved in current working directory
2025-02-26 11:23:34,161:INFO:Pipeline(memory=FastMemory(location=/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/joblib),
         steps=[('date_feature_extractor',
                 TransformerWrapper(include=['Date'],
                                    transformer=ExtractDateTimeFeatures())),
                ('iterative_imputer',
                 TransformerWrapper(transformer=IterativeImputer(cat_estimator=LGBMClassifier(n_jobs=-1,
                                                                                              random_state=123),
                                                                 cat_estimator_prepare_...
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('remove_multicollinearity',
                 TransformerWrapper(exclude=[],
                                    transformer=RemoveMulticollinearity(threshold=0.55))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('feature_selection',
                 TransformerWrapper(exclude=[],
                                    transformer=SelectFromModel(estimator=RandomForestRegressor(),
                                                                max_features=12,
                                                                threshold=-inf)))])
2025-02-26 11:23:34,161:INFO:save_model() successfully completed......................................
2025-02-26 11:23:34,255:INFO:SubProcess save_model() end ==================================
2025-02-26 11:23:34,312:INFO:setup() successfully completed in 4.37s...............
2025-02-26 11:23:34,565:INFO:Initializing compare_models()
2025-02-26 11:23:34,566:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x16d9cd7b0>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x16d9cd7b0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2025-02-26 11:23:34,566:INFO:Checking exceptions
2025-02-26 11:23:34,569:INFO:Preparing display monitor
2025-02-26 11:23:34,603:INFO:Initializing Linear Regression
2025-02-26 11:23:34,603:INFO:Total runtime is 7.379055023193359e-06 minutes
2025-02-26 11:23:34,605:INFO:SubProcess create_model() called ==================================
2025-02-26 11:23:34,606:INFO:Initializing create_model()
2025-02-26 11:23:34,606:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x16d9cd7b0>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x16666add0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-26 11:23:34,606:INFO:Checking exceptions
2025-02-26 11:23:34,606:INFO:Importing libraries
2025-02-26 11:23:34,606:INFO:Copying training dataset
2025-02-26 11:23:34,614:INFO:Defining folds
2025-02-26 11:23:34,615:INFO:Declaring metric variables
2025-02-26 11:23:34,616:INFO:Importing untrained model
2025-02-26 11:23:34,618:INFO:Linear Regression Imported successfully
2025-02-26 11:23:34,621:INFO:Starting cross validation
2025-02-26 11:23:34,741:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-26 11:24:15,078:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:24:15,601:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:24:15,768:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:24:15,959:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:24:16,099:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:24:16,122:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:24:16,223:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:24:16,255:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:24:41,301:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:24:41,908:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:24:55,643:INFO:Calculating mean and std
2025-02-26 11:24:55,645:INFO:Creating metrics dataframe
2025-02-26 11:24:55,656:INFO:Uploading results into container
2025-02-26 11:24:55,656:INFO:Uploading model into container now
2025-02-26 11:24:55,657:INFO:_master_model_container: 1
2025-02-26 11:24:55,657:INFO:_display_container: 2
2025-02-26 11:24:55,657:INFO:LinearRegression(n_jobs=-1)
2025-02-26 11:24:55,657:INFO:create_model() successfully completed......................................
2025-02-26 11:24:55,785:INFO:SubProcess create_model() end ==================================
2025-02-26 11:24:55,785:INFO:Creating metrics dataframe
2025-02-26 11:24:55,788:INFO:Initializing Lasso Regression
2025-02-26 11:24:55,788:INFO:Total runtime is 1.3530894637107849 minutes
2025-02-26 11:24:55,790:INFO:SubProcess create_model() called ==================================
2025-02-26 11:24:55,790:INFO:Initializing create_model()
2025-02-26 11:24:55,790:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x16d9cd7b0>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x16666add0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-26 11:24:55,790:INFO:Checking exceptions
2025-02-26 11:24:55,790:INFO:Importing libraries
2025-02-26 11:24:55,790:INFO:Copying training dataset
2025-02-26 11:24:55,797:INFO:Defining folds
2025-02-26 11:24:55,797:INFO:Declaring metric variables
2025-02-26 11:24:55,799:INFO:Importing untrained model
2025-02-26 11:24:55,801:INFO:Lasso Regression Imported successfully
2025-02-26 11:24:55,804:INFO:Starting cross validation
2025-02-26 11:24:55,954:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-26 11:25:32,256:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:25:32,617:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:25:32,900:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:25:32,910:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:25:33,237:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:25:33,384:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:25:33,631:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:25:33,674:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:25:51,028:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.382e+12, tolerance: 3.335e+11
  model = cd_fast.enet_coordinate_descent(

2025-02-26 11:25:51,491:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.795e+12, tolerance: 3.340e+11
  model = cd_fast.enet_coordinate_descent(

2025-02-26 11:25:51,841:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.001e+13, tolerance: 3.323e+11
  model = cd_fast.enet_coordinate_descent(

2025-02-26 11:25:52,210:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.356e+12, tolerance: 3.328e+11
  model = cd_fast.enet_coordinate_descent(

2025-02-26 11:25:52,237:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.076e+12, tolerance: 3.332e+11
  model = cd_fast.enet_coordinate_descent(

2025-02-26 11:25:52,635:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.696e+12, tolerance: 3.306e+11
  model = cd_fast.enet_coordinate_descent(

2025-02-26 11:25:52,719:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.236e+13, tolerance: 3.361e+11
  model = cd_fast.enet_coordinate_descent(

2025-02-26 11:25:53,285:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.665e+12, tolerance: 3.411e+11
  model = cd_fast.enet_coordinate_descent(

2025-02-26 11:25:58,363:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:25:58,928:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:26:12,646:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.994e+12, tolerance: 3.363e+11
  model = cd_fast.enet_coordinate_descent(

2025-02-26 11:26:12,677:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.123e+12, tolerance: 3.294e+11
  model = cd_fast.enet_coordinate_descent(

2025-02-26 11:26:12,774:INFO:Calculating mean and std
2025-02-26 11:26:12,776:INFO:Creating metrics dataframe
2025-02-26 11:26:12,790:INFO:Uploading results into container
2025-02-26 11:26:12,791:INFO:Uploading model into container now
2025-02-26 11:26:12,791:INFO:_master_model_container: 2
2025-02-26 11:26:12,791:INFO:_display_container: 2
2025-02-26 11:26:12,792:INFO:Lasso(random_state=123)
2025-02-26 11:26:12,792:INFO:create_model() successfully completed......................................
2025-02-26 11:26:12,917:INFO:SubProcess create_model() end ==================================
2025-02-26 11:26:12,917:INFO:Creating metrics dataframe
2025-02-26 11:26:12,920:INFO:Initializing Ridge Regression
2025-02-26 11:26:12,921:INFO:Total runtime is 2.638626229763031 minutes
2025-02-26 11:26:12,922:INFO:SubProcess create_model() called ==================================
2025-02-26 11:26:12,923:INFO:Initializing create_model()
2025-02-26 11:26:12,924:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x16d9cd7b0>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x16666add0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-26 11:26:12,924:INFO:Checking exceptions
2025-02-26 11:26:12,924:INFO:Importing libraries
2025-02-26 11:26:12,924:INFO:Copying training dataset
2025-02-26 11:26:12,930:INFO:Defining folds
2025-02-26 11:26:12,931:INFO:Declaring metric variables
2025-02-26 11:26:12,932:INFO:Importing untrained model
2025-02-26 11:26:12,934:INFO:Ridge Regression Imported successfully
2025-02-26 11:26:12,937:INFO:Starting cross validation
2025-02-26 11:26:13,116:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-26 11:26:50,274:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:26:50,689:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:26:50,863:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:26:50,929:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:26:51,033:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:26:51,083:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:26:51,433:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:26:51,465:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:27:16,547:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:27:16,563:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:27:30,823:INFO:Calculating mean and std
2025-02-26 11:27:30,824:INFO:Creating metrics dataframe
2025-02-26 11:27:30,833:INFO:Uploading results into container
2025-02-26 11:27:30,834:INFO:Uploading model into container now
2025-02-26 11:27:30,834:INFO:_master_model_container: 3
2025-02-26 11:27:30,834:INFO:_display_container: 2
2025-02-26 11:27:30,834:INFO:Ridge(random_state=123)
2025-02-26 11:27:30,834:INFO:create_model() successfully completed......................................
2025-02-26 11:27:30,916:INFO:SubProcess create_model() end ==================================
2025-02-26 11:27:30,916:INFO:Creating metrics dataframe
2025-02-26 11:27:30,920:INFO:Initializing Elastic Net
2025-02-26 11:27:30,920:INFO:Total runtime is 3.9386101603508 minutes
2025-02-26 11:27:30,921:INFO:SubProcess create_model() called ==================================
2025-02-26 11:27:30,921:INFO:Initializing create_model()
2025-02-26 11:27:30,921:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x16d9cd7b0>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x16666add0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-26 11:27:30,921:INFO:Checking exceptions
2025-02-26 11:27:30,921:INFO:Importing libraries
2025-02-26 11:27:30,921:INFO:Copying training dataset
2025-02-26 11:27:30,927:INFO:Defining folds
2025-02-26 11:27:30,927:INFO:Declaring metric variables
2025-02-26 11:27:30,929:INFO:Importing untrained model
2025-02-26 11:27:30,930:INFO:Elastic Net Imported successfully
2025-02-26 11:27:30,933:INFO:Starting cross validation
2025-02-26 11:27:31,026:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-26 11:28:08,621:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:28:08,903:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:28:09,414:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:28:09,657:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:28:09,729:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:28:09,794:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:28:10,317:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:28:10,430:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:28:33,153:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:28:33,491:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:28:47,273:INFO:Calculating mean and std
2025-02-26 11:28:47,274:INFO:Creating metrics dataframe
2025-02-26 11:28:47,285:INFO:Uploading results into container
2025-02-26 11:28:47,286:INFO:Uploading model into container now
2025-02-26 11:28:47,286:INFO:_master_model_container: 4
2025-02-26 11:28:47,286:INFO:_display_container: 2
2025-02-26 11:28:47,286:INFO:ElasticNet(random_state=123)
2025-02-26 11:28:47,286:INFO:create_model() successfully completed......................................
2025-02-26 11:28:47,370:INFO:SubProcess create_model() end ==================================
2025-02-26 11:28:47,370:INFO:Creating metrics dataframe
2025-02-26 11:28:47,373:INFO:Initializing Least Angle Regression
2025-02-26 11:28:47,373:INFO:Total runtime is 5.212831282615662 minutes
2025-02-26 11:28:47,374:INFO:SubProcess create_model() called ==================================
2025-02-26 11:28:47,374:INFO:Initializing create_model()
2025-02-26 11:28:47,374:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x16d9cd7b0>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x16666add0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-26 11:28:47,375:INFO:Checking exceptions
2025-02-26 11:28:47,375:INFO:Importing libraries
2025-02-26 11:28:47,375:INFO:Copying training dataset
2025-02-26 11:28:47,379:INFO:Defining folds
2025-02-26 11:28:47,380:INFO:Declaring metric variables
2025-02-26 11:28:47,381:INFO:Importing untrained model
2025-02-26 11:28:47,383:INFO:Least Angle Regression Imported successfully
2025-02-26 11:28:47,386:INFO:Starting cross validation
2025-02-26 11:28:47,478:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-26 11:29:25,171:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:29:25,320:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:29:25,419:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:29:25,737:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:29:25,758:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:29:26,017:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:29:26,030:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:29:26,074:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:29:57,580:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:29:57,879:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:30:15,237:INFO:Calculating mean and std
2025-02-26 11:30:15,238:INFO:Creating metrics dataframe
2025-02-26 11:30:15,249:INFO:Uploading results into container
2025-02-26 11:30:15,250:INFO:Uploading model into container now
2025-02-26 11:30:15,250:INFO:_master_model_container: 5
2025-02-26 11:30:15,250:INFO:_display_container: 2
2025-02-26 11:30:15,250:INFO:Lars(random_state=123)
2025-02-26 11:30:15,250:INFO:create_model() successfully completed......................................
2025-02-26 11:30:15,338:INFO:SubProcess create_model() end ==================================
2025-02-26 11:30:15,338:INFO:Creating metrics dataframe
2025-02-26 11:30:15,343:INFO:Initializing Lasso Least Angle Regression
2025-02-26 11:30:15,343:INFO:Total runtime is 6.6790022651354475 minutes
2025-02-26 11:30:15,345:INFO:SubProcess create_model() called ==================================
2025-02-26 11:30:15,345:INFO:Initializing create_model()
2025-02-26 11:30:15,345:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x16d9cd7b0>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x16666add0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-26 11:30:15,345:INFO:Checking exceptions
2025-02-26 11:30:15,345:INFO:Importing libraries
2025-02-26 11:30:15,346:INFO:Copying training dataset
2025-02-26 11:30:15,352:INFO:Defining folds
2025-02-26 11:30:15,352:INFO:Declaring metric variables
2025-02-26 11:30:15,354:INFO:Importing untrained model
2025-02-26 11:30:15,356:INFO:Lasso Least Angle Regression Imported successfully
2025-02-26 11:30:15,360:INFO:Starting cross validation
2025-02-26 11:30:15,501:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-26 11:30:55,981:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:30:56,926:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:30:56,954:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:30:56,979:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:30:57,220:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:30:57,243:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:30:57,750:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:30:57,993:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:31:28,210:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:31:28,667:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:31:45,991:INFO:Calculating mean and std
2025-02-26 11:31:45,992:INFO:Creating metrics dataframe
2025-02-26 11:31:46,004:INFO:Uploading results into container
2025-02-26 11:31:46,004:INFO:Uploading model into container now
2025-02-26 11:31:46,005:INFO:_master_model_container: 6
2025-02-26 11:31:46,005:INFO:_display_container: 2
2025-02-26 11:31:46,005:INFO:LassoLars(random_state=123)
2025-02-26 11:31:46,005:INFO:create_model() successfully completed......................................
2025-02-26 11:31:46,107:INFO:SubProcess create_model() end ==================================
2025-02-26 11:31:46,107:INFO:Creating metrics dataframe
2025-02-26 11:31:46,112:INFO:Initializing Orthogonal Matching Pursuit
2025-02-26 11:31:46,112:INFO:Total runtime is 8.191812479496003 minutes
2025-02-26 11:31:46,114:INFO:SubProcess create_model() called ==================================
2025-02-26 11:31:46,114:INFO:Initializing create_model()
2025-02-26 11:31:46,114:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x16d9cd7b0>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x16666add0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-26 11:31:46,114:INFO:Checking exceptions
2025-02-26 11:31:46,114:INFO:Importing libraries
2025-02-26 11:31:46,114:INFO:Copying training dataset
2025-02-26 11:31:46,121:INFO:Defining folds
2025-02-26 11:31:46,121:INFO:Declaring metric variables
2025-02-26 11:31:46,123:INFO:Importing untrained model
2025-02-26 11:31:46,125:INFO:Orthogonal Matching Pursuit Imported successfully
2025-02-26 11:31:46,129:INFO:Starting cross validation
2025-02-26 11:31:46,246:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-26 11:32:25,713:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:32:26,086:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:32:26,220:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:32:26,244:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:32:26,368:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:32:26,577:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:32:26,578:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:32:26,760:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:32:57,869:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:32:58,065:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:33:16,008:INFO:Calculating mean and std
2025-02-26 11:33:16,010:INFO:Creating metrics dataframe
2025-02-26 11:33:16,021:INFO:Uploading results into container
2025-02-26 11:33:16,022:INFO:Uploading model into container now
2025-02-26 11:33:16,022:INFO:_master_model_container: 7
2025-02-26 11:33:16,022:INFO:_display_container: 2
2025-02-26 11:33:16,022:INFO:OrthogonalMatchingPursuit()
2025-02-26 11:33:16,022:INFO:create_model() successfully completed......................................
2025-02-26 11:33:16,112:INFO:SubProcess create_model() end ==================================
2025-02-26 11:33:16,112:INFO:Creating metrics dataframe
2025-02-26 11:33:16,117:INFO:Initializing Bayesian Ridge
2025-02-26 11:33:16,117:INFO:Total runtime is 9.69189466238022 minutes
2025-02-26 11:33:16,119:INFO:SubProcess create_model() called ==================================
2025-02-26 11:33:16,119:INFO:Initializing create_model()
2025-02-26 11:33:16,119:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x16d9cd7b0>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x16666add0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-26 11:33:16,119:INFO:Checking exceptions
2025-02-26 11:33:16,119:INFO:Importing libraries
2025-02-26 11:33:16,119:INFO:Copying training dataset
2025-02-26 11:33:16,126:INFO:Defining folds
2025-02-26 11:33:16,127:INFO:Declaring metric variables
2025-02-26 11:33:16,129:INFO:Importing untrained model
2025-02-26 11:33:16,131:INFO:Bayesian Ridge Imported successfully
2025-02-26 11:33:16,135:INFO:Starting cross validation
2025-02-26 11:33:16,245:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-26 11:33:58,187:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:33:58,481:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:33:58,858:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:33:59,012:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:33:59,022:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:33:59,381:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:33:59,424:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:33:59,858:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:34:30,654:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:34:30,851:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:34:48,829:INFO:Calculating mean and std
2025-02-26 11:34:48,832:INFO:Creating metrics dataframe
2025-02-26 11:34:48,850:INFO:Uploading results into container
2025-02-26 11:34:48,851:INFO:Uploading model into container now
2025-02-26 11:34:48,851:INFO:_master_model_container: 8
2025-02-26 11:34:48,851:INFO:_display_container: 2
2025-02-26 11:34:48,852:INFO:BayesianRidge()
2025-02-26 11:34:48,852:INFO:create_model() successfully completed......................................
2025-02-26 11:34:48,984:INFO:SubProcess create_model() end ==================================
2025-02-26 11:34:48,984:INFO:Creating metrics dataframe
2025-02-26 11:34:48,989:INFO:Initializing Passive Aggressive Regressor
2025-02-26 11:34:48,990:INFO:Total runtime is 11.239775880177817 minutes
2025-02-26 11:34:48,992:INFO:SubProcess create_model() called ==================================
2025-02-26 11:34:48,992:INFO:Initializing create_model()
2025-02-26 11:34:48,992:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x16d9cd7b0>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x16666add0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-26 11:34:48,992:INFO:Checking exceptions
2025-02-26 11:34:48,992:INFO:Importing libraries
2025-02-26 11:34:48,992:INFO:Copying training dataset
2025-02-26 11:34:49,000:INFO:Defining folds
2025-02-26 11:34:49,000:INFO:Declaring metric variables
2025-02-26 11:34:49,002:INFO:Importing untrained model
2025-02-26 11:34:49,004:INFO:Passive Aggressive Regressor Imported successfully
2025-02-26 11:34:49,008:INFO:Starting cross validation
2025-02-26 11:34:49,141:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-26 11:35:29,429:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:35:29,706:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:35:30,226:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:35:30,291:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:35:30,492:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:35:30,602:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:35:31,052:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:35:31,405:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:35:51,097:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py:1575: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
  warnings.warn(

2025-02-26 11:35:51,240:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py:1575: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
  warnings.warn(

2025-02-26 11:35:51,372:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py:1575: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
  warnings.warn(

2025-02-26 11:35:51,547:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py:1575: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
  warnings.warn(

2025-02-26 11:35:51,994:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py:1575: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
  warnings.warn(

2025-02-26 11:35:52,047:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py:1575: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
  warnings.warn(

2025-02-26 11:35:52,166:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py:1575: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
  warnings.warn(

2025-02-26 11:35:52,522:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py:1575: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
  warnings.warn(

2025-02-26 11:36:03,297:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:36:03,794:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:36:21,547:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py:1575: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
  warnings.warn(

2025-02-26 11:36:21,608:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py:1575: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
  warnings.warn(

2025-02-26 11:36:21,722:INFO:Calculating mean and std
2025-02-26 11:36:21,723:INFO:Creating metrics dataframe
2025-02-26 11:36:21,735:INFO:Uploading results into container
2025-02-26 11:36:21,735:INFO:Uploading model into container now
2025-02-26 11:36:21,735:INFO:_master_model_container: 9
2025-02-26 11:36:21,735:INFO:_display_container: 2
2025-02-26 11:36:21,735:INFO:PassiveAggressiveRegressor(random_state=123)
2025-02-26 11:36:21,735:INFO:create_model() successfully completed......................................
2025-02-26 11:36:21,823:INFO:SubProcess create_model() end ==================================
2025-02-26 11:36:21,824:INFO:Creating metrics dataframe
2025-02-26 11:36:21,829:INFO:Initializing Huber Regressor
2025-02-26 11:36:21,829:INFO:Total runtime is 12.787097497781119 minutes
2025-02-26 11:36:21,831:INFO:SubProcess create_model() called ==================================
2025-02-26 11:36:21,831:INFO:Initializing create_model()
2025-02-26 11:36:21,831:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x16d9cd7b0>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x16666add0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-26 11:36:21,831:INFO:Checking exceptions
2025-02-26 11:36:21,832:INFO:Importing libraries
2025-02-26 11:36:21,832:INFO:Copying training dataset
2025-02-26 11:36:21,840:INFO:Defining folds
2025-02-26 11:36:21,840:INFO:Declaring metric variables
2025-02-26 11:36:21,842:INFO:Importing untrained model
2025-02-26 11:36:21,844:INFO:Huber Regressor Imported successfully
2025-02-26 11:36:21,848:INFO:Starting cross validation
2025-02-26 11:36:21,959:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-26 11:37:02,246:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:37:02,917:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:37:03,103:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:37:03,224:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:37:03,333:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:37:03,704:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:37:04,043:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:37:04,226:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:37:35,037:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:37:35,181:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:37:52,807:INFO:Calculating mean and std
2025-02-26 11:37:52,809:INFO:Creating metrics dataframe
2025-02-26 11:37:52,819:INFO:Uploading results into container
2025-02-26 11:37:52,819:INFO:Uploading model into container now
2025-02-26 11:37:52,819:INFO:_master_model_container: 10
2025-02-26 11:37:52,819:INFO:_display_container: 2
2025-02-26 11:37:52,819:INFO:HuberRegressor()
2025-02-26 11:37:52,820:INFO:create_model() successfully completed......................................
2025-02-26 11:37:52,907:INFO:SubProcess create_model() end ==================================
2025-02-26 11:37:52,907:INFO:Creating metrics dataframe
2025-02-26 11:37:52,912:INFO:Initializing K Neighbors Regressor
2025-02-26 11:37:52,912:INFO:Total runtime is 14.305147496859234 minutes
2025-02-26 11:37:52,914:INFO:SubProcess create_model() called ==================================
2025-02-26 11:37:52,914:INFO:Initializing create_model()
2025-02-26 11:37:52,914:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x16d9cd7b0>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x16666add0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-26 11:37:52,914:INFO:Checking exceptions
2025-02-26 11:37:52,914:INFO:Importing libraries
2025-02-26 11:37:52,914:INFO:Copying training dataset
2025-02-26 11:37:52,921:INFO:Defining folds
2025-02-26 11:37:52,921:INFO:Declaring metric variables
2025-02-26 11:37:52,923:INFO:Importing untrained model
2025-02-26 11:37:52,925:INFO:K Neighbors Regressor Imported successfully
2025-02-26 11:37:52,928:INFO:Starting cross validation
2025-02-26 11:37:53,039:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-26 11:38:33,913:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:38:34,433:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:38:34,454:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:38:34,482:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:38:34,552:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:38:34,907:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:38:35,119:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:38:35,245:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:39:07,417:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:39:07,890:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:39:25,437:INFO:Calculating mean and std
2025-02-26 11:39:25,438:INFO:Creating metrics dataframe
2025-02-26 11:39:25,446:INFO:Uploading results into container
2025-02-26 11:39:25,446:INFO:Uploading model into container now
2025-02-26 11:39:25,446:INFO:_master_model_container: 11
2025-02-26 11:39:25,446:INFO:_display_container: 2
2025-02-26 11:39:25,447:INFO:KNeighborsRegressor(n_jobs=-1)
2025-02-26 11:39:25,447:INFO:create_model() successfully completed......................................
2025-02-26 11:39:25,529:INFO:SubProcess create_model() end ==================================
2025-02-26 11:39:25,529:INFO:Creating metrics dataframe
2025-02-26 11:39:25,534:INFO:Initializing Decision Tree Regressor
2025-02-26 11:39:25,535:INFO:Total runtime is 15.84885864655177 minutes
2025-02-26 11:39:25,536:INFO:SubProcess create_model() called ==================================
2025-02-26 11:39:25,536:INFO:Initializing create_model()
2025-02-26 11:39:25,537:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x16d9cd7b0>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x16666add0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-26 11:39:25,537:INFO:Checking exceptions
2025-02-26 11:39:25,537:INFO:Importing libraries
2025-02-26 11:39:25,537:INFO:Copying training dataset
2025-02-26 11:39:25,542:INFO:Defining folds
2025-02-26 11:39:25,542:INFO:Declaring metric variables
2025-02-26 11:39:25,545:INFO:Importing untrained model
2025-02-26 11:39:25,547:INFO:Decision Tree Regressor Imported successfully
2025-02-26 11:39:25,551:INFO:Starting cross validation
2025-02-26 11:39:25,662:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-26 11:40:08,366:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:40:08,557:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:40:08,579:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:40:08,600:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:40:08,858:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:40:09,130:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:40:09,383:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:40:09,468:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:40:41,796:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:40:42,305:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:41:00,424:INFO:Calculating mean and std
2025-02-26 11:41:00,425:INFO:Creating metrics dataframe
2025-02-26 11:41:00,441:INFO:Uploading results into container
2025-02-26 11:41:00,441:INFO:Uploading model into container now
2025-02-26 11:41:00,441:INFO:_master_model_container: 12
2025-02-26 11:41:00,441:INFO:_display_container: 2
2025-02-26 11:41:00,442:INFO:DecisionTreeRegressor(random_state=123)
2025-02-26 11:41:00,442:INFO:create_model() successfully completed......................................
2025-02-26 11:41:00,537:INFO:SubProcess create_model() end ==================================
2025-02-26 11:41:00,537:INFO:Creating metrics dataframe
2025-02-26 11:41:00,542:INFO:Initializing Random Forest Regressor
2025-02-26 11:41:00,542:INFO:Total runtime is 17.432314145565034 minutes
2025-02-26 11:41:00,543:INFO:SubProcess create_model() called ==================================
2025-02-26 11:41:00,544:INFO:Initializing create_model()
2025-02-26 11:41:00,544:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x16d9cd7b0>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x16666add0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-26 11:41:00,544:INFO:Checking exceptions
2025-02-26 11:41:00,544:INFO:Importing libraries
2025-02-26 11:41:00,544:INFO:Copying training dataset
2025-02-26 11:41:00,549:INFO:Defining folds
2025-02-26 11:41:00,550:INFO:Declaring metric variables
2025-02-26 11:41:00,552:INFO:Importing untrained model
2025-02-26 11:41:00,554:INFO:Random Forest Regressor Imported successfully
2025-02-26 11:41:00,557:INFO:Starting cross validation
2025-02-26 11:41:00,668:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-26 11:41:41,448:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:41:41,903:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:41:42,249:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:41:42,415:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:41:42,497:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:41:42,624:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:41:42,894:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:41:43,022:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:42:22,296:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:42:22,657:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:42:42,254:INFO:Calculating mean and std
2025-02-26 11:42:42,255:INFO:Creating metrics dataframe
2025-02-26 11:42:42,267:INFO:Uploading results into container
2025-02-26 11:42:42,268:INFO:Uploading model into container now
2025-02-26 11:42:42,268:INFO:_master_model_container: 13
2025-02-26 11:42:42,268:INFO:_display_container: 2
2025-02-26 11:42:42,268:INFO:RandomForestRegressor(n_jobs=-1, random_state=123)
2025-02-26 11:42:42,268:INFO:create_model() successfully completed......................................
2025-02-26 11:42:42,356:INFO:SubProcess create_model() end ==================================
2025-02-26 11:42:42,356:INFO:Creating metrics dataframe
2025-02-26 11:42:42,363:INFO:Initializing Extra Trees Regressor
2025-02-26 11:42:42,363:INFO:Total runtime is 19.129327813784283 minutes
2025-02-26 11:42:42,365:INFO:SubProcess create_model() called ==================================
2025-02-26 11:42:42,365:INFO:Initializing create_model()
2025-02-26 11:42:42,365:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x16d9cd7b0>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x16666add0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-26 11:42:42,365:INFO:Checking exceptions
2025-02-26 11:42:42,365:INFO:Importing libraries
2025-02-26 11:42:42,365:INFO:Copying training dataset
2025-02-26 11:42:42,372:INFO:Defining folds
2025-02-26 11:42:42,372:INFO:Declaring metric variables
2025-02-26 11:42:42,374:INFO:Importing untrained model
2025-02-26 11:42:42,376:INFO:Extra Trees Regressor Imported successfully
2025-02-26 11:42:42,380:INFO:Starting cross validation
2025-02-26 11:42:42,512:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-26 11:43:22,007:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:43:22,203:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:43:22,210:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:43:22,277:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:43:22,719:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:43:22,776:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:43:23,034:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:43:23,413:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:43:56,154:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:43:56,435:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:44:14,282:INFO:Calculating mean and std
2025-02-26 11:44:14,283:INFO:Creating metrics dataframe
2025-02-26 11:44:14,299:INFO:Uploading results into container
2025-02-26 11:44:14,299:INFO:Uploading model into container now
2025-02-26 11:44:14,299:INFO:_master_model_container: 14
2025-02-26 11:44:14,299:INFO:_display_container: 2
2025-02-26 11:44:14,299:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=123)
2025-02-26 11:44:14,299:INFO:create_model() successfully completed......................................
2025-02-26 11:44:14,381:INFO:SubProcess create_model() end ==================================
2025-02-26 11:44:14,382:INFO:Creating metrics dataframe
2025-02-26 11:44:14,387:INFO:Initializing AdaBoost Regressor
2025-02-26 11:44:14,387:INFO:Total runtime is 20.663062131404878 minutes
2025-02-26 11:44:14,388:INFO:SubProcess create_model() called ==================================
2025-02-26 11:44:14,389:INFO:Initializing create_model()
2025-02-26 11:44:14,389:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x16d9cd7b0>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x16666add0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-26 11:44:14,389:INFO:Checking exceptions
2025-02-26 11:44:14,389:INFO:Importing libraries
2025-02-26 11:44:14,389:INFO:Copying training dataset
2025-02-26 11:44:14,395:INFO:Defining folds
2025-02-26 11:44:14,396:INFO:Declaring metric variables
2025-02-26 11:44:14,398:INFO:Importing untrained model
2025-02-26 11:44:14,399:INFO:AdaBoost Regressor Imported successfully
2025-02-26 11:44:14,403:INFO:Starting cross validation
2025-02-26 11:44:14,514:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-26 11:44:54,955:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:44:55,120:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:44:55,274:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:44:55,312:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:44:55,510:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:44:55,570:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:44:55,795:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:44:55,864:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:45:26,757:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:45:27,278:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:45:45,380:INFO:Calculating mean and std
2025-02-26 11:45:45,382:INFO:Creating metrics dataframe
2025-02-26 11:45:45,401:INFO:Uploading results into container
2025-02-26 11:45:45,402:INFO:Uploading model into container now
2025-02-26 11:45:45,402:INFO:_master_model_container: 15
2025-02-26 11:45:45,402:INFO:_display_container: 2
2025-02-26 11:45:45,403:INFO:AdaBoostRegressor(random_state=123)
2025-02-26 11:45:45,403:INFO:create_model() successfully completed......................................
2025-02-26 11:45:45,502:INFO:SubProcess create_model() end ==================================
2025-02-26 11:45:45,502:INFO:Creating metrics dataframe
2025-02-26 11:45:45,508:INFO:Initializing Gradient Boosting Regressor
2025-02-26 11:45:45,508:INFO:Total runtime is 22.18174561262131 minutes
2025-02-26 11:45:45,509:INFO:SubProcess create_model() called ==================================
2025-02-26 11:45:45,510:INFO:Initializing create_model()
2025-02-26 11:45:45,510:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x16d9cd7b0>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x16666add0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-26 11:45:45,510:INFO:Checking exceptions
2025-02-26 11:45:45,510:INFO:Importing libraries
2025-02-26 11:45:45,510:INFO:Copying training dataset
2025-02-26 11:45:45,516:INFO:Defining folds
2025-02-26 11:45:45,516:INFO:Declaring metric variables
2025-02-26 11:45:45,518:INFO:Importing untrained model
2025-02-26 11:45:45,520:INFO:Gradient Boosting Regressor Imported successfully
2025-02-26 11:45:45,524:INFO:Starting cross validation
2025-02-26 11:45:45,642:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-26 11:46:25,416:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:46:25,452:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:46:25,484:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:46:25,904:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:46:26,112:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:46:26,197:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:46:26,222:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:46:26,436:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:47:02,493:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:47:02,649:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:47:23,162:INFO:Calculating mean and std
2025-02-26 11:47:23,163:INFO:Creating metrics dataframe
2025-02-26 11:47:23,179:INFO:Uploading results into container
2025-02-26 11:47:23,180:INFO:Uploading model into container now
2025-02-26 11:47:23,180:INFO:_master_model_container: 16
2025-02-26 11:47:23,180:INFO:_display_container: 2
2025-02-26 11:47:23,180:INFO:GradientBoostingRegressor(random_state=123)
2025-02-26 11:47:23,181:INFO:create_model() successfully completed......................................
2025-02-26 11:47:23,269:INFO:SubProcess create_model() end ==================================
2025-02-26 11:47:23,269:INFO:Creating metrics dataframe
2025-02-26 11:47:23,275:INFO:Initializing Extreme Gradient Boosting
2025-02-26 11:47:23,275:INFO:Total runtime is 23.81120104789734 minutes
2025-02-26 11:47:23,277:INFO:SubProcess create_model() called ==================================
2025-02-26 11:47:23,277:INFO:Initializing create_model()
2025-02-26 11:47:23,278:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x16d9cd7b0>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x16666add0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-26 11:47:23,278:INFO:Checking exceptions
2025-02-26 11:47:23,278:INFO:Importing libraries
2025-02-26 11:47:23,278:INFO:Copying training dataset
2025-02-26 11:47:23,284:INFO:Defining folds
2025-02-26 11:47:23,284:INFO:Declaring metric variables
2025-02-26 11:47:23,286:INFO:Importing untrained model
2025-02-26 11:47:23,289:INFO:Extreme Gradient Boosting Imported successfully
2025-02-26 11:47:23,293:INFO:Starting cross validation
2025-02-26 11:47:23,408:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-26 11:48:05,314:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:48:05,444:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:48:05,585:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:48:05,899:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:48:05,963:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:48:06,309:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:48:06,418:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:48:06,671:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:48:37,467:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:48:37,810:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:48:55,891:INFO:Calculating mean and std
2025-02-26 11:48:55,892:INFO:Creating metrics dataframe
2025-02-26 11:48:55,905:INFO:Uploading results into container
2025-02-26 11:48:55,906:INFO:Uploading model into container now
2025-02-26 11:48:55,906:INFO:_master_model_container: 17
2025-02-26 11:48:55,906:INFO:_display_container: 2
2025-02-26 11:48:55,906:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device='cpu', early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, random_state=123, ...)
2025-02-26 11:48:55,906:INFO:create_model() successfully completed......................................
2025-02-26 11:48:55,992:INFO:SubProcess create_model() end ==================================
2025-02-26 11:48:55,992:INFO:Creating metrics dataframe
2025-02-26 11:48:55,997:INFO:Initializing Light Gradient Boosting Machine
2025-02-26 11:48:55,998:INFO:Total runtime is 25.356575981775922 minutes
2025-02-26 11:48:56,000:INFO:SubProcess create_model() called ==================================
2025-02-26 11:48:56,000:INFO:Initializing create_model()
2025-02-26 11:48:56,000:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x16d9cd7b0>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x16666add0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-26 11:48:56,000:INFO:Checking exceptions
2025-02-26 11:48:56,000:INFO:Importing libraries
2025-02-26 11:48:56,000:INFO:Copying training dataset
2025-02-26 11:48:56,007:INFO:Defining folds
2025-02-26 11:48:56,007:INFO:Declaring metric variables
2025-02-26 11:48:56,009:INFO:Importing untrained model
2025-02-26 11:48:56,011:INFO:Light Gradient Boosting Machine Imported successfully
2025-02-26 11:48:56,015:INFO:Starting cross validation
2025-02-26 11:48:56,150:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-26 11:49:37,255:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:49:37,359:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:49:37,765:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:49:37,846:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:49:38,050:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:49:38,124:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:49:38,193:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:49:38,349:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:50:13,920:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:50:14,090:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:50:32,990:INFO:Calculating mean and std
2025-02-26 11:50:32,992:INFO:Creating metrics dataframe
2025-02-26 11:50:33,010:INFO:Uploading results into container
2025-02-26 11:50:33,010:INFO:Uploading model into container now
2025-02-26 11:50:33,011:INFO:_master_model_container: 18
2025-02-26 11:50:33,011:INFO:_display_container: 2
2025-02-26 11:50:33,011:INFO:LGBMRegressor(n_jobs=-1, random_state=123)
2025-02-26 11:50:33,011:INFO:create_model() successfully completed......................................
2025-02-26 11:50:33,103:INFO:SubProcess create_model() end ==================================
2025-02-26 11:50:33,103:INFO:Creating metrics dataframe
2025-02-26 11:50:33,109:INFO:Initializing CatBoost Regressor
2025-02-26 11:50:33,109:INFO:Total runtime is 26.97510171333949 minutes
2025-02-26 11:50:33,111:INFO:SubProcess create_model() called ==================================
2025-02-26 11:50:33,111:INFO:Initializing create_model()
2025-02-26 11:50:33,112:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x16d9cd7b0>, estimator=catboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x16666add0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-26 11:50:33,112:INFO:Checking exceptions
2025-02-26 11:50:33,112:INFO:Importing libraries
2025-02-26 11:50:33,112:INFO:Copying training dataset
2025-02-26 11:50:33,119:INFO:Defining folds
2025-02-26 11:50:33,119:INFO:Declaring metric variables
2025-02-26 11:50:33,121:INFO:Importing untrained model
2025-02-26 11:50:33,123:INFO:CatBoost Regressor Imported successfully
2025-02-26 11:50:33,127:INFO:Starting cross validation
2025-02-26 11:50:33,239:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-26 11:51:14,506:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:51:14,721:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:51:14,861:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:51:14,863:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:51:15,058:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:51:15,100:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:51:15,336:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:51:15,603:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:51:53,013:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:51:53,695:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:52:13,991:INFO:Calculating mean and std
2025-02-26 11:52:13,993:INFO:Creating metrics dataframe
2025-02-26 11:52:14,006:INFO:Uploading results into container
2025-02-26 11:52:14,006:INFO:Uploading model into container now
2025-02-26 11:52:14,007:INFO:_master_model_container: 19
2025-02-26 11:52:14,007:INFO:_display_container: 2
2025-02-26 11:52:14,007:INFO:<catboost.core.CatBoostRegressor object at 0x166371ab0>
2025-02-26 11:52:14,007:INFO:create_model() successfully completed......................................
2025-02-26 11:52:14,109:INFO:SubProcess create_model() end ==================================
2025-02-26 11:52:14,109:INFO:Creating metrics dataframe
2025-02-26 11:52:14,116:INFO:Initializing Dummy Regressor
2025-02-26 11:52:14,116:INFO:Total runtime is 28.658547278245294 minutes
2025-02-26 11:52:14,118:INFO:SubProcess create_model() called ==================================
2025-02-26 11:52:14,118:INFO:Initializing create_model()
2025-02-26 11:52:14,118:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x16d9cd7b0>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x16666add0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-26 11:52:14,118:INFO:Checking exceptions
2025-02-26 11:52:14,118:INFO:Importing libraries
2025-02-26 11:52:14,118:INFO:Copying training dataset
2025-02-26 11:52:14,125:INFO:Defining folds
2025-02-26 11:52:14,125:INFO:Declaring metric variables
2025-02-26 11:52:14,127:INFO:Importing untrained model
2025-02-26 11:52:14,129:INFO:Dummy Regressor Imported successfully
2025-02-26 11:52:14,132:INFO:Starting cross validation
2025-02-26 11:52:14,250:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-26 11:52:55,343:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:52:55,953:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:52:56,352:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:52:56,462:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:52:56,724:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:52:56,818:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:52:56,847:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:52:57,241:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:53:30,586:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:53:31,046:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:53:49,194:INFO:Calculating mean and std
2025-02-26 11:53:49,195:INFO:Creating metrics dataframe
2025-02-26 11:53:49,215:INFO:Uploading results into container
2025-02-26 11:53:49,216:INFO:Uploading model into container now
2025-02-26 11:53:49,216:INFO:_master_model_container: 20
2025-02-26 11:53:49,216:INFO:_display_container: 2
2025-02-26 11:53:49,217:INFO:DummyRegressor()
2025-02-26 11:53:49,217:INFO:create_model() successfully completed......................................
2025-02-26 11:53:49,311:INFO:SubProcess create_model() end ==================================
2025-02-26 11:53:49,311:INFO:Creating metrics dataframe
2025-02-26 11:53:49,322:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/pycaret_experiment/supervised_experiment.py:323: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  master_display_.apply(

2025-02-26 11:53:49,326:INFO:Initializing create_model()
2025-02-26 11:53:49,326:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x16d9cd7b0>, estimator=<catboost.core.CatBoostRegressor object at 0x166371ab0>, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-26 11:53:49,326:INFO:Checking exceptions
2025-02-26 11:53:49,328:INFO:Importing libraries
2025-02-26 11:53:49,328:INFO:Copying training dataset
2025-02-26 11:53:49,333:INFO:Defining folds
2025-02-26 11:53:49,333:INFO:Declaring metric variables
2025-02-26 11:53:49,334:INFO:Importing untrained model
2025-02-26 11:53:49,334:INFO:Declaring custom model
2025-02-26 11:53:49,334:INFO:CatBoost Regressor Imported successfully
2025-02-26 11:53:49,469:INFO:Cross validation set to False
2025-02-26 11:53:49,469:INFO:Fitting Model
2025-02-26 11:53:49,499:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 11:53:49,499:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 11:53:49,503:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000266 seconds.
2025-02-26 11:53:49,503:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 11:53:49,503:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 11:53:49,503:INFO:[LightGBM] [Info] Total Bins 1877
2025-02-26 11:53:49,503:INFO:[LightGBM] [Info] Number of data points in the train set: 9467, number of used features: 17
2025-02-26 11:53:49,503:INFO:[LightGBM] [Info] Start training from score 1.610964
2025-02-26 11:53:50,045:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 11:53:50,045:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 11:53:50,049:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000264 seconds.
2025-02-26 11:53:50,049:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 11:53:50,049:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 11:53:50,049:INFO:[LightGBM] [Info] Total Bins 1810
2025-02-26 11:53:50,049:INFO:[LightGBM] [Info] Number of data points in the train set: 8543, number of used features: 17
2025-02-26 11:53:50,050:INFO:[LightGBM] [Info] Start training from score 14.875219
2025-02-26 11:53:50,519:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 11:53:50,519:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 11:53:50,521:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000391 seconds.
2025-02-26 11:53:50,521:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 11:53:50,521:INFO:[LightGBM] [Info] Total Bins 1813
2025-02-26 11:53:50,521:INFO:[LightGBM] [Info] Number of data points in the train set: 5729, number of used features: 17
2025-02-26 11:53:50,522:INFO:[LightGBM] [Info] Start training from score 1965.296038
2025-02-26 11:53:51,135:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 11:53:51,135:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 11:53:51,138:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000248 seconds.
2025-02-26 11:53:51,138:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 11:53:51,138:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 11:53:51,138:INFO:[LightGBM] [Info] Total Bins 2245
2025-02-26 11:53:51,138:INFO:[LightGBM] [Info] Number of data points in the train set: 9467, number of used features: 17
2025-02-26 11:53:51,138:INFO:[LightGBM] [Info] Start training from score 1.610964
2025-02-26 11:53:51,819:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 11:53:51,819:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 11:53:51,821:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000268 seconds.
2025-02-26 11:53:51,822:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 11:53:51,822:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 11:53:51,822:INFO:[LightGBM] [Info] Total Bins 1962
2025-02-26 11:53:51,822:INFO:[LightGBM] [Info] Number of data points in the train set: 8543, number of used features: 17
2025-02-26 11:53:51,822:INFO:[LightGBM] [Info] Start training from score 14.875219
2025-02-26 11:53:52,406:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 11:53:52,406:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 11:53:52,408:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000228 seconds.
2025-02-26 11:53:52,408:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 11:53:52,408:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 11:53:52,408:INFO:[LightGBM] [Info] Total Bins 1812
2025-02-26 11:53:52,408:INFO:[LightGBM] [Info] Number of data points in the train set: 5729, number of used features: 17
2025-02-26 11:53:52,409:INFO:[LightGBM] [Info] Start training from score 1965.296038
2025-02-26 11:53:53,064:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 11:53:53,064:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 11:53:53,067:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000449 seconds.
2025-02-26 11:53:53,067:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 11:53:53,067:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 11:53:53,068:INFO:[LightGBM] [Info] Total Bins 2244
2025-02-26 11:53:53,068:INFO:[LightGBM] [Info] Number of data points in the train set: 9467, number of used features: 17
2025-02-26 11:53:53,068:INFO:[LightGBM] [Info] Start training from score 1.610964
2025-02-26 11:53:53,748:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 11:53:53,748:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 11:53:53,750:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000241 seconds.
2025-02-26 11:53:53,750:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 11:53:53,750:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 11:53:53,750:INFO:[LightGBM] [Info] Total Bins 1962
2025-02-26 11:53:53,751:INFO:[LightGBM] [Info] Number of data points in the train set: 8543, number of used features: 17
2025-02-26 11:53:53,751:INFO:[LightGBM] [Info] Start training from score 14.875219
2025-02-26 11:53:54,393:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 11:53:54,394:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 11:53:54,395:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000415 seconds.
2025-02-26 11:53:54,396:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 11:53:54,396:INFO:[LightGBM] [Info] Total Bins 1813
2025-02-26 11:53:54,396:INFO:[LightGBM] [Info] Number of data points in the train set: 5729, number of used features: 17
2025-02-26 11:53:54,396:INFO:[LightGBM] [Info] Start training from score 1965.296038
2025-02-26 11:53:54,959:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 11:53:54,959:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 11:53:54,962:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000239 seconds.
2025-02-26 11:53:54,962:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 11:53:54,962:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 11:53:54,962:INFO:[LightGBM] [Info] Total Bins 2243
2025-02-26 11:53:54,962:INFO:[LightGBM] [Info] Number of data points in the train set: 9467, number of used features: 17
2025-02-26 11:53:54,963:INFO:[LightGBM] [Info] Start training from score 1.610964
2025-02-26 11:53:55,577:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 11:53:55,577:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 11:53:55,580:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000501 seconds.
2025-02-26 11:53:55,580:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 11:53:55,580:INFO:[LightGBM] [Info] Total Bins 1961
2025-02-26 11:53:55,580:INFO:[LightGBM] [Info] Number of data points in the train set: 8543, number of used features: 17
2025-02-26 11:53:55,580:INFO:[LightGBM] [Info] Start training from score 14.875219
2025-02-26 11:53:56,103:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 11:53:56,103:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 11:53:56,105:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000372 seconds.
2025-02-26 11:53:56,105:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 11:53:56,105:INFO:[LightGBM] [Info] Total Bins 1813
2025-02-26 11:53:56,105:INFO:[LightGBM] [Info] Number of data points in the train set: 5729, number of used features: 17
2025-02-26 11:53:56,106:INFO:[LightGBM] [Info] Start training from score 1965.296038
2025-02-26 11:53:56,654:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 11:53:56,654:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 11:53:56,657:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000313 seconds.
2025-02-26 11:53:56,657:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 11:53:56,657:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 11:53:56,657:INFO:[LightGBM] [Info] Total Bins 2244
2025-02-26 11:53:56,657:INFO:[LightGBM] [Info] Number of data points in the train set: 9467, number of used features: 17
2025-02-26 11:53:56,658:INFO:[LightGBM] [Info] Start training from score 1.610964
2025-02-26 11:53:57,337:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 11:53:57,337:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 11:53:57,340:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000272 seconds.
2025-02-26 11:53:57,340:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 11:53:57,340:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 11:53:57,340:INFO:[LightGBM] [Info] Total Bins 1962
2025-02-26 11:53:57,340:INFO:[LightGBM] [Info] Number of data points in the train set: 8543, number of used features: 17
2025-02-26 11:53:57,340:INFO:[LightGBM] [Info] Start training from score 14.875219
2025-02-26 11:53:58,020:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 11:53:58,020:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 11:53:58,022:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000435 seconds.
2025-02-26 11:53:58,023:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 11:53:58,023:INFO:[LightGBM] [Info] Total Bins 1813
2025-02-26 11:53:58,023:INFO:[LightGBM] [Info] Number of data points in the train set: 5729, number of used features: 17
2025-02-26 11:53:58,023:INFO:[LightGBM] [Info] Start training from score 1965.296038
2025-02-26 11:53:58,588:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:54:20,232:INFO:<catboost.core.CatBoostRegressor object at 0x1663ba380>
2025-02-26 11:54:20,233:INFO:create_model() successfully completed......................................
2025-02-26 11:54:20,320:INFO:Creating Dashboard logs
2025-02-26 11:54:20,322:INFO:Model: CatBoost Regressor
2025-02-26 11:54:20,333:INFO:Logged params: {'nan_mode': 'Min', 'eval_metric': 'RMSE', 'iterations': 1000, 'sampling_frequency': 'PerTree', 'leaf_estimation_method': 'Newton', 'grow_policy': 'SymmetricTree', 'penalties_coefficient': 1, 'boosting_type': 'Plain', 'model_shrink_mode': 'Constant', 'feature_border_type': 'GreedyLogSum', 'bayesian_matrix_reg': 0.10000000149011612, 'eval_fraction': 0, 'force_unit_auto_pair_weights': False, 'l2_leaf_reg': 3, 'random_strength': 1, 'rsm': 1, 'boost_from_average': True, 'model_size_reg': 0.5, 'pool_metainfo_options': {'tags': {}}, 'subsample': 0.800000011920929, 'use_best_model': False, 'random_seed': 123, 'depth': 6, 'posterior_sampling': False, 'border_count': 254, 'classes_count': 0, 'auto_class_weights': 'None', 'sparse_features_conflict_fraction': 0, 'leaf_estimation_backtracking': 'AnyImprovement', 'best_model_min_trees': 1, 'model_shrink_rate': 0, 'min_data_in_leaf': 1, 'loss_function': 'RMSE', 'learning_rate': 0.05843900144100189, 'score_function': 'Cosine', 'task_type': 'CPU', 'leaf_estimation_iterations': 1, 'bootstrap_type': 'MVS', 'max_leaves': 64}
2025-02-26 11:54:20,366:INFO:Initializing predict_model()
2025-02-26 11:54:20,366:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x16d9cd7b0>, estimator=<catboost.core.CatBoostRegressor object at 0x1663ba380>, probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=False, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x2954448b0>)
2025-02-26 11:54:20,366:INFO:Checking exceptions
2025-02-26 11:54:20,366:INFO:Preloading libraries
2025-02-26 11:54:20,675:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.
  warnings.warn(

2025-02-26 11:54:22,016:INFO:Creating Dashboard logs
2025-02-26 11:54:22,019:INFO:Model: Light Gradient Boosting Machine
2025-02-26 11:54:22,028:INFO:Logged params: {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'num_leaves': 31, 'objective': None, 'random_state': 123, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0}
2025-02-26 11:54:22,496:INFO:Creating Dashboard logs
2025-02-26 11:54:22,498:INFO:Model: Extra Trees Regressor
2025-02-26 11:54:22,506:INFO:Logged params: {'bootstrap': False, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False}
2025-02-26 11:54:22,957:INFO:Creating Dashboard logs
2025-02-26 11:54:22,959:INFO:Model: Random Forest Regressor
2025-02-26 11:54:22,966:INFO:Logged params: {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False}
2025-02-26 11:54:23,461:INFO:Creating Dashboard logs
2025-02-26 11:54:23,463:INFO:Model: Gradient Boosting Regressor
2025-02-26 11:54:23,471:INFO:Logged params: {'alpha': 0.9, 'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.1, 'loss': 'squared_error', 'max_depth': 3, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_iter_no_change': None, 'random_state': 123, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}
2025-02-26 11:54:23,957:INFO:Creating Dashboard logs
2025-02-26 11:54:23,959:INFO:Model: Extreme Gradient Boosting
2025-02-26 11:54:23,967:INFO:Logged params: {'objective': 'reg:squarederror', 'base_score': None, 'booster': 'gbtree', 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': None, 'device': 'cpu', 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': None, 'feature_types': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': None, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': None, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': None, 'n_jobs': -1, 'num_parallel_tree': None, 'random_state': 123, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': None, 'tree_method': 'auto', 'validate_parameters': None, 'verbosity': 0}
2025-02-26 11:54:24,394:INFO:Creating Dashboard logs
2025-02-26 11:54:24,396:INFO:Model: K Neighbors Regressor
2025-02-26 11:54:24,405:INFO:Logged params: {'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': -1, 'n_neighbors': 5, 'p': 2, 'weights': 'uniform'}
2025-02-26 11:54:24,764:INFO:Creating Dashboard logs
2025-02-26 11:54:24,766:INFO:Model: Huber Regressor
2025-02-26 11:54:24,774:INFO:Logged params: {'alpha': 0.0001, 'epsilon': 1.35, 'fit_intercept': True, 'max_iter': 100, 'tol': 1e-05, 'warm_start': False}
2025-02-26 11:54:25,237:INFO:Creating Dashboard logs
2025-02-26 11:54:25,239:INFO:Model: Bayesian Ridge
2025-02-26 11:54:25,247:INFO:Logged params: {'alpha_1': 1e-06, 'alpha_2': 1e-06, 'alpha_init': None, 'compute_score': False, 'copy_X': True, 'fit_intercept': True, 'lambda_1': 1e-06, 'lambda_2': 1e-06, 'lambda_init': None, 'max_iter': None, 'n_iter': 'deprecated', 'tol': 0.001, 'verbose': False}
2025-02-26 11:54:25,716:INFO:Creating Dashboard logs
2025-02-26 11:54:25,718:INFO:Model: Ridge Regression
2025-02-26 11:54:25,725:INFO:Logged params: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'positive': False, 'random_state': 123, 'solver': 'auto', 'tol': 0.0001}
2025-02-26 11:54:26,139:INFO:Creating Dashboard logs
2025-02-26 11:54:26,141:INFO:Model: Lasso Least Angle Regression
2025-02-26 11:54:26,150:INFO:Logged params: {'alpha': 1.0, 'copy_X': True, 'eps': 2.220446049250313e-16, 'fit_intercept': True, 'fit_path': True, 'jitter': None, 'max_iter': 500, 'positive': False, 'precompute': 'auto', 'random_state': 123, 'verbose': False}
2025-02-26 11:54:26,620:INFO:Creating Dashboard logs
2025-02-26 11:54:26,622:INFO:Model: Linear Regression
2025-02-26 11:54:26,632:INFO:Logged params: {'copy_X': True, 'fit_intercept': True, 'n_jobs': -1, 'positive': False}
2025-02-26 11:54:27,003:INFO:Creating Dashboard logs
2025-02-26 11:54:27,005:INFO:Model: Lasso Regression
2025-02-26 11:54:27,015:INFO:Logged params: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': 1000, 'positive': False, 'precompute': False, 'random_state': 123, 'selection': 'cyclic', 'tol': 0.0001, 'warm_start': False}
2025-02-26 11:54:27,459:INFO:Creating Dashboard logs
2025-02-26 11:54:27,461:INFO:Model: Passive Aggressive Regressor
2025-02-26 11:54:27,469:INFO:Logged params: {'C': 1.0, 'average': False, 'early_stopping': False, 'epsilon': 0.1, 'fit_intercept': True, 'loss': 'epsilon_insensitive', 'max_iter': 1000, 'n_iter_no_change': 5, 'random_state': 123, 'shuffle': True, 'tol': 0.001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}
2025-02-26 11:54:27,902:INFO:Creating Dashboard logs
2025-02-26 11:54:27,905:INFO:Model: Least Angle Regression
2025-02-26 11:54:27,912:INFO:Logged params: {'copy_X': True, 'eps': 2.220446049250313e-16, 'fit_intercept': True, 'fit_path': True, 'jitter': None, 'n_nonzero_coefs': 500, 'precompute': 'auto', 'random_state': 123, 'verbose': False}
2025-02-26 11:54:28,311:INFO:Creating Dashboard logs
2025-02-26 11:54:28,313:INFO:Model: Elastic Net
2025-02-26 11:54:28,321:INFO:Logged params: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'l1_ratio': 0.5, 'max_iter': 1000, 'positive': False, 'precompute': False, 'random_state': 123, 'selection': 'cyclic', 'tol': 0.0001, 'warm_start': False}
2025-02-26 11:54:28,704:INFO:Creating Dashboard logs
2025-02-26 11:54:28,706:INFO:Model: Decision Tree Regressor
2025-02-26 11:54:28,713:INFO:Logged params: {'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 123, 'splitter': 'best'}
2025-02-26 11:54:29,173:INFO:Creating Dashboard logs
2025-02-26 11:54:29,175:INFO:Model: Orthogonal Matching Pursuit
2025-02-26 11:54:29,186:INFO:Logged params: {'fit_intercept': True, 'n_nonzero_coefs': None, 'precompute': 'auto', 'tol': None}
2025-02-26 11:54:29,564:INFO:Creating Dashboard logs
2025-02-26 11:54:29,568:INFO:Model: AdaBoost Regressor
2025-02-26 11:54:29,577:INFO:Logged params: {'estimator': None, 'learning_rate': 1.0, 'loss': 'linear', 'n_estimators': 50, 'random_state': 123}
2025-02-26 11:54:29,935:INFO:Creating Dashboard logs
2025-02-26 11:54:29,937:INFO:Model: Dummy Regressor
2025-02-26 11:54:29,945:INFO:Logged params: {'constant': None, 'quantile': None, 'strategy': 'mean'}
2025-02-26 11:54:30,316:INFO:_master_model_container: 20
2025-02-26 11:54:30,316:INFO:_display_container: 2
2025-02-26 11:54:30,316:INFO:<catboost.core.CatBoostRegressor object at 0x1663ba380>
2025-02-26 11:54:30,316:INFO:compare_models() successfully completed......................................
2025-02-26 11:54:30,319:INFO:Initializing create_model()
2025-02-26 11:54:30,319:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x16d9cd7b0>, estimator=et, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-26 11:54:30,319:INFO:Checking exceptions
2025-02-26 11:54:30,327:INFO:Importing libraries
2025-02-26 11:54:30,327:INFO:Copying training dataset
2025-02-26 11:54:30,335:INFO:Defining folds
2025-02-26 11:54:30,336:INFO:Declaring metric variables
2025-02-26 11:54:30,338:INFO:Importing untrained model
2025-02-26 11:54:30,339:INFO:Extra Trees Regressor Imported successfully
2025-02-26 11:54:30,343:INFO:Starting cross validation
2025-02-26 11:54:30,453:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-26 11:55:12,176:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:55:12,528:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:55:12,620:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:55:12,668:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:55:12,669:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:55:12,702:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:55:12,923:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:55:13,753:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:55:46,305:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:55:46,568:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:56:04,734:INFO:Calculating mean and std
2025-02-26 11:56:04,735:INFO:Creating metrics dataframe
2025-02-26 11:56:04,738:INFO:Finalizing model
2025-02-26 11:56:04,769:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 11:56:04,770:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 11:56:04,772:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000259 seconds.
2025-02-26 11:56:04,773:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 11:56:04,773:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 11:56:04,773:INFO:[LightGBM] [Info] Total Bins 1877
2025-02-26 11:56:04,773:INFO:[LightGBM] [Info] Number of data points in the train set: 9467, number of used features: 17
2025-02-26 11:56:04,773:INFO:[LightGBM] [Info] Start training from score 1.610964
2025-02-26 11:56:05,357:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 11:56:05,357:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 11:56:05,360:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000221 seconds.
2025-02-26 11:56:05,360:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 11:56:05,360:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 11:56:05,360:INFO:[LightGBM] [Info] Total Bins 1810
2025-02-26 11:56:05,360:INFO:[LightGBM] [Info] Number of data points in the train set: 8543, number of used features: 17
2025-02-26 11:56:05,360:INFO:[LightGBM] [Info] Start training from score 14.875219
2025-02-26 11:56:05,962:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 11:56:05,962:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 11:56:05,964:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000194 seconds.
2025-02-26 11:56:05,964:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 11:56:05,964:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 11:56:05,964:INFO:[LightGBM] [Info] Total Bins 1813
2025-02-26 11:56:05,964:INFO:[LightGBM] [Info] Number of data points in the train set: 5729, number of used features: 17
2025-02-26 11:56:05,964:INFO:[LightGBM] [Info] Start training from score 1965.296038
2025-02-26 11:56:06,505:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 11:56:06,505:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 11:56:06,507:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000512 seconds.
2025-02-26 11:56:06,508:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 11:56:06,508:INFO:[LightGBM] [Info] Total Bins 2245
2025-02-26 11:56:06,508:INFO:[LightGBM] [Info] Number of data points in the train set: 9467, number of used features: 17
2025-02-26 11:56:06,508:INFO:[LightGBM] [Info] Start training from score 1.610964
2025-02-26 11:56:07,050:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 11:56:07,050:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 11:56:07,053:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000267 seconds.
2025-02-26 11:56:07,053:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 11:56:07,053:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 11:56:07,053:INFO:[LightGBM] [Info] Total Bins 1962
2025-02-26 11:56:07,053:INFO:[LightGBM] [Info] Number of data points in the train set: 8543, number of used features: 17
2025-02-26 11:56:07,053:INFO:[LightGBM] [Info] Start training from score 14.875219
2025-02-26 11:56:07,457:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 11:56:07,457:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 11:56:07,459:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000411 seconds.
2025-02-26 11:56:07,459:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 11:56:07,459:INFO:[LightGBM] [Info] Total Bins 1812
2025-02-26 11:56:07,459:INFO:[LightGBM] [Info] Number of data points in the train set: 5729, number of used features: 17
2025-02-26 11:56:07,460:INFO:[LightGBM] [Info] Start training from score 1965.296038
2025-02-26 11:56:07,840:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 11:56:07,840:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 11:56:07,843:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000258 seconds.
2025-02-26 11:56:07,843:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 11:56:07,843:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 11:56:07,843:INFO:[LightGBM] [Info] Total Bins 2244
2025-02-26 11:56:07,843:INFO:[LightGBM] [Info] Number of data points in the train set: 9467, number of used features: 17
2025-02-26 11:56:07,844:INFO:[LightGBM] [Info] Start training from score 1.610964
2025-02-26 11:56:08,485:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 11:56:08,485:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 11:56:08,488:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000270 seconds.
2025-02-26 11:56:08,488:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 11:56:08,488:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 11:56:08,488:INFO:[LightGBM] [Info] Total Bins 1962
2025-02-26 11:56:08,488:INFO:[LightGBM] [Info] Number of data points in the train set: 8543, number of used features: 17
2025-02-26 11:56:08,488:INFO:[LightGBM] [Info] Start training from score 14.875219
2025-02-26 11:56:09,016:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 11:56:09,016:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 11:56:09,018:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000353 seconds.
2025-02-26 11:56:09,018:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 11:56:09,018:INFO:[LightGBM] [Info] Total Bins 1813
2025-02-26 11:56:09,018:INFO:[LightGBM] [Info] Number of data points in the train set: 5729, number of used features: 17
2025-02-26 11:56:09,019:INFO:[LightGBM] [Info] Start training from score 1965.296038
2025-02-26 11:56:09,493:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 11:56:09,493:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 11:56:09,495:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000249 seconds.
2025-02-26 11:56:09,495:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 11:56:09,495:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 11:56:09,496:INFO:[LightGBM] [Info] Total Bins 2243
2025-02-26 11:56:09,496:INFO:[LightGBM] [Info] Number of data points in the train set: 9467, number of used features: 17
2025-02-26 11:56:09,496:INFO:[LightGBM] [Info] Start training from score 1.610964
2025-02-26 11:56:10,123:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 11:56:10,124:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 11:56:10,126:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000238 seconds.
2025-02-26 11:56:10,126:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 11:56:10,126:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 11:56:10,126:INFO:[LightGBM] [Info] Total Bins 1961
2025-02-26 11:56:10,126:INFO:[LightGBM] [Info] Number of data points in the train set: 8543, number of used features: 17
2025-02-26 11:56:10,127:INFO:[LightGBM] [Info] Start training from score 14.875219
2025-02-26 11:56:10,645:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 11:56:10,645:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 11:56:10,647:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000360 seconds.
2025-02-26 11:56:10,647:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 11:56:10,647:INFO:[LightGBM] [Info] Total Bins 1813
2025-02-26 11:56:10,648:INFO:[LightGBM] [Info] Number of data points in the train set: 5729, number of used features: 17
2025-02-26 11:56:10,648:INFO:[LightGBM] [Info] Start training from score 1965.296038
2025-02-26 11:56:11,179:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 11:56:11,179:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 11:56:11,182:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000226 seconds.
2025-02-26 11:56:11,182:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 11:56:11,182:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 11:56:11,182:INFO:[LightGBM] [Info] Total Bins 2244
2025-02-26 11:56:11,182:INFO:[LightGBM] [Info] Number of data points in the train set: 9467, number of used features: 17
2025-02-26 11:56:11,183:INFO:[LightGBM] [Info] Start training from score 1.610964
2025-02-26 11:56:11,735:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 11:56:11,736:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 11:56:11,738:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000277 seconds.
2025-02-26 11:56:11,738:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 11:56:11,738:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 11:56:11,738:INFO:[LightGBM] [Info] Total Bins 1962
2025-02-26 11:56:11,738:INFO:[LightGBM] [Info] Number of data points in the train set: 8543, number of used features: 17
2025-02-26 11:56:11,739:INFO:[LightGBM] [Info] Start training from score 14.875219
2025-02-26 11:56:12,382:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 11:56:12,382:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 11:56:12,384:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000424 seconds.
2025-02-26 11:56:12,384:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 11:56:12,384:INFO:[LightGBM] [Info] Total Bins 1813
2025-02-26 11:56:12,385:INFO:[LightGBM] [Info] Number of data points in the train set: 5729, number of used features: 17
2025-02-26 11:56:12,385:INFO:[LightGBM] [Info] Start training from score 1965.296038
2025-02-26 11:56:12,820:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:56:33,191:INFO:Creating Dashboard logs
2025-02-26 11:56:33,194:INFO:Model: Extra Trees Regressor
2025-02-26 11:56:33,205:INFO:Logged params: {'bootstrap': False, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False}
2025-02-26 11:56:33,233:INFO:Initializing predict_model()
2025-02-26 11:56:33,233:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x16d9cd7b0>, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=123), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=False, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x2957df400>)
2025-02-26 11:56:33,233:INFO:Checking exceptions
2025-02-26 11:56:33,233:INFO:Preloading libraries
2025-02-26 11:56:33,717:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.
  warnings.warn(

2025-02-26 11:56:34,575:INFO:Uploading results into container
2025-02-26 11:56:34,575:INFO:Uploading model into container now
2025-02-26 11:56:34,581:INFO:_master_model_container: 21
2025-02-26 11:56:34,581:INFO:_display_container: 3
2025-02-26 11:56:34,581:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=123)
2025-02-26 11:56:34,581:INFO:create_model() successfully completed......................................
2025-02-26 11:56:34,680:INFO:Initializing tune_model()
2025-02-26 11:56:34,681:INFO:tune_model(estimator=ExtraTreesRegressor(n_jobs=-1, random_state=123), fold=None, round=4, n_iter=50, custom_grid=None, optimize=RMSLE, custom_scorer=None, search_library=optuna, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=True, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.regression.oop.RegressionExperiment object at 0x16d9cd7b0>)
2025-02-26 11:56:34,681:INFO:Checking exceptions
2025-02-26 11:56:34,681:INFO:Soft dependency imported: optuna: 4.1.0
2025-02-26 11:56:34,728:INFO:Copying training dataset
2025-02-26 11:56:34,734:INFO:Checking base model
2025-02-26 11:56:34,734:INFO:Base model : Extra Trees Regressor
2025-02-26 11:56:34,736:INFO:Declaring metric variables
2025-02-26 11:56:34,737:INFO:Defining Hyperparameters
2025-02-26 11:56:34,931:INFO:Tuning with n_jobs=-1
2025-02-26 11:56:34,932:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/optuna/_experimental.py:31: ExperimentalWarning: Argument ``multivariate`` is an experimental feature. The interface can change in the future.
  warnings.warn(

2025-02-26 11:56:34,932:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/optuna/_experimental.py:31: ExperimentalWarning: Argument ``constant_liar`` is an experimental feature. The interface can change in the future.
  warnings.warn(

2025-02-26 11:56:34,932:INFO:Initializing optuna.integration.OptunaSearchCV
2025-02-26 11:56:34,939:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/pycaret_experiment/supervised_experiment.py:2458: ExperimentalWarning: OptunaSearchCV is experimental (supported from v0.17.0). The interface can change in the future.
  model_grid = optuna.integration.OptunaSearchCV(  # type: ignore

2025-02-26 11:57:27,578:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:57:27,919:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:57:28,071:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:57:28,399:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:57:28,442:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:57:28,776:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:57:28,828:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 11:57:28,957:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:03:06,359:INFO:Initializing create_model()
2025-02-26 12:03:06,360:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x16d9cd7b0>, estimator=catboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-26 12:03:06,361:INFO:Checking exceptions
2025-02-26 12:03:06,376:INFO:Importing libraries
2025-02-26 12:03:06,377:INFO:Copying training dataset
2025-02-26 12:03:06,389:INFO:Defining folds
2025-02-26 12:03:06,389:INFO:Declaring metric variables
2025-02-26 12:03:06,392:INFO:Importing untrained model
2025-02-26 12:03:06,395:INFO:CatBoost Regressor Imported successfully
2025-02-26 12:03:06,399:INFO:Starting cross validation
2025-02-26 12:03:06,557:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-26 12:03:07,989:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:03:07,990:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:03:08,012:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006396 seconds.
2025-02-26 12:03:08,013:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:03:08,013:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:03:08,013:INFO:[LightGBM] [Info] Total Bins 1757
2025-02-26 12:03:08,013:INFO:[LightGBM] [Info] Number of data points in the train set: 5152, number of used features: 17
2025-02-26 12:03:08,013:INFO:[LightGBM] [Info] Start training from score 1965.258152
2025-02-26 12:03:09,274:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:03:09,274:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:03:09,279:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000495 seconds.
2025-02-26 12:03:09,279:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:03:09,279:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:03:09,279:INFO:[LightGBM] [Info] Total Bins 2212
2025-02-26 12:03:09,279:INFO:[LightGBM] [Info] Number of data points in the train set: 8520, number of used features: 17
2025-02-26 12:03:09,280:INFO:[LightGBM] [Info] Start training from score 1.608099
2025-02-26 12:03:10,370:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:03:10,370:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:03:10,378:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000935 seconds.
2025-02-26 12:03:10,378:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:03:10,378:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:03:10,378:INFO:[LightGBM] [Info] Total Bins 1919
2025-02-26 12:03:10,379:INFO:[LightGBM] [Info] Number of data points in the train set: 7690, number of used features: 17
2025-02-26 12:03:10,379:INFO:[LightGBM] [Info] Start training from score 14.825618
2025-02-26 12:03:11,999:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:03:12,000:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:03:12,004:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001247 seconds.
2025-02-26 12:03:12,004:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:03:12,004:INFO:[LightGBM] [Info] Total Bins 1758
2025-02-26 12:03:12,004:INFO:[LightGBM] [Info] Number of data points in the train set: 5152, number of used features: 17
2025-02-26 12:03:12,005:INFO:[LightGBM] [Info] Start training from score 1965.258152
2025-02-26 12:03:15,362:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:03:15,362:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:03:15,365:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000822 seconds.
2025-02-26 12:03:15,365:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:03:15,365:INFO:[LightGBM] [Info] Total Bins 2212
2025-02-26 12:03:15,365:INFO:[LightGBM] [Info] Number of data points in the train set: 8520, number of used features: 17
2025-02-26 12:03:15,366:INFO:[LightGBM] [Info] Start training from score 1.608099
2025-02-26 12:03:18,716:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:03:18,717:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:03:18,720:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000578 seconds.
2025-02-26 12:03:18,720:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:03:18,720:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:03:18,720:INFO:[LightGBM] [Info] Total Bins 1919
2025-02-26 12:03:18,720:INFO:[LightGBM] [Info] Number of data points in the train set: 7690, number of used features: 17
2025-02-26 12:03:18,721:INFO:[LightGBM] [Info] Start training from score 14.825618
2025-02-26 12:03:22,767:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:03:22,767:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:03:22,769:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000938 seconds.
2025-02-26 12:03:22,770:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:03:22,770:INFO:[LightGBM] [Info] Total Bins 1758
2025-02-26 12:03:22,770:INFO:[LightGBM] [Info] Number of data points in the train set: 5152, number of used features: 17
2025-02-26 12:03:22,771:INFO:[LightGBM] [Info] Start training from score 1965.258152
2025-02-26 12:03:26,073:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:03:26,073:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:03:26,079:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002437 seconds.
2025-02-26 12:03:26,079:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:03:26,080:INFO:[LightGBM] [Info] Total Bins 2212
2025-02-26 12:03:26,081:INFO:[LightGBM] [Info] Number of data points in the train set: 8520, number of used features: 17
2025-02-26 12:03:26,082:INFO:[LightGBM] [Info] Start training from score 1.608099
2025-02-26 12:03:29,536:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:03:29,536:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:03:29,598:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.030112 seconds.
2025-02-26 12:03:29,598:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:03:29,600:INFO:[LightGBM] [Info] Total Bins 1919
2025-02-26 12:03:29,601:INFO:[LightGBM] [Info] Number of data points in the train set: 7690, number of used features: 17
2025-02-26 12:03:29,623:INFO:[LightGBM] [Info] Start training from score 14.825618
2025-02-26 12:03:32,934:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:03:32,934:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:03:32,936:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000556 seconds.
2025-02-26 12:03:32,936:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:03:32,936:INFO:[LightGBM] [Info] Total Bins 1757
2025-02-26 12:03:32,936:INFO:[LightGBM] [Info] Number of data points in the train set: 5152, number of used features: 17
2025-02-26 12:03:32,937:INFO:[LightGBM] [Info] Start training from score 1965.258152
2025-02-26 12:03:36,474:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:03:36,474:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:03:36,478:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001048 seconds.
2025-02-26 12:03:36,478:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:03:36,478:INFO:[LightGBM] [Info] Total Bins 2212
2025-02-26 12:03:36,478:INFO:[LightGBM] [Info] Number of data points in the train set: 8520, number of used features: 17
2025-02-26 12:03:36,479:INFO:[LightGBM] [Info] Start training from score 1.608099
2025-02-26 12:03:40,176:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:03:40,176:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:03:40,179:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000919 seconds.
2025-02-26 12:03:40,179:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:03:40,179:INFO:[LightGBM] [Info] Total Bins 1920
2025-02-26 12:03:40,180:INFO:[LightGBM] [Info] Number of data points in the train set: 7690, number of used features: 17
2025-02-26 12:03:40,181:INFO:[LightGBM] [Info] Start training from score 14.825618
2025-02-26 12:03:43,729:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:03:43,729:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:03:43,732:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000753 seconds.
2025-02-26 12:03:43,733:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:03:43,733:INFO:[LightGBM] [Info] Total Bins 1757
2025-02-26 12:03:43,733:INFO:[LightGBM] [Info] Number of data points in the train set: 5152, number of used features: 17
2025-02-26 12:03:43,733:INFO:[LightGBM] [Info] Start training from score 1965.258152
2025-02-26 12:03:58,815:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:04:00,096:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:04:00,355:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:04:00,753:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:04:00,969:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:04:01,010:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:04:01,010:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:04:01,013:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000333 seconds.
2025-02-26 12:04:01,013:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:04:01,013:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:04:01,013:INFO:[LightGBM] [Info] Total Bins 1844
2025-02-26 12:04:01,013:INFO:[LightGBM] [Info] Number of data points in the train set: 8520, number of used features: 17
2025-02-26 12:04:01,118:INFO:[LightGBM] [Info] Start training from score 1.609038
2025-02-26 12:04:01,355:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:04:01,449:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:04:01,780:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:04:07,637:INFO:Initializing create_model()
2025-02-26 12:04:07,637:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x16d9cd7b0>, estimator=et, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-26 12:04:07,637:INFO:Checking exceptions
2025-02-26 12:04:07,648:INFO:Importing libraries
2025-02-26 12:04:07,648:INFO:Copying training dataset
2025-02-26 12:04:07,665:INFO:Defining folds
2025-02-26 12:04:07,665:INFO:Declaring metric variables
2025-02-26 12:04:07,670:INFO:Importing untrained model
2025-02-26 12:04:07,673:INFO:Extra Trees Regressor Imported successfully
2025-02-26 12:04:07,678:INFO:Starting cross validation
2025-02-26 12:04:07,827:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-26 12:04:08,407:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:04:08,407:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:04:08,422:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001641 seconds.
2025-02-26 12:04:08,422:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:04:08,423:INFO:[LightGBM] [Info] Total Bins 1766
2025-02-26 12:04:08,424:INFO:[LightGBM] [Info] Number of data points in the train set: 7690, number of used features: 17
2025-02-26 12:04:08,427:INFO:[LightGBM] [Info] Start training from score 14.825618
2025-02-26 12:04:08,489:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:04:08,489:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:04:08,493:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000989 seconds.
2025-02-26 12:04:08,493:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:04:08,493:INFO:[LightGBM] [Info] Total Bins 1767
2025-02-26 12:04:08,493:INFO:[LightGBM] [Info] Number of data points in the train set: 7681, number of used features: 17
2025-02-26 12:04:08,510:INFO:[LightGBM] [Info] Start training from score 14.834266
2025-02-26 12:04:09,715:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:04:09,716:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:04:09,720:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001142 seconds.
2025-02-26 12:04:09,720:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:04:09,720:INFO:[LightGBM] [Info] Total Bins 1920
2025-02-26 12:04:09,721:INFO:[LightGBM] [Info] Number of data points in the train set: 7681, number of used features: 17
2025-02-26 12:04:09,721:INFO:[LightGBM] [Info] Start training from score 14.834266
2025-02-26 12:04:10,102:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:04:10,102:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:04:10,104:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000237 seconds.
2025-02-26 12:04:10,104:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:04:10,104:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:04:10,104:INFO:[LightGBM] [Info] Total Bins 1771
2025-02-26 12:04:10,105:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-26 12:04:10,105:INFO:[LightGBM] [Info] Start training from score 1965.251163
2025-02-26 12:04:10,521:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:04:10,521:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:04:10,525:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000668 seconds.
2025-02-26 12:04:10,525:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:04:10,525:INFO:[LightGBM] [Info] Total Bins 1771
2025-02-26 12:04:10,525:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-26 12:04:10,526:INFO:[LightGBM] [Info] Start training from score 1965.251163
2025-02-26 12:04:10,875:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:04:10,875:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:04:10,877:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000710 seconds.
2025-02-26 12:04:10,878:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:04:10,878:INFO:[LightGBM] [Info] Total Bins 1771
2025-02-26 12:04:10,878:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-26 12:04:10,878:INFO:[LightGBM] [Info] Start training from score 1965.251163
2025-02-26 12:04:11,055:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:04:11,055:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:04:11,061:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001062 seconds.
2025-02-26 12:04:11,061:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:04:11,061:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:04:11,061:INFO:[LightGBM] [Info] Total Bins 2215
2025-02-26 12:04:11,061:INFO:[LightGBM] [Info] Number of data points in the train set: 8520, number of used features: 17
2025-02-26 12:04:11,062:INFO:[LightGBM] [Info] Start training from score 1.609038
2025-02-26 12:04:11,279:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:04:11,279:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:04:11,282:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000900 seconds.
2025-02-26 12:04:11,282:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:04:11,282:INFO:[LightGBM] [Info] Total Bins 1757
2025-02-26 12:04:11,283:INFO:[LightGBM] [Info] Number of data points in the train set: 5152, number of used features: 17
2025-02-26 12:04:11,283:INFO:[LightGBM] [Info] Start training from score 1965.258152
2025-02-26 12:04:12,240:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:04:12,240:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:04:12,242:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000599 seconds.
2025-02-26 12:04:12,243:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:04:12,243:INFO:[LightGBM] [Info] Total Bins 1771
2025-02-26 12:04:12,243:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-26 12:04:12,243:INFO:[LightGBM] [Info] Start training from score 1965.251163
2025-02-26 12:04:13,138:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:04:13,138:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:04:13,138:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:04:13,138:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:04:13,144:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001021 seconds.
2025-02-26 12:04:13,144:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:04:13,144:INFO:[LightGBM] [Info] Total Bins 2215
2025-02-26 12:04:13,144:INFO:[LightGBM] [Info] Number of data points in the train set: 8520, number of used features: 17
2025-02-26 12:04:13,145:INFO:[LightGBM] [Info] Start training from score 1.609038
2025-02-26 12:04:13,145:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001118 seconds.
2025-02-26 12:04:13,145:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:04:13,145:INFO:[LightGBM] [Info] Total Bins 2215
2025-02-26 12:04:13,145:INFO:[LightGBM] [Info] Number of data points in the train set: 8520, number of used features: 17
2025-02-26 12:04:13,146:INFO:[LightGBM] [Info] Start training from score 1.609038
2025-02-26 12:04:13,397:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:04:13,397:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:04:13,403:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001267 seconds.
2025-02-26 12:04:13,403:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:04:13,403:INFO:[LightGBM] [Info] Total Bins 2215
2025-02-26 12:04:13,403:INFO:[LightGBM] [Info] Number of data points in the train set: 8520, number of used features: 17
2025-02-26 12:04:13,404:INFO:[LightGBM] [Info] Start training from score 1.609038
2025-02-26 12:04:13,847:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:04:13,847:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:04:13,853:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001537 seconds.
2025-02-26 12:04:13,853:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:04:13,853:INFO:[LightGBM] [Info] Total Bins 2212
2025-02-26 12:04:13,854:INFO:[LightGBM] [Info] Number of data points in the train set: 8520, number of used features: 17
2025-02-26 12:04:13,855:INFO:[LightGBM] [Info] Start training from score 1.608099
2025-02-26 12:04:14,384:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:04:14,384:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:04:14,391:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003397 seconds.
2025-02-26 12:04:14,391:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:04:14,391:INFO:[LightGBM] [Info] Total Bins 1920
2025-02-26 12:04:14,392:INFO:[LightGBM] [Info] Number of data points in the train set: 7681, number of used features: 17
2025-02-26 12:04:14,395:INFO:[LightGBM] [Info] Start training from score 14.834266
2025-02-26 12:04:16,162:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:04:16,162:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:04:16,176:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004092 seconds.
2025-02-26 12:04:16,177:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:04:16,177:INFO:[LightGBM] [Info] Total Bins 2215
2025-02-26 12:04:16,177:INFO:[LightGBM] [Info] Number of data points in the train set: 8520, number of used features: 17
2025-02-26 12:04:16,179:INFO:[LightGBM] [Info] Start training from score 1.609038
2025-02-26 12:04:18,098:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:04:18,098:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:04:18,104:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000948 seconds.
2025-02-26 12:04:18,104:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:04:18,104:INFO:[LightGBM] [Info] Total Bins 1920
2025-02-26 12:04:18,104:INFO:[LightGBM] [Info] Number of data points in the train set: 7681, number of used features: 17
2025-02-26 12:04:18,105:INFO:[LightGBM] [Info] Start training from score 14.834266
2025-02-26 12:04:18,207:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:04:18,207:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:04:18,211:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001208 seconds.
2025-02-26 12:04:18,211:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:04:18,211:INFO:[LightGBM] [Info] Total Bins 1920
2025-02-26 12:04:18,211:INFO:[LightGBM] [Info] Number of data points in the train set: 7681, number of used features: 17
2025-02-26 12:04:18,212:INFO:[LightGBM] [Info] Start training from score 14.834266
2025-02-26 12:04:18,813:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:04:18,813:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:04:18,816:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001040 seconds.
2025-02-26 12:04:18,816:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:04:18,816:INFO:[LightGBM] [Info] Total Bins 1920
2025-02-26 12:04:18,817:INFO:[LightGBM] [Info] Number of data points in the train set: 7681, number of used features: 17
2025-02-26 12:04:18,817:INFO:[LightGBM] [Info] Start training from score 14.834266
2025-02-26 12:04:19,062:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:04:19,062:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:04:19,065:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000748 seconds.
2025-02-26 12:04:19,065:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:04:19,065:INFO:[LightGBM] [Info] Total Bins 1919
2025-02-26 12:04:19,065:INFO:[LightGBM] [Info] Number of data points in the train set: 7690, number of used features: 17
2025-02-26 12:04:19,066:INFO:[LightGBM] [Info] Start training from score 14.825618
2025-02-26 12:04:20,236:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:04:20,236:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:04:20,238:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000759 seconds.
2025-02-26 12:04:20,238:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:04:20,238:INFO:[LightGBM] [Info] Total Bins 1771
2025-02-26 12:04:20,238:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-26 12:04:20,239:INFO:[LightGBM] [Info] Start training from score 1965.251163
2025-02-26 12:04:22,643:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:04:22,643:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:04:22,648:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002414 seconds.
2025-02-26 12:04:22,648:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:04:22,648:INFO:[LightGBM] [Info] Total Bins 1921
2025-02-26 12:04:22,649:INFO:[LightGBM] [Info] Number of data points in the train set: 7681, number of used features: 17
2025-02-26 12:04:22,650:INFO:[LightGBM] [Info] Start training from score 14.834266
2025-02-26 12:04:24,366:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:04:24,366:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:04:24,370:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001570 seconds.
2025-02-26 12:04:24,370:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:04:24,370:INFO:[LightGBM] [Info] Total Bins 1771
2025-02-26 12:04:24,370:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-26 12:04:24,371:INFO:[LightGBM] [Info] Start training from score 1965.251163
2025-02-26 12:04:24,941:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:04:24,942:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:04:24,945:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:04:24,946:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:04:24,949:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000694 seconds.
2025-02-26 12:04:24,949:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:04:24,949:INFO:[LightGBM] [Info] Total Bins 1771
2025-02-26 12:04:24,949:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-26 12:04:24,950:INFO:[LightGBM] [Info] Start training from score 1965.251163
2025-02-26 12:04:24,963:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008476 seconds.
2025-02-26 12:04:24,963:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:04:24,965:INFO:[LightGBM] [Info] Total Bins 1771
2025-02-26 12:04:24,966:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-26 12:04:24,972:INFO:[LightGBM] [Info] Start training from score 1965.251163
2025-02-26 12:04:25,447:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:04:25,447:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:04:25,451:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000859 seconds.
2025-02-26 12:04:25,451:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:04:25,451:INFO:[LightGBM] [Info] Total Bins 1758
2025-02-26 12:04:25,451:INFO:[LightGBM] [Info] Number of data points in the train set: 5152, number of used features: 17
2025-02-26 12:04:25,452:INFO:[LightGBM] [Info] Start training from score 1965.258152
2025-02-26 12:04:27,505:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:04:27,505:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:04:27,509:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001090 seconds.
2025-02-26 12:04:27,509:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:04:27,509:INFO:[LightGBM] [Info] Total Bins 2215
2025-02-26 12:04:27,509:INFO:[LightGBM] [Info] Number of data points in the train set: 8520, number of used features: 17
2025-02-26 12:04:27,510:INFO:[LightGBM] [Info] Start training from score 1.609038
2025-02-26 12:04:30,928:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:04:30,928:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:04:31,037:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.062595 seconds.
2025-02-26 12:04:31,038:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:04:31,038:INFO:[LightGBM] [Info] Total Bins 1844
2025-02-26 12:04:31,039:INFO:[LightGBM] [Info] Number of data points in the train set: 8520, number of used features: 17
2025-02-26 12:04:31,044:INFO:[LightGBM] [Info] Start training from score 1.609038
2025-02-26 12:04:31,694:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:04:31,694:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:04:31,696:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000608 seconds.
2025-02-26 12:04:31,696:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:04:31,696:INFO:[LightGBM] [Info] Total Bins 1771
2025-02-26 12:04:31,696:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-26 12:04:31,697:INFO:[LightGBM] [Info] Start training from score 1965.251163
2025-02-26 12:04:33,358:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:04:33,359:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:04:33,364:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001743 seconds.
2025-02-26 12:04:33,364:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:04:33,364:INFO:[LightGBM] [Info] Total Bins 2215
2025-02-26 12:04:33,364:INFO:[LightGBM] [Info] Number of data points in the train set: 8520, number of used features: 17
2025-02-26 12:04:33,366:INFO:[LightGBM] [Info] Start training from score 1.609038
2025-02-26 12:04:33,654:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:04:33,654:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:04:33,657:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001012 seconds.
2025-02-26 12:04:33,657:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:04:33,657:INFO:[LightGBM] [Info] Total Bins 2215
2025-02-26 12:04:33,658:INFO:[LightGBM] [Info] Number of data points in the train set: 8520, number of used features: 17
2025-02-26 12:04:33,659:INFO:[LightGBM] [Info] Start training from score 1.609038
2025-02-26 12:04:33,897:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:04:33,897:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:04:33,901:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000639 seconds.
2025-02-26 12:04:33,901:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:04:33,901:INFO:[LightGBM] [Info] Total Bins 2212
2025-02-26 12:04:33,901:INFO:[LightGBM] [Info] Number of data points in the train set: 8520, number of used features: 17
2025-02-26 12:04:33,902:INFO:[LightGBM] [Info] Start training from score 1.608099
2025-02-26 12:04:34,170:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:04:34,170:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:04:34,173:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000423 seconds.
2025-02-26 12:04:34,173:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:04:34,173:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:04:34,173:INFO:[LightGBM] [Info] Total Bins 2215
2025-02-26 12:04:34,173:INFO:[LightGBM] [Info] Number of data points in the train set: 8520, number of used features: 17
2025-02-26 12:04:34,173:INFO:[LightGBM] [Info] Start training from score 1.609038
2025-02-26 12:04:36,530:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:04:36,530:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:04:36,662:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005752 seconds.
2025-02-26 12:04:36,662:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:04:36,662:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:04:36,662:INFO:[LightGBM] [Info] Total Bins 1921
2025-02-26 12:04:36,662:INFO:[LightGBM] [Info] Number of data points in the train set: 7681, number of used features: 17
2025-02-26 12:04:36,663:INFO:[LightGBM] [Info] Start training from score 14.834266
2025-02-26 12:04:38,059:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:04:38,059:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:04:38,062:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000908 seconds.
2025-02-26 12:04:38,062:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:04:38,062:INFO:[LightGBM] [Info] Total Bins 1767
2025-02-26 12:04:38,062:INFO:[LightGBM] [Info] Number of data points in the train set: 7681, number of used features: 17
2025-02-26 12:04:38,063:INFO:[LightGBM] [Info] Start training from score 14.834266
2025-02-26 12:04:38,489:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:04:38,489:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:04:38,493:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001780 seconds.
2025-02-26 12:04:38,493:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:04:38,494:INFO:[LightGBM] [Info] Total Bins 2215
2025-02-26 12:04:38,494:INFO:[LightGBM] [Info] Number of data points in the train set: 8520, number of used features: 17
2025-02-26 12:04:38,496:INFO:[LightGBM] [Info] Start training from score 1.609038
2025-02-26 12:04:40,391:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:04:40,391:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:04:40,395:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001292 seconds.
2025-02-26 12:04:40,395:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:04:40,395:INFO:[LightGBM] [Info] Total Bins 1921
2025-02-26 12:04:40,395:INFO:[LightGBM] [Info] Number of data points in the train set: 7681, number of used features: 17
2025-02-26 12:04:40,396:INFO:[LightGBM] [Info] Start training from score 14.834266
2025-02-26 12:04:40,584:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:04:40,585:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:04:40,588:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001449 seconds.
2025-02-26 12:04:40,588:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:04:40,588:INFO:[LightGBM] [Info] Total Bins 1921
2025-02-26 12:04:40,589:INFO:[LightGBM] [Info] Number of data points in the train set: 7681, number of used features: 17
2025-02-26 12:04:40,590:INFO:[LightGBM] [Info] Start training from score 14.834266
2025-02-26 12:04:40,988:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:04:40,988:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:04:40,992:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001006 seconds.
2025-02-26 12:04:40,992:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:04:40,992:INFO:[LightGBM] [Info] Total Bins 1919
2025-02-26 12:04:40,992:INFO:[LightGBM] [Info] Number of data points in the train set: 7690, number of used features: 17
2025-02-26 12:04:40,993:INFO:[LightGBM] [Info] Start training from score 14.825618
2025-02-26 12:04:41,906:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:04:41,906:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:04:41,909:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001055 seconds.
2025-02-26 12:04:41,910:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:04:41,910:INFO:[LightGBM] [Info] Total Bins 1921
2025-02-26 12:04:41,910:INFO:[LightGBM] [Info] Number of data points in the train set: 7681, number of used features: 17
2025-02-26 12:04:41,911:INFO:[LightGBM] [Info] Start training from score 14.834266
2025-02-26 12:04:44,899:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:04:44,899:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:04:44,903:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001949 seconds.
2025-02-26 12:04:44,903:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:04:44,903:INFO:[LightGBM] [Info] Total Bins 1771
2025-02-26 12:04:44,904:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-26 12:04:44,905:INFO:[LightGBM] [Info] Start training from score 1965.251163
2025-02-26 12:04:45,056:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:04:45,057:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:04:45,061:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001927 seconds.
2025-02-26 12:04:45,061:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:04:45,061:INFO:[LightGBM] [Info] Total Bins 1771
2025-02-26 12:04:45,061:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-26 12:04:45,063:INFO:[LightGBM] [Info] Start training from score 1965.251163
2025-02-26 12:04:45,834:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:04:45,834:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:04:45,848:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007368 seconds.
2025-02-26 12:04:45,848:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:04:45,848:INFO:[LightGBM] [Info] Total Bins 1920
2025-02-26 12:04:45,849:INFO:[LightGBM] [Info] Number of data points in the train set: 7681, number of used features: 17
2025-02-26 12:04:45,856:INFO:[LightGBM] [Info] Start training from score 14.834266
2025-02-26 12:04:47,115:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:04:47,115:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:04:47,139:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017464 seconds.
2025-02-26 12:04:47,139:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:04:47,139:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:04:47,139:INFO:[LightGBM] [Info] Total Bins 1771
2025-02-26 12:04:47,151:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-26 12:04:47,152:INFO:[LightGBM] [Info] Start training from score 1965.251163
2025-02-26 12:04:47,239:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:04:47,240:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:04:47,244:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001554 seconds.
2025-02-26 12:04:47,244:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:04:47,244:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:04:47,245:INFO:[LightGBM] [Info] Total Bins 1771
2025-02-26 12:04:47,245:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-26 12:04:47,246:INFO:[LightGBM] [Info] Start training from score 1965.251163
2025-02-26 12:04:48,606:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:04:48,606:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:04:48,609:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000933 seconds.
2025-02-26 12:04:48,609:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:04:48,609:INFO:[LightGBM] [Info] Total Bins 1758
2025-02-26 12:04:48,609:INFO:[LightGBM] [Info] Number of data points in the train set: 5152, number of used features: 17
2025-02-26 12:04:48,610:INFO:[LightGBM] [Info] Start training from score 1965.258152
2025-02-26 12:04:49,104:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:04:49,104:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:04:49,107:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001306 seconds.
2025-02-26 12:04:49,107:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:04:49,107:INFO:[LightGBM] [Info] Total Bins 1771
2025-02-26 12:04:49,108:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-26 12:04:49,109:INFO:[LightGBM] [Info] Start training from score 1965.251163
2025-02-26 12:04:52,342:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:04:52,342:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:04:52,346:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001418 seconds.
2025-02-26 12:04:52,346:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:04:52,346:INFO:[LightGBM] [Info] Total Bins 2215
2025-02-26 12:04:52,346:INFO:[LightGBM] [Info] Number of data points in the train set: 8520, number of used features: 17
2025-02-26 12:04:52,352:INFO:[LightGBM] [Info] Start training from score 1.609038
2025-02-26 12:04:52,566:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:04:52,567:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:04:52,571:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001048 seconds.
2025-02-26 12:04:52,571:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:04:52,571:INFO:[LightGBM] [Info] Total Bins 2215
2025-02-26 12:04:52,571:INFO:[LightGBM] [Info] Number of data points in the train set: 8520, number of used features: 17
2025-02-26 12:04:52,572:INFO:[LightGBM] [Info] Start training from score 1.609038
2025-02-26 12:04:53,067:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:04:53,067:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:04:53,071:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001669 seconds.
2025-02-26 12:04:53,071:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:04:53,071:INFO:[LightGBM] [Info] Total Bins 1771
2025-02-26 12:04:53,071:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-26 12:04:53,072:INFO:[LightGBM] [Info] Start training from score 1965.251163
2025-02-26 12:04:54,727:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:04:54,727:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:04:54,731:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001326 seconds.
2025-02-26 12:04:54,731:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:04:54,731:INFO:[LightGBM] [Info] Total Bins 2215
2025-02-26 12:04:54,731:INFO:[LightGBM] [Info] Number of data points in the train set: 8520, number of used features: 17
2025-02-26 12:04:54,732:INFO:[LightGBM] [Info] Start training from score 1.609038
2025-02-26 12:04:55,141:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:04:55,141:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:04:55,144:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000826 seconds.
2025-02-26 12:04:55,144:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:04:55,144:INFO:[LightGBM] [Info] Total Bins 2212
2025-02-26 12:04:55,145:INFO:[LightGBM] [Info] Number of data points in the train set: 8520, number of used features: 17
2025-02-26 12:04:55,145:INFO:[LightGBM] [Info] Start training from score 1.608099
2025-02-26 12:04:57,079:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:04:57,080:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:04:57,711:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.491830 seconds.
2025-02-26 12:04:57,711:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:04:57,711:INFO:[LightGBM] [Info] Total Bins 2215
2025-02-26 12:04:57,748:INFO:[LightGBM] [Info] Number of data points in the train set: 8520, number of used features: 17
2025-02-26 12:04:57,802:INFO:[LightGBM] [Info] Start training from score 1.609038
2025-02-26 12:04:58,115:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:04:58,115:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:04:58,119:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000501 seconds.
2025-02-26 12:04:58,119:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:04:58,119:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:04:58,119:INFO:[LightGBM] [Info] Total Bins 2215
2025-02-26 12:04:58,119:INFO:[LightGBM] [Info] Number of data points in the train set: 8520, number of used features: 17
2025-02-26 12:04:58,119:INFO:[LightGBM] [Info] Start training from score 1.609038
2025-02-26 12:05:00,998:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:05:00,998:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:05:01,001:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000449 seconds.
2025-02-26 12:05:01,001:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:05:01,001:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:05:01,001:INFO:[LightGBM] [Info] Total Bins 1920
2025-02-26 12:05:01,001:INFO:[LightGBM] [Info] Number of data points in the train set: 7681, number of used features: 17
2025-02-26 12:05:01,002:INFO:[LightGBM] [Info] Start training from score 14.834266
2025-02-26 12:05:01,281:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:05:01,281:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:05:01,286:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001351 seconds.
2025-02-26 12:05:01,286:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:05:01,286:INFO:[LightGBM] [Info] Total Bins 1920
2025-02-26 12:05:01,286:INFO:[LightGBM] [Info] Number of data points in the train set: 7681, number of used features: 17
2025-02-26 12:05:01,287:INFO:[LightGBM] [Info] Start training from score 14.834266
2025-02-26 12:05:01,368:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:05:01,369:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:05:01,373:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001033 seconds.
2025-02-26 12:05:01,373:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:05:01,373:INFO:[LightGBM] [Info] Total Bins 2215
2025-02-26 12:05:01,373:INFO:[LightGBM] [Info] Number of data points in the train set: 8520, number of used features: 17
2025-02-26 12:05:01,374:INFO:[LightGBM] [Info] Start training from score 1.609038
2025-02-26 12:05:03,575:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:05:03,575:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:05:03,579:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000768 seconds.
2025-02-26 12:05:03,579:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:05:03,579:INFO:[LightGBM] [Info] Total Bins 1919
2025-02-26 12:05:03,579:INFO:[LightGBM] [Info] Number of data points in the train set: 7690, number of used features: 17
2025-02-26 12:05:03,579:INFO:[LightGBM] [Info] Start training from score 14.825618
2025-02-26 12:05:03,826:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:05:03,826:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:05:03,901:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037587 seconds.
2025-02-26 12:05:03,901:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:05:03,901:INFO:[LightGBM] [Info] Total Bins 1920
2025-02-26 12:05:04,020:INFO:[LightGBM] [Info] Number of data points in the train set: 7681, number of used features: 17
2025-02-26 12:05:04,175:INFO:[LightGBM] [Info] Start training from score 14.834266
2025-02-26 12:05:05,322:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:05:05,323:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:05:05,326:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001120 seconds.
2025-02-26 12:05:05,326:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:05:05,326:INFO:[LightGBM] [Info] Total Bins 1920
2025-02-26 12:05:05,326:INFO:[LightGBM] [Info] Number of data points in the train set: 7681, number of used features: 17
2025-02-26 12:05:05,327:INFO:[LightGBM] [Info] Start training from score 14.834266
2025-02-26 12:05:06,273:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:05:06,273:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:05:06,278:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000802 seconds.
2025-02-26 12:05:06,278:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:05:06,278:INFO:[LightGBM] [Info] Total Bins 1920
2025-02-26 12:05:06,278:INFO:[LightGBM] [Info] Number of data points in the train set: 7681, number of used features: 17
2025-02-26 12:05:06,279:INFO:[LightGBM] [Info] Start training from score 14.834266
2025-02-26 12:05:09,900:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:05:09,901:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:05:09,904:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001152 seconds.
2025-02-26 12:05:09,904:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:05:09,904:INFO:[LightGBM] [Info] Total Bins 1920
2025-02-26 12:05:09,904:INFO:[LightGBM] [Info] Number of data points in the train set: 7681, number of used features: 17
2025-02-26 12:05:09,906:INFO:[LightGBM] [Info] Start training from score 14.834266
2025-02-26 12:05:09,967:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:05:09,967:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:05:09,970:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000611 seconds.
2025-02-26 12:05:09,970:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:05:09,970:INFO:[LightGBM] [Info] Total Bins 1771
2025-02-26 12:05:09,970:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-26 12:05:09,971:INFO:[LightGBM] [Info] Start training from score 1965.251163
2025-02-26 12:05:10,350:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:05:10,350:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:05:10,353:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001223 seconds.
2025-02-26 12:05:10,353:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:05:10,353:INFO:[LightGBM] [Info] Total Bins 1771
2025-02-26 12:05:10,354:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-26 12:05:10,355:INFO:[LightGBM] [Info] Start training from score 1965.251163
2025-02-26 12:05:12,175:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:05:12,175:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:05:12,179:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001736 seconds.
2025-02-26 12:05:12,179:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:05:12,179:INFO:[LightGBM] [Info] Total Bins 1757
2025-02-26 12:05:12,180:INFO:[LightGBM] [Info] Number of data points in the train set: 5152, number of used features: 17
2025-02-26 12:05:12,181:INFO:[LightGBM] [Info] Start training from score 1965.258152
2025-02-26 12:05:12,379:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:05:12,379:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:05:12,415:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001573 seconds.
2025-02-26 12:05:12,415:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:05:12,415:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:05:12,415:INFO:[LightGBM] [Info] Total Bins 1771
2025-02-26 12:05:12,415:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-26 12:05:12,417:INFO:[LightGBM] [Info] Start training from score 1965.251163
2025-02-26 12:05:13,090:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:05:13,090:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:05:13,093:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001191 seconds.
2025-02-26 12:05:13,093:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:05:13,093:INFO:[LightGBM] [Info] Total Bins 1771
2025-02-26 12:05:13,093:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-26 12:05:13,094:INFO:[LightGBM] [Info] Start training from score 1965.251163
2025-02-26 12:05:14,430:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:05:14,430:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:05:14,432:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001205 seconds.
2025-02-26 12:05:14,432:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:05:14,433:INFO:[LightGBM] [Info] Total Bins 1771
2025-02-26 12:05:14,433:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-26 12:05:14,434:INFO:[LightGBM] [Info] Start training from score 1965.251163
2025-02-26 12:05:15,882:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:05:15,882:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:05:15,887:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001335 seconds.
2025-02-26 12:05:15,887:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:05:15,887:INFO:[LightGBM] [Info] Total Bins 2215
2025-02-26 12:05:15,887:INFO:[LightGBM] [Info] Number of data points in the train set: 8520, number of used features: 17
2025-02-26 12:05:15,888:INFO:[LightGBM] [Info] Start training from score 1.609038
2025-02-26 12:05:16,817:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:05:16,817:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:05:16,822:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002376 seconds.
2025-02-26 12:05:16,822:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:05:16,822:INFO:[LightGBM] [Info] Total Bins 1771
2025-02-26 12:05:16,823:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-26 12:05:16,825:INFO:[LightGBM] [Info] Start training from score 1965.251163
2025-02-26 12:05:17,423:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:05:17,423:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:05:17,427:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000631 seconds.
2025-02-26 12:05:17,427:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:05:17,427:INFO:[LightGBM] [Info] Total Bins 1844
2025-02-26 12:05:17,427:INFO:[LightGBM] [Info] Number of data points in the train set: 8520, number of used features: 17
2025-02-26 12:05:17,427:INFO:[LightGBM] [Info] Start training from score 1.609038
2025-02-26 12:05:17,470:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:05:17,470:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:05:17,473:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000964 seconds.
2025-02-26 12:05:17,473:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:05:17,473:INFO:[LightGBM] [Info] Total Bins 2215
2025-02-26 12:05:17,473:INFO:[LightGBM] [Info] Number of data points in the train set: 8520, number of used features: 17
2025-02-26 12:05:17,475:INFO:[LightGBM] [Info] Start training from score 1.609038
2025-02-26 12:05:18,233:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:05:18,233:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:05:18,243:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004345 seconds.
2025-02-26 12:05:18,243:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:05:18,243:INFO:[LightGBM] [Info] Total Bins 2212
2025-02-26 12:05:18,244:INFO:[LightGBM] [Info] Number of data points in the train set: 8520, number of used features: 17
2025-02-26 12:05:18,247:INFO:[LightGBM] [Info] Start training from score 1.608099
2025-02-26 12:05:19,527:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:05:19,527:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:05:19,530:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000835 seconds.
2025-02-26 12:05:19,530:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:05:19,530:INFO:[LightGBM] [Info] Total Bins 2215
2025-02-26 12:05:19,530:INFO:[LightGBM] [Info] Number of data points in the train set: 8520, number of used features: 17
2025-02-26 12:05:19,531:INFO:[LightGBM] [Info] Start training from score 1.609038
2025-02-26 12:05:20,193:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:05:20,193:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:05:20,197:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000869 seconds.
2025-02-26 12:05:20,197:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:05:20,197:INFO:[LightGBM] [Info] Total Bins 2215
2025-02-26 12:05:20,197:INFO:[LightGBM] [Info] Number of data points in the train set: 8520, number of used features: 17
2025-02-26 12:05:20,198:INFO:[LightGBM] [Info] Start training from score 1.609038
2025-02-26 12:05:20,693:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:05:20,693:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:05:20,696:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000907 seconds.
2025-02-26 12:05:20,696:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:05:20,696:INFO:[LightGBM] [Info] Total Bins 2215
2025-02-26 12:05:20,696:INFO:[LightGBM] [Info] Number of data points in the train set: 8520, number of used features: 17
2025-02-26 12:05:20,697:INFO:[LightGBM] [Info] Start training from score 1.609038
2025-02-26 12:05:22,662:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:05:22,663:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:05:22,916:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000938 seconds.
2025-02-26 12:05:22,916:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:05:22,916:INFO:[LightGBM] [Info] Total Bins 1920
2025-02-26 12:05:22,916:INFO:[LightGBM] [Info] Number of data points in the train set: 7681, number of used features: 17
2025-02-26 12:05:22,917:INFO:[LightGBM] [Info] Start training from score 14.834266
2025-02-26 12:05:23,890:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:05:23,890:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:05:23,897:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003193 seconds.
2025-02-26 12:05:23,897:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:05:23,897:INFO:[LightGBM] [Info] Total Bins 1767
2025-02-26 12:05:23,897:INFO:[LightGBM] [Info] Number of data points in the train set: 7681, number of used features: 17
2025-02-26 12:05:23,899:INFO:[LightGBM] [Info] Start training from score 14.834266
2025-02-26 12:05:24,220:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:05:24,220:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:05:24,222:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000384 seconds.
2025-02-26 12:05:24,222:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:05:24,222:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:05:24,222:INFO:[LightGBM] [Info] Total Bins 1921
2025-02-26 12:05:24,222:INFO:[LightGBM] [Info] Number of data points in the train set: 7681, number of used features: 17
2025-02-26 12:05:24,223:INFO:[LightGBM] [Info] Start training from score 14.834266
2025-02-26 12:05:25,382:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:05:25,383:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:05:25,386:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001155 seconds.
2025-02-26 12:05:25,386:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:05:25,386:INFO:[LightGBM] [Info] Total Bins 1920
2025-02-26 12:05:25,386:INFO:[LightGBM] [Info] Number of data points in the train set: 7690, number of used features: 17
2025-02-26 12:05:25,388:INFO:[LightGBM] [Info] Start training from score 14.825618
2025-02-26 12:05:26,156:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:05:26,156:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:05:26,159:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000639 seconds.
2025-02-26 12:05:26,159:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:05:26,159:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:05:26,159:INFO:[LightGBM] [Info] Total Bins 1920
2025-02-26 12:05:26,160:INFO:[LightGBM] [Info] Number of data points in the train set: 7681, number of used features: 17
2025-02-26 12:05:26,161:INFO:[LightGBM] [Info] Start training from score 14.834266
2025-02-26 12:05:26,817:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:05:26,817:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:05:26,820:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001381 seconds.
2025-02-26 12:05:26,820:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:05:26,820:INFO:[LightGBM] [Info] Total Bins 1920
2025-02-26 12:05:26,821:INFO:[LightGBM] [Info] Number of data points in the train set: 7681, number of used features: 17
2025-02-26 12:05:26,822:INFO:[LightGBM] [Info] Start training from score 14.834266
2025-02-26 12:05:27,024:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:05:27,024:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:05:27,027:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000841 seconds.
2025-02-26 12:05:27,027:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:05:27,027:INFO:[LightGBM] [Info] Total Bins 1920
2025-02-26 12:05:27,027:INFO:[LightGBM] [Info] Number of data points in the train set: 7681, number of used features: 17
2025-02-26 12:05:27,028:INFO:[LightGBM] [Info] Start training from score 14.834266
2025-02-26 12:05:29,718:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:05:29,719:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:05:29,723:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001802 seconds.
2025-02-26 12:05:29,723:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:05:29,723:INFO:[LightGBM] [Info] Total Bins 1771
2025-02-26 12:05:29,723:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-26 12:05:29,725:INFO:[LightGBM] [Info] Start training from score 1965.251163
2025-02-26 12:05:30,602:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:05:30,602:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:05:30,605:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000745 seconds.
2025-02-26 12:05:30,605:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:05:30,605:INFO:[LightGBM] [Info] Total Bins 1771
2025-02-26 12:05:30,605:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-26 12:05:30,606:INFO:[LightGBM] [Info] Start training from score 1965.251163
2025-02-26 12:05:31,884:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:05:31,884:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:05:31,888:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002083 seconds.
2025-02-26 12:05:31,888:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:05:31,888:INFO:[LightGBM] [Info] Total Bins 1757
2025-02-26 12:05:31,889:INFO:[LightGBM] [Info] Number of data points in the train set: 5152, number of used features: 17
2025-02-26 12:05:31,890:INFO:[LightGBM] [Info] Start training from score 1965.258152
2025-02-26 12:05:32,070:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:05:32,070:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:05:32,081:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006840 seconds.
2025-02-26 12:05:32,082:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:05:32,082:INFO:[LightGBM] [Info] Total Bins 1771
2025-02-26 12:05:32,082:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-26 12:05:32,087:INFO:[LightGBM] [Info] Start training from score 1965.251163
2025-02-26 12:05:33,481:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:05:33,481:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:05:33,488:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003910 seconds.
2025-02-26 12:05:33,488:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:05:33,489:INFO:[LightGBM] [Info] Total Bins 1771
2025-02-26 12:05:33,489:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-26 12:05:33,493:INFO:[LightGBM] [Info] Start training from score 1965.251163
2025-02-26 12:05:33,513:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:05:33,513:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:05:33,515:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000717 seconds.
2025-02-26 12:05:33,516:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:05:33,516:INFO:[LightGBM] [Info] Total Bins 1771
2025-02-26 12:05:33,516:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-26 12:05:33,516:INFO:[LightGBM] [Info] Start training from score 1965.251163
2025-02-26 12:05:33,984:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:05:33,984:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:05:33,989:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002084 seconds.
2025-02-26 12:05:33,989:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:05:33,989:INFO:[LightGBM] [Info] Total Bins 1771
2025-02-26 12:05:33,989:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-26 12:05:33,991:INFO:[LightGBM] [Info] Start training from score 1965.251163
2025-02-26 12:05:37,204:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:05:37,204:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:05:37,292:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000724 seconds.
2025-02-26 12:05:37,292:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:05:37,292:INFO:[LightGBM] [Info] Total Bins 2215
2025-02-26 12:05:37,292:INFO:[LightGBM] [Info] Number of data points in the train set: 8520, number of used features: 17
2025-02-26 12:05:37,293:INFO:[LightGBM] [Info] Start training from score 1.609038
2025-02-26 12:05:38,891:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:05:38,892:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:05:38,897:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002528 seconds.
2025-02-26 12:05:38,898:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:05:38,898:INFO:[LightGBM] [Info] Total Bins 2215
2025-02-26 12:05:38,898:INFO:[LightGBM] [Info] Number of data points in the train set: 8520, number of used features: 17
2025-02-26 12:05:38,900:INFO:[LightGBM] [Info] Start training from score 1.609038
2025-02-26 12:05:41,413:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:05:41,995:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:05:42,508:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:05:42,569:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:05:42,569:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:05:42,572:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000491 seconds.
2025-02-26 12:05:42,572:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:05:42,572:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:05:42,572:INFO:[LightGBM] [Info] Total Bins 1920
2025-02-26 12:05:42,572:INFO:[LightGBM] [Info] Number of data points in the train set: 7681, number of used features: 17
2025-02-26 12:05:42,572:INFO:[LightGBM] [Info] Start training from score 14.834266
2025-02-26 12:05:42,621:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:05:42,954:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:05:43,216:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:05:43,217:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:05:43,221:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000783 seconds.
2025-02-26 12:05:43,221:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:05:43,221:INFO:[LightGBM] [Info] Total Bins 1920
2025-02-26 12:05:43,221:INFO:[LightGBM] [Info] Number of data points in the train set: 7681, number of used features: 17
2025-02-26 12:05:43,221:INFO:[LightGBM] [Info] Start training from score 14.834266
2025-02-26 12:05:43,267:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:05:43,420:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:05:43,573:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:05:44,426:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:05:44,427:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:05:44,430:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001004 seconds.
2025-02-26 12:05:44,430:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:05:44,430:INFO:[LightGBM] [Info] Total Bins 1771
2025-02-26 12:05:44,430:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-26 12:05:44,431:INFO:[LightGBM] [Info] Start training from score 1965.251163
2025-02-26 12:05:44,453:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:05:44,453:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:05:44,456:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000350 seconds.
2025-02-26 12:05:44,456:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:05:44,456:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:05:44,456:INFO:[LightGBM] [Info] Total Bins 1771
2025-02-26 12:05:44,456:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-26 12:05:44,456:INFO:[LightGBM] [Info] Start training from score 1965.251163
2025-02-26 12:05:45,302:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:05:45,302:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:05:45,346:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016241 seconds.
2025-02-26 12:05:45,346:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:05:45,346:INFO:[LightGBM] [Info] Total Bins 2215
2025-02-26 12:05:45,348:INFO:[LightGBM] [Info] Number of data points in the train set: 8520, number of used features: 17
2025-02-26 12:05:45,349:INFO:[LightGBM] [Info] Start training from score 1.609038
2025-02-26 12:05:45,452:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:05:45,453:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:05:45,458:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000869 seconds.
2025-02-26 12:05:45,458:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:05:45,458:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:05:45,458:INFO:[LightGBM] [Info] Total Bins 2215
2025-02-26 12:05:45,458:INFO:[LightGBM] [Info] Number of data points in the train set: 8520, number of used features: 17
2025-02-26 12:05:45,459:INFO:[LightGBM] [Info] Start training from score 1.609038
2025-02-26 12:05:46,360:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:05:46,360:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:05:46,363:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000321 seconds.
2025-02-26 12:05:46,363:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:05:46,363:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:05:46,363:INFO:[LightGBM] [Info] Total Bins 1921
2025-02-26 12:05:46,363:INFO:[LightGBM] [Info] Number of data points in the train set: 7681, number of used features: 17
2025-02-26 12:05:46,364:INFO:[LightGBM] [Info] Start training from score 14.834266
2025-02-26 12:05:46,583:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:05:46,584:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:05:46,588:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001133 seconds.
2025-02-26 12:05:46,588:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:05:46,588:INFO:[LightGBM] [Info] Total Bins 1920
2025-02-26 12:05:46,588:INFO:[LightGBM] [Info] Number of data points in the train set: 7681, number of used features: 17
2025-02-26 12:05:46,589:INFO:[LightGBM] [Info] Start training from score 14.834266
2025-02-26 12:05:47,542:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:05:47,542:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:05:47,545:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000907 seconds.
2025-02-26 12:05:47,545:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:05:47,545:INFO:[LightGBM] [Info] Total Bins 1771
2025-02-26 12:05:47,545:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-26 12:05:47,546:INFO:[LightGBM] [Info] Start training from score 1965.251163
2025-02-26 12:05:47,604:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:05:47,604:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:05:47,610:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000411 seconds.
2025-02-26 12:05:47,610:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:05:47,610:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:05:47,610:INFO:[LightGBM] [Info] Total Bins 1771
2025-02-26 12:05:47,610:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-26 12:05:47,611:INFO:[LightGBM] [Info] Start training from score 1965.251163
2025-02-26 12:05:48,404:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:05:48,404:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:05:48,409:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000558 seconds.
2025-02-26 12:05:48,410:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:05:48,410:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:05:48,410:INFO:[LightGBM] [Info] Total Bins 2215
2025-02-26 12:05:48,410:INFO:[LightGBM] [Info] Number of data points in the train set: 8520, number of used features: 17
2025-02-26 12:05:48,410:INFO:[LightGBM] [Info] Start training from score 1.609038
2025-02-26 12:05:49,373:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:05:49,373:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:05:49,379:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001415 seconds.
2025-02-26 12:05:49,379:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:05:49,379:INFO:[LightGBM] [Info] Total Bins 1920
2025-02-26 12:05:49,380:INFO:[LightGBM] [Info] Number of data points in the train set: 7681, number of used features: 17
2025-02-26 12:05:49,380:INFO:[LightGBM] [Info] Start training from score 14.834266
2025-02-26 12:05:50,048:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:05:50,048:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:05:50,052:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000660 seconds.
2025-02-26 12:05:50,052:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:05:50,052:INFO:[LightGBM] [Info] Total Bins 1771
2025-02-26 12:05:50,052:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-26 12:05:50,052:INFO:[LightGBM] [Info] Start training from score 1965.251163
2025-02-26 12:05:50,631:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:05:50,631:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:05:50,638:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000820 seconds.
2025-02-26 12:05:50,638:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:05:50,638:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:05:50,638:INFO:[LightGBM] [Info] Total Bins 2215
2025-02-26 12:05:50,638:INFO:[LightGBM] [Info] Number of data points in the train set: 8520, number of used features: 17
2025-02-26 12:05:50,638:INFO:[LightGBM] [Info] Start training from score 1.609038
2025-02-26 12:05:51,448:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:05:51,448:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:05:51,457:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000843 seconds.
2025-02-26 12:05:51,457:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:05:51,457:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:05:51,457:INFO:[LightGBM] [Info] Total Bins 1920
2025-02-26 12:05:51,457:INFO:[LightGBM] [Info] Number of data points in the train set: 7681, number of used features: 17
2025-02-26 12:05:51,458:INFO:[LightGBM] [Info] Start training from score 14.834266
2025-02-26 12:05:52,206:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:05:52,206:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:05:52,215:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000602 seconds.
2025-02-26 12:05:52,215:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:05:52,215:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:05:52,215:INFO:[LightGBM] [Info] Total Bins 1771
2025-02-26 12:05:52,215:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-26 12:05:52,215:INFO:[LightGBM] [Info] Start training from score 1965.251163
2025-02-26 12:06:18,499:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:06:18,501:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:06:18,519:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000767 seconds.
2025-02-26 12:06:18,519:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:06:18,519:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:06:18,519:INFO:[LightGBM] [Info] Total Bins 1845
2025-02-26 12:06:18,519:INFO:[LightGBM] [Info] Number of data points in the train set: 8523, number of used features: 17
2025-02-26 12:06:18,520:INFO:[LightGBM] [Info] Start training from score 1.615394
2025-02-26 12:06:19,429:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:06:19,429:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:06:19,433:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000454 seconds.
2025-02-26 12:06:19,433:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:06:19,433:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:06:19,433:INFO:[LightGBM] [Info] Total Bins 1772
2025-02-26 12:06:19,433:INFO:[LightGBM] [Info] Number of data points in the train set: 7714, number of used features: 17
2025-02-26 12:06:19,434:INFO:[LightGBM] [Info] Start training from score 14.845735
2025-02-26 12:06:20,306:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:06:20,307:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:06:20,311:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000303 seconds.
2025-02-26 12:06:20,311:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:06:20,311:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:06:20,311:INFO:[LightGBM] [Info] Total Bins 1766
2025-02-26 12:06:20,311:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-26 12:06:20,311:INFO:[LightGBM] [Info] Start training from score 1965.418992
2025-02-26 12:06:21,227:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:06:21,227:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:06:21,235:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000662 seconds.
2025-02-26 12:06:21,235:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:06:21,235:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:06:21,235:INFO:[LightGBM] [Info] Total Bins 2216
2025-02-26 12:06:21,235:INFO:[LightGBM] [Info] Number of data points in the train set: 8523, number of used features: 17
2025-02-26 12:06:21,236:INFO:[LightGBM] [Info] Start training from score 1.615394
2025-02-26 12:06:22,097:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:06:22,097:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:06:22,103:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000889 seconds.
2025-02-26 12:06:22,103:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:06:22,104:INFO:[LightGBM] [Info] Total Bins 1926
2025-02-26 12:06:22,104:INFO:[LightGBM] [Info] Number of data points in the train set: 7714, number of used features: 17
2025-02-26 12:06:22,104:INFO:[LightGBM] [Info] Start training from score 14.845735
2025-02-26 12:06:22,841:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:06:22,841:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:06:22,850:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000504 seconds.
2025-02-26 12:06:22,850:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:06:22,850:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:06:22,850:INFO:[LightGBM] [Info] Total Bins 1767
2025-02-26 12:06:22,850:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-26 12:06:22,851:INFO:[LightGBM] [Info] Start training from score 1965.418992
2025-02-26 12:06:23,660:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:06:23,661:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:06:23,665:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000349 seconds.
2025-02-26 12:06:23,666:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:06:23,666:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:06:23,666:INFO:[LightGBM] [Info] Total Bins 2215
2025-02-26 12:06:23,666:INFO:[LightGBM] [Info] Number of data points in the train set: 8523, number of used features: 17
2025-02-26 12:06:23,668:INFO:[LightGBM] [Info] Start training from score 1.615394
2025-02-26 12:06:24,531:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:06:24,531:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:06:24,536:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000502 seconds.
2025-02-26 12:06:24,536:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:06:24,536:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:06:24,536:INFO:[LightGBM] [Info] Total Bins 1925
2025-02-26 12:06:24,536:INFO:[LightGBM] [Info] Number of data points in the train set: 7714, number of used features: 17
2025-02-26 12:06:24,536:INFO:[LightGBM] [Info] Start training from score 14.845735
2025-02-26 12:06:25,341:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:06:25,342:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:06:25,344:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000278 seconds.
2025-02-26 12:06:25,345:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:06:25,345:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:06:25,345:INFO:[LightGBM] [Info] Total Bins 1766
2025-02-26 12:06:25,345:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-26 12:06:25,346:INFO:[LightGBM] [Info] Start training from score 1965.418992
2025-02-26 12:06:26,081:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:06:26,082:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:06:26,086:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001256 seconds.
2025-02-26 12:06:26,086:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:06:26,086:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:06:26,086:INFO:[LightGBM] [Info] Total Bins 2215
2025-02-26 12:06:26,086:INFO:[LightGBM] [Info] Number of data points in the train set: 8523, number of used features: 17
2025-02-26 12:06:26,086:INFO:[LightGBM] [Info] Start training from score 1.615394
2025-02-26 12:06:27,434:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:06:27,435:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:06:27,438:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001291 seconds.
2025-02-26 12:06:27,439:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:06:27,439:INFO:[LightGBM] [Info] Total Bins 1926
2025-02-26 12:06:27,439:INFO:[LightGBM] [Info] Number of data points in the train set: 7714, number of used features: 17
2025-02-26 12:06:27,439:INFO:[LightGBM] [Info] Start training from score 14.845735
2025-02-26 12:06:27,577:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:06:27,578:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:06:27,711:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003203 seconds.
2025-02-26 12:06:27,711:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:06:27,711:INFO:[LightGBM] [Info] Total Bins 1845
2025-02-26 12:06:27,713:INFO:[LightGBM] [Info] Number of data points in the train set: 8523, number of used features: 17
2025-02-26 12:06:27,714:INFO:[LightGBM] [Info] Start training from score 1.615394
2025-02-26 12:06:28,860:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:06:28,860:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:06:28,887:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001074 seconds.
2025-02-26 12:06:28,887:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:06:28,887:INFO:[LightGBM] [Info] Total Bins 1845
2025-02-26 12:06:28,887:INFO:[LightGBM] [Info] Number of data points in the train set: 8523, number of used features: 17
2025-02-26 12:06:28,888:INFO:[LightGBM] [Info] Start training from score 1.615394
2025-02-26 12:06:29,050:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:06:29,050:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:06:29,053:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000255 seconds.
2025-02-26 12:06:29,053:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:06:29,053:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:06:29,054:INFO:[LightGBM] [Info] Total Bins 1766
2025-02-26 12:06:29,054:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-26 12:06:29,055:INFO:[LightGBM] [Info] Start training from score 1965.418992
2025-02-26 12:06:29,977:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:06:29,977:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:06:29,983:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000616 seconds.
2025-02-26 12:06:29,983:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:06:29,983:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:06:29,983:INFO:[LightGBM] [Info] Total Bins 1772
2025-02-26 12:06:29,983:INFO:[LightGBM] [Info] Number of data points in the train set: 7714, number of used features: 17
2025-02-26 12:06:29,984:INFO:[LightGBM] [Info] Start training from score 14.845735
2025-02-26 12:06:30,271:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:06:30,271:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:06:30,277:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003529 seconds.
2025-02-26 12:06:30,278:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:06:30,278:INFO:[LightGBM] [Info] Total Bins 1845
2025-02-26 12:06:30,279:INFO:[LightGBM] [Info] Number of data points in the train set: 8523, number of used features: 17
2025-02-26 12:06:30,285:INFO:[LightGBM] [Info] Start training from score 1.615394
2025-02-26 12:06:30,691:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:06:30,691:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:06:30,693:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000273 seconds.
2025-02-26 12:06:30,693:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:06:30,693:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:06:30,693:INFO:[LightGBM] [Info] Total Bins 1845
2025-02-26 12:06:30,694:INFO:[LightGBM] [Info] Number of data points in the train set: 8523, number of used features: 17
2025-02-26 12:06:30,694:INFO:[LightGBM] [Info] Start training from score 1.615394
2025-02-26 12:06:32,081:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:06:32,082:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:06:32,084:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000839 seconds.
2025-02-26 12:06:32,085:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:06:32,085:INFO:[LightGBM] [Info] Total Bins 1772
2025-02-26 12:06:32,085:INFO:[LightGBM] [Info] Number of data points in the train set: 7714, number of used features: 17
2025-02-26 12:06:32,085:INFO:[LightGBM] [Info] Start training from score 14.845735
2025-02-26 12:06:32,313:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:06:32,314:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:06:32,320:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001549 seconds.
2025-02-26 12:06:32,320:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:06:32,320:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:06:32,320:INFO:[LightGBM] [Info] Total Bins 2214
2025-02-26 12:06:32,320:INFO:[LightGBM] [Info] Number of data points in the train set: 8523, number of used features: 17
2025-02-26 12:06:32,321:INFO:[LightGBM] [Info] Start training from score 1.615394
2025-02-26 12:06:33,190:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:06:33,191:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:06:33,193:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000357 seconds.
2025-02-26 12:06:33,193:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:06:33,193:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:06:33,193:INFO:[LightGBM] [Info] Total Bins 1772
2025-02-26 12:06:33,193:INFO:[LightGBM] [Info] Number of data points in the train set: 7714, number of used features: 17
2025-02-26 12:06:33,193:INFO:[LightGBM] [Info] Start training from score 14.845735
2025-02-26 12:06:33,497:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:06:33,497:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:06:33,502:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002047 seconds.
2025-02-26 12:06:33,502:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:06:33,502:INFO:[LightGBM] [Info] Total Bins 1766
2025-02-26 12:06:33,502:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-26 12:06:33,504:INFO:[LightGBM] [Info] Start training from score 1965.418992
2025-02-26 12:06:34,987:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:06:34,987:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:06:35,000:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003238 seconds.
2025-02-26 12:06:35,001:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:06:35,001:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:06:35,001:INFO:[LightGBM] [Info] Total Bins 1772
2025-02-26 12:06:35,002:INFO:[LightGBM] [Info] Number of data points in the train set: 7714, number of used features: 17
2025-02-26 12:06:35,003:INFO:[LightGBM] [Info] Start training from score 14.845735
2025-02-26 12:06:35,956:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:06:35,956:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:06:35,959:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001023 seconds.
2025-02-26 12:06:35,959:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:06:35,959:INFO:[LightGBM] [Info] Total Bins 1766
2025-02-26 12:06:35,960:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-26 12:06:35,961:INFO:[LightGBM] [Info] Start training from score 1965.418992
2025-02-26 12:06:36,368:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:06:36,368:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:06:36,596:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.120674 seconds.
2025-02-26 12:06:36,596:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:06:36,596:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:06:36,596:INFO:[LightGBM] [Info] Total Bins 1925
2025-02-26 12:06:36,596:INFO:[LightGBM] [Info] Number of data points in the train set: 7714, number of used features: 17
2025-02-26 12:06:36,597:INFO:[LightGBM] [Info] Start training from score 14.845735
2025-02-26 12:06:37,091:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:06:37,091:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:06:37,212:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015663 seconds.
2025-02-26 12:06:37,212:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:06:37,212:INFO:[LightGBM] [Info] Total Bins 2216
2025-02-26 12:06:37,212:INFO:[LightGBM] [Info] Number of data points in the train set: 8523, number of used features: 17
2025-02-26 12:06:37,213:INFO:[LightGBM] [Info] Start training from score 1.615394
2025-02-26 12:06:37,623:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:06:37,624:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:06:37,626:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000539 seconds.
2025-02-26 12:06:37,626:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:06:37,626:INFO:[LightGBM] [Info] Total Bins 1766
2025-02-26 12:06:37,626:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-26 12:06:37,626:INFO:[LightGBM] [Info] Start training from score 1965.418992
2025-02-26 12:06:38,652:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:06:38,652:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:06:38,655:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000755 seconds.
2025-02-26 12:06:38,655:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:06:38,655:INFO:[LightGBM] [Info] Total Bins 1766
2025-02-26 12:06:38,655:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-26 12:06:38,656:INFO:[LightGBM] [Info] Start training from score 1965.418992
2025-02-26 12:06:39,266:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:06:39,266:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:06:39,337:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000613 seconds.
2025-02-26 12:06:39,337:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:06:39,337:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:06:39,337:INFO:[LightGBM] [Info] Total Bins 2216
2025-02-26 12:06:39,337:INFO:[LightGBM] [Info] Number of data points in the train set: 8523, number of used features: 17
2025-02-26 12:06:39,338:INFO:[LightGBM] [Info] Start training from score 1.615394
2025-02-26 12:06:39,819:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:06:39,819:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:06:39,821:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000620 seconds.
2025-02-26 12:06:39,822:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:06:39,822:INFO:[LightGBM] [Info] Total Bins 1766
2025-02-26 12:06:39,822:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-26 12:06:39,822:INFO:[LightGBM] [Info] Start training from score 1965.418992
2025-02-26 12:06:39,895:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:06:39,896:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:06:39,904:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000988 seconds.
2025-02-26 12:06:39,905:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:06:39,905:INFO:[LightGBM] [Info] Total Bins 1926
2025-02-26 12:06:39,905:INFO:[LightGBM] [Info] Number of data points in the train set: 7714, number of used features: 17
2025-02-26 12:06:39,905:INFO:[LightGBM] [Info] Start training from score 14.845735
2025-02-26 12:06:40,511:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:06:40,511:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:06:40,516:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001391 seconds.
2025-02-26 12:06:40,517:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:06:40,517:INFO:[LightGBM] [Info] Total Bins 2216
2025-02-26 12:06:40,517:INFO:[LightGBM] [Info] Number of data points in the train set: 8523, number of used features: 17
2025-02-26 12:06:40,517:INFO:[LightGBM] [Info] Start training from score 1.615394
2025-02-26 12:06:41,680:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:06:41,680:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:06:41,685:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001059 seconds.
2025-02-26 12:06:41,685:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:06:41,685:INFO:[LightGBM] [Info] Total Bins 2216
2025-02-26 12:06:41,685:INFO:[LightGBM] [Info] Number of data points in the train set: 8523, number of used features: 17
2025-02-26 12:06:41,686:INFO:[LightGBM] [Info] Start training from score 1.615394
2025-02-26 12:06:42,527:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:06:42,527:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:06:42,530:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000883 seconds.
2025-02-26 12:06:42,530:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:06:42,530:INFO:[LightGBM] [Info] Total Bins 1926
2025-02-26 12:06:42,530:INFO:[LightGBM] [Info] Number of data points in the train set: 7714, number of used features: 17
2025-02-26 12:06:42,531:INFO:[LightGBM] [Info] Start training from score 14.845735
2025-02-26 12:06:42,704:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:06:42,704:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:06:42,744:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021685 seconds.
2025-02-26 12:06:42,744:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:06:42,744:INFO:[LightGBM] [Info] Total Bins 1767
2025-02-26 12:06:42,745:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-26 12:06:42,786:INFO:[LightGBM] [Info] Start training from score 1965.418992
2025-02-26 12:06:43,234:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:06:43,235:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:06:43,299:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.026745 seconds.
2025-02-26 12:06:43,300:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:06:43,300:INFO:[LightGBM] [Info] Total Bins 1926
2025-02-26 12:06:43,301:INFO:[LightGBM] [Info] Number of data points in the train set: 7714, number of used features: 17
2025-02-26 12:06:43,305:INFO:[LightGBM] [Info] Start training from score 14.845735
2025-02-26 12:06:44,300:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:06:44,304:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:06:44,305:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000281 seconds.
2025-02-26 12:06:44,305:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:06:44,305:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:06:44,305:INFO:[LightGBM] [Info] Total Bins 1926
2025-02-26 12:06:44,305:INFO:[LightGBM] [Info] Number of data points in the train set: 7714, number of used features: 17
2025-02-26 12:06:44,306:INFO:[LightGBM] [Info] Start training from score 14.845735
2025-02-26 12:06:45,053:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:06:45,053:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:06:45,057:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001559 seconds.
2025-02-26 12:06:45,057:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:06:45,057:INFO:[LightGBM] [Info] Total Bins 1767
2025-02-26 12:06:45,058:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-26 12:06:45,059:INFO:[LightGBM] [Info] Start training from score 1965.418992
2025-02-26 12:06:45,160:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:06:45,160:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:06:45,166:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000568 seconds.
2025-02-26 12:06:45,166:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:06:45,166:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:06:45,166:INFO:[LightGBM] [Info] Total Bins 2215
2025-02-26 12:06:45,166:INFO:[LightGBM] [Info] Number of data points in the train set: 8523, number of used features: 17
2025-02-26 12:06:45,167:INFO:[LightGBM] [Info] Start training from score 1.615394
2025-02-26 12:06:45,651:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:06:45,651:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:06:45,653:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000257 seconds.
2025-02-26 12:06:45,653:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:06:45,653:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:06:45,653:INFO:[LightGBM] [Info] Total Bins 1767
2025-02-26 12:06:45,653:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-26 12:06:45,653:INFO:[LightGBM] [Info] Start training from score 1965.418992
2025-02-26 12:06:47,111:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:06:47,111:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:06:47,115:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002082 seconds.
2025-02-26 12:06:47,116:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:06:47,116:INFO:[LightGBM] [Info] Total Bins 1767
2025-02-26 12:06:47,116:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-26 12:06:47,117:INFO:[LightGBM] [Info] Start training from score 1965.418992
2025-02-26 12:06:47,464:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:06:47,464:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:06:47,487:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001580 seconds.
2025-02-26 12:06:47,488:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:06:47,488:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:06:47,488:INFO:[LightGBM] [Info] Total Bins 2215
2025-02-26 12:06:47,488:INFO:[LightGBM] [Info] Number of data points in the train set: 8523, number of used features: 17
2025-02-26 12:06:47,489:INFO:[LightGBM] [Info] Start training from score 1.615394
2025-02-26 12:06:48,001:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:06:48,001:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:06:48,005:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000589 seconds.
2025-02-26 12:06:48,006:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:06:48,006:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:06:48,006:INFO:[LightGBM] [Info] Total Bins 1925
2025-02-26 12:06:48,006:INFO:[LightGBM] [Info] Number of data points in the train set: 7714, number of used features: 17
2025-02-26 12:06:48,007:INFO:[LightGBM] [Info] Start training from score 14.845735
2025-02-26 12:06:48,383:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:06:48,383:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:06:48,403:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001447 seconds.
2025-02-26 12:06:48,404:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:06:48,404:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:06:48,405:INFO:[LightGBM] [Info] Total Bins 2215
2025-02-26 12:06:48,406:INFO:[LightGBM] [Info] Number of data points in the train set: 8523, number of used features: 17
2025-02-26 12:06:48,407:INFO:[LightGBM] [Info] Start training from score 1.615394
2025-02-26 12:06:49,375:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:06:49,375:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:06:49,380:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001161 seconds.
2025-02-26 12:06:49,380:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:06:49,380:INFO:[LightGBM] [Info] Total Bins 2215
2025-02-26 12:06:49,380:INFO:[LightGBM] [Info] Number of data points in the train set: 8523, number of used features: 17
2025-02-26 12:06:49,381:INFO:[LightGBM] [Info] Start training from score 1.615394
2025-02-26 12:06:50,247:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:06:50,247:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:06:50,250:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000285 seconds.
2025-02-26 12:06:50,250:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:06:50,250:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:06:50,250:INFO:[LightGBM] [Info] Total Bins 1925
2025-02-26 12:06:50,250:INFO:[LightGBM] [Info] Number of data points in the train set: 7714, number of used features: 17
2025-02-26 12:06:50,250:INFO:[LightGBM] [Info] Start training from score 14.845735
2025-02-26 12:06:50,771:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:06:50,771:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:06:50,773:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000245 seconds.
2025-02-26 12:06:50,773:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:06:50,773:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:06:50,773:INFO:[LightGBM] [Info] Total Bins 1766
2025-02-26 12:06:50,773:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-26 12:06:50,774:INFO:[LightGBM] [Info] Start training from score 1965.418992
2025-02-26 12:06:51,367:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:06:51,367:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:06:51,371:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001003 seconds.
2025-02-26 12:06:51,371:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:06:51,371:INFO:[LightGBM] [Info] Total Bins 1925
2025-02-26 12:06:51,371:INFO:[LightGBM] [Info] Number of data points in the train set: 7714, number of used features: 17
2025-02-26 12:06:51,372:INFO:[LightGBM] [Info] Start training from score 14.845735
2025-02-26 12:06:51,787:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:06:51,787:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:06:51,792:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000990 seconds.
2025-02-26 12:06:51,792:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:06:51,792:INFO:[LightGBM] [Info] Total Bins 1925
2025-02-26 12:06:51,792:INFO:[LightGBM] [Info] Number of data points in the train set: 7714, number of used features: 17
2025-02-26 12:06:51,792:INFO:[LightGBM] [Info] Start training from score 14.845735
2025-02-26 12:06:53,322:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:06:53,323:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:06:53,332:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002015 seconds.
2025-02-26 12:06:53,332:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:06:53,332:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:06:53,332:INFO:[LightGBM] [Info] Total Bins 1766
2025-02-26 12:06:53,332:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-26 12:06:53,333:INFO:[LightGBM] [Info] Start training from score 1965.418992
2025-02-26 12:06:53,495:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:06:53,495:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:06:53,504:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002203 seconds.
2025-02-26 12:06:53,504:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:06:53,505:INFO:[LightGBM] [Info] Total Bins 2215
2025-02-26 12:06:53,505:INFO:[LightGBM] [Info] Number of data points in the train set: 8523, number of used features: 17
2025-02-26 12:06:53,507:INFO:[LightGBM] [Info] Start training from score 1.615394
2025-02-26 12:06:53,795:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:06:53,795:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:06:53,798:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000449 seconds.
2025-02-26 12:06:53,798:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:06:53,798:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:06:53,798:INFO:[LightGBM] [Info] Total Bins 1766
2025-02-26 12:06:53,798:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-26 12:06:53,798:INFO:[LightGBM] [Info] Start training from score 1965.418992
2025-02-26 12:06:54,483:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:06:54,483:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:06:54,486:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001084 seconds.
2025-02-26 12:06:54,486:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:06:54,486:INFO:[LightGBM] [Info] Total Bins 1766
2025-02-26 12:06:54,486:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-26 12:06:54,487:INFO:[LightGBM] [Info] Start training from score 1965.418992
2025-02-26 12:06:56,501:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:06:56,502:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:06:56,506:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001080 seconds.
2025-02-26 12:06:56,506:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:06:56,506:INFO:[LightGBM] [Info] Total Bins 1844
2025-02-26 12:06:56,507:INFO:[LightGBM] [Info] Number of data points in the train set: 8520, number of used features: 17
2025-02-26 12:06:56,507:INFO:[LightGBM] [Info] Start training from score 1.609038
2025-02-26 12:06:56,628:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:06:56,628:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:06:56,631:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000486 seconds.
2025-02-26 12:06:56,631:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:06:56,631:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:06:56,631:INFO:[LightGBM] [Info] Total Bins 1926
2025-02-26 12:06:56,632:INFO:[LightGBM] [Info] Number of data points in the train set: 7714, number of used features: 17
2025-02-26 12:06:56,632:INFO:[LightGBM] [Info] Start training from score 14.845735
2025-02-26 12:06:56,987:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:06:56,988:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:06:56,996:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002795 seconds.
2025-02-26 12:06:56,996:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:06:56,996:INFO:[LightGBM] [Info] Total Bins 2215
2025-02-26 12:06:56,997:INFO:[LightGBM] [Info] Number of data points in the train set: 8523, number of used features: 17
2025-02-26 12:06:57,000:INFO:[LightGBM] [Info] Start training from score 1.615394
2025-02-26 12:06:58,180:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:06:58,180:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:06:58,340:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001720 seconds.
2025-02-26 12:06:58,340:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:06:58,340:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:06:58,340:INFO:[LightGBM] [Info] Total Bins 2215
2025-02-26 12:06:58,340:INFO:[LightGBM] [Info] Number of data points in the train set: 8523, number of used features: 17
2025-02-26 12:06:58,343:INFO:[LightGBM] [Info] Start training from score 1.615394
2025-02-26 12:06:58,440:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:06:58,440:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:06:58,443:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000942 seconds.
2025-02-26 12:06:58,443:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:06:58,443:INFO:[LightGBM] [Info] Total Bins 2215
2025-02-26 12:06:58,443:INFO:[LightGBM] [Info] Number of data points in the train set: 8523, number of used features: 17
2025-02-26 12:06:58,444:INFO:[LightGBM] [Info] Start training from score 1.615394
2025-02-26 12:07:00,115:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:00,115:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:00,119:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000356 seconds.
2025-02-26 12:07:00,119:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:07:00,119:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:07:00,119:INFO:[LightGBM] [Info] Total Bins 1767
2025-02-26 12:07:00,119:INFO:[LightGBM] [Info] Number of data points in the train set: 7681, number of used features: 17
2025-02-26 12:07:00,120:INFO:[LightGBM] [Info] Start training from score 14.834266
2025-02-26 12:07:00,659:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:00,659:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:00,665:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001309 seconds.
2025-02-26 12:07:00,665:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:00,665:INFO:[LightGBM] [Info] Total Bins 1926
2025-02-26 12:07:00,665:INFO:[LightGBM] [Info] Number of data points in the train set: 7714, number of used features: 17
2025-02-26 12:07:00,666:INFO:[LightGBM] [Info] Start training from score 14.845735
2025-02-26 12:07:01,023:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:01,023:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:01,025:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000605 seconds.
2025-02-26 12:07:01,025:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:01,025:INFO:[LightGBM] [Info] Total Bins 1766
2025-02-26 12:07:01,025:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-26 12:07:01,025:INFO:[LightGBM] [Info] Start training from score 1965.418992
2025-02-26 12:07:01,772:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:01,772:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:01,801:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002491 seconds.
2025-02-26 12:07:01,801:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:01,801:INFO:[LightGBM] [Info] Total Bins 1926
2025-02-26 12:07:01,802:INFO:[LightGBM] [Info] Number of data points in the train set: 7714, number of used features: 17
2025-02-26 12:07:01,803:INFO:[LightGBM] [Info] Start training from score 14.845735
2025-02-26 12:07:02,145:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:02,145:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:02,149:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000806 seconds.
2025-02-26 12:07:02,149:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:02,149:INFO:[LightGBM] [Info] Total Bins 1926
2025-02-26 12:07:02,149:INFO:[LightGBM] [Info] Number of data points in the train set: 7714, number of used features: 17
2025-02-26 12:07:02,150:INFO:[LightGBM] [Info] Start training from score 14.845735
2025-02-26 12:07:03,657:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:03,658:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:03,659:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000438 seconds.
2025-02-26 12:07:03,659:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:03,659:INFO:[LightGBM] [Info] Total Bins 1766
2025-02-26 12:07:03,659:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-26 12:07:03,660:INFO:[LightGBM] [Info] Start training from score 1965.418992
2025-02-26 12:07:03,908:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:03,908:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:03,911:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000572 seconds.
2025-02-26 12:07:03,911:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:03,911:INFO:[LightGBM] [Info] Total Bins 1771
2025-02-26 12:07:03,911:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-26 12:07:03,913:INFO:[LightGBM] [Info] Start training from score 1965.251163
2025-02-26 12:07:04,378:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:04,378:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:04,382:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000347 seconds.
2025-02-26 12:07:04,382:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:07:04,382:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:07:04,382:INFO:[LightGBM] [Info] Total Bins 2214
2025-02-26 12:07:04,382:INFO:[LightGBM] [Info] Number of data points in the train set: 8523, number of used features: 17
2025-02-26 12:07:04,383:INFO:[LightGBM] [Info] Start training from score 1.615394
2025-02-26 12:07:04,536:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:04,536:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:04,539:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000689 seconds.
2025-02-26 12:07:04,539:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:04,539:INFO:[LightGBM] [Info] Total Bins 1766
2025-02-26 12:07:04,540:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-26 12:07:04,540:INFO:[LightGBM] [Info] Start training from score 1965.418992
2025-02-26 12:07:05,289:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:05,289:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:05,292:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000655 seconds.
2025-02-26 12:07:05,292:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:05,292:INFO:[LightGBM] [Info] Total Bins 1766
2025-02-26 12:07:05,292:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-26 12:07:05,293:INFO:[LightGBM] [Info] Start training from score 1965.418992
2025-02-26 12:07:05,337:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:05,338:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:05,347:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004381 seconds.
2025-02-26 12:07:05,347:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:05,347:INFO:[LightGBM] [Info] Total Bins 1845
2025-02-26 12:07:05,347:INFO:[LightGBM] [Info] Number of data points in the train set: 8523, number of used features: 17
2025-02-26 12:07:05,349:INFO:[LightGBM] [Info] Start training from score 1.615394
2025-02-26 12:07:06,735:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:06,739:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:06,759:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009933 seconds.
2025-02-26 12:07:06,759:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:07:06,759:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:07:06,759:INFO:[LightGBM] [Info] Total Bins 2214
2025-02-26 12:07:06,759:INFO:[LightGBM] [Info] Number of data points in the train set: 8523, number of used features: 17
2025-02-26 12:07:06,760:INFO:[LightGBM] [Info] Start training from score 1.615394
2025-02-26 12:07:07,091:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:07,091:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:07,095:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001183 seconds.
2025-02-26 12:07:07,095:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:07,095:INFO:[LightGBM] [Info] Total Bins 2215
2025-02-26 12:07:07,095:INFO:[LightGBM] [Info] Number of data points in the train set: 8520, number of used features: 17
2025-02-26 12:07:07,096:INFO:[LightGBM] [Info] Start training from score 1.609038
2025-02-26 12:07:07,577:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:07,577:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:07,580:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000784 seconds.
2025-02-26 12:07:07,580:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:07,580:INFO:[LightGBM] [Info] Total Bins 2214
2025-02-26 12:07:07,580:INFO:[LightGBM] [Info] Number of data points in the train set: 8523, number of used features: 17
2025-02-26 12:07:07,581:INFO:[LightGBM] [Info] Start training from score 1.615394
2025-02-26 12:07:08,423:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:08,423:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:08,427:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000360 seconds.
2025-02-26 12:07:08,427:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:07:08,427:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:07:08,427:INFO:[LightGBM] [Info] Total Bins 2214
2025-02-26 12:07:08,427:INFO:[LightGBM] [Info] Number of data points in the train set: 8523, number of used features: 17
2025-02-26 12:07:08,427:INFO:[LightGBM] [Info] Start training from score 1.615394
2025-02-26 12:07:08,531:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:08,532:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:08,535:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001443 seconds.
2025-02-26 12:07:08,535:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:08,535:INFO:[LightGBM] [Info] Total Bins 1925
2025-02-26 12:07:08,536:INFO:[LightGBM] [Info] Number of data points in the train set: 7714, number of used features: 17
2025-02-26 12:07:08,537:INFO:[LightGBM] [Info] Start training from score 14.845735
2025-02-26 12:07:08,892:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:08,893:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:08,910:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006553 seconds.
2025-02-26 12:07:08,910:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:08,910:INFO:[LightGBM] [Info] Total Bins 1772
2025-02-26 12:07:08,911:INFO:[LightGBM] [Info] Number of data points in the train set: 7714, number of used features: 17
2025-02-26 12:07:08,917:INFO:[LightGBM] [Info] Start training from score 14.845735
2025-02-26 12:07:10,284:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:10,284:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:10,287:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000797 seconds.
2025-02-26 12:07:10,287:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:10,287:INFO:[LightGBM] [Info] Total Bins 1920
2025-02-26 12:07:10,287:INFO:[LightGBM] [Info] Number of data points in the train set: 7681, number of used features: 17
2025-02-26 12:07:10,288:INFO:[LightGBM] [Info] Start training from score 14.834266
2025-02-26 12:07:10,499:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:07:10,536:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:10,536:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:10,540:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001055 seconds.
2025-02-26 12:07:10,540:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:10,540:INFO:[LightGBM] [Info] Total Bins 1925
2025-02-26 12:07:10,540:INFO:[LightGBM] [Info] Number of data points in the train set: 7714, number of used features: 17
2025-02-26 12:07:10,540:INFO:[LightGBM] [Info] Start training from score 14.845735
2025-02-26 12:07:11,128:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:11,128:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:11,132:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001831 seconds.
2025-02-26 12:07:11,133:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:11,133:INFO:[LightGBM] [Info] Total Bins 1925
2025-02-26 12:07:11,134:INFO:[LightGBM] [Info] Number of data points in the train set: 7714, number of used features: 17
2025-02-26 12:07:11,137:INFO:[LightGBM] [Info] Start training from score 14.845735
2025-02-26 12:07:11,490:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:11,490:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:11,493:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000565 seconds.
2025-02-26 12:07:11,493:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:07:11,493:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:07:11,493:INFO:[LightGBM] [Info] Total Bins 1766
2025-02-26 12:07:11,493:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-26 12:07:11,494:INFO:[LightGBM] [Info] Start training from score 1965.418992
2025-02-26 12:07:11,770:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:07:11,942:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:11,943:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:11,947:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001205 seconds.
2025-02-26 12:07:11,948:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:11,948:INFO:[LightGBM] [Info] Total Bins 1766
2025-02-26 12:07:11,948:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-26 12:07:11,948:INFO:[LightGBM] [Info] Start training from score 1965.418992
2025-02-26 12:07:12,495:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:12,495:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:12,498:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000772 seconds.
2025-02-26 12:07:12,498:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:12,498:INFO:[LightGBM] [Info] Total Bins 1925
2025-02-26 12:07:12,498:INFO:[LightGBM] [Info] Number of data points in the train set: 7714, number of used features: 17
2025-02-26 12:07:12,499:INFO:[LightGBM] [Info] Start training from score 14.845735
2025-02-26 12:07:13,025:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:13,026:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:13,028:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000663 seconds.
2025-02-26 12:07:13,028:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:07:13,028:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:07:13,028:INFO:[LightGBM] [Info] Total Bins 1771
2025-02-26 12:07:13,028:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-26 12:07:13,029:INFO:[LightGBM] [Info] Start training from score 1965.251163
2025-02-26 12:07:13,044:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:13,045:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:13,054:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003933 seconds.
2025-02-26 12:07:13,054:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:13,055:INFO:[LightGBM] [Info] Total Bins 1845
2025-02-26 12:07:13,055:INFO:[LightGBM] [Info] Number of data points in the train set: 8523, number of used features: 17
2025-02-26 12:07:13,056:INFO:[LightGBM] [Info] Start training from score 1.615394
2025-02-26 12:07:13,195:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:13,196:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:13,198:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000879 seconds.
2025-02-26 12:07:13,198:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:13,198:INFO:[LightGBM] [Info] Total Bins 1766
2025-02-26 12:07:13,198:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-26 12:07:13,199:INFO:[LightGBM] [Info] Start training from score 1965.418992
2025-02-26 12:07:14,152:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:14,152:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:14,154:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000567 seconds.
2025-02-26 12:07:14,154:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:14,154:INFO:[LightGBM] [Info] Total Bins 1766
2025-02-26 12:07:14,155:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-26 12:07:14,155:INFO:[LightGBM] [Info] Start training from score 1965.418992
2025-02-26 12:07:15,006:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:15,006:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:15,009:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000796 seconds.
2025-02-26 12:07:15,009:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:15,009:INFO:[LightGBM] [Info] Total Bins 2216
2025-02-26 12:07:15,009:INFO:[LightGBM] [Info] Number of data points in the train set: 8523, number of used features: 17
2025-02-26 12:07:15,010:INFO:[LightGBM] [Info] Start training from score 1.615394
2025-02-26 12:07:15,304:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:15,304:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:15,306:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000485 seconds.
2025-02-26 12:07:15,306:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:15,306:INFO:[LightGBM] [Info] Total Bins 1766
2025-02-26 12:07:15,306:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-26 12:07:15,306:INFO:[LightGBM] [Info] Start training from score 1965.418992
2025-02-26 12:07:15,925:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:15,925:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:15,928:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000571 seconds.
2025-02-26 12:07:15,928:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:07:15,928:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:07:15,928:INFO:[LightGBM] [Info] Total Bins 1772
2025-02-26 12:07:15,928:INFO:[LightGBM] [Info] Number of data points in the train set: 7714, number of used features: 17
2025-02-26 12:07:15,928:INFO:[LightGBM] [Info] Start training from score 14.845735
2025-02-26 12:07:16,330:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:16,330:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:16,334:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001218 seconds.
2025-02-26 12:07:16,334:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:16,334:INFO:[LightGBM] [Info] Total Bins 2215
2025-02-26 12:07:16,334:INFO:[LightGBM] [Info] Number of data points in the train set: 8520, number of used features: 17
2025-02-26 12:07:16,335:INFO:[LightGBM] [Info] Start training from score 1.609038
2025-02-26 12:07:17,252:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:17,252:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:17,255:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000496 seconds.
2025-02-26 12:07:17,255:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:07:17,255:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:07:17,255:INFO:[LightGBM] [Info] Total Bins 1926
2025-02-26 12:07:17,255:INFO:[LightGBM] [Info] Number of data points in the train set: 7714, number of used features: 17
2025-02-26 12:07:17,256:INFO:[LightGBM] [Info] Start training from score 14.845735
2025-02-26 12:07:17,850:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:17,850:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:17,853:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000956 seconds.
2025-02-26 12:07:17,853:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:17,853:INFO:[LightGBM] [Info] Total Bins 1766
2025-02-26 12:07:17,853:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-26 12:07:17,854:INFO:[LightGBM] [Info] Start training from score 1965.418992
2025-02-26 12:07:17,952:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:17,954:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:17,958:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001564 seconds.
2025-02-26 12:07:17,959:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:17,959:INFO:[LightGBM] [Info] Total Bins 1921
2025-02-26 12:07:17,959:INFO:[LightGBM] [Info] Number of data points in the train set: 7681, number of used features: 17
2025-02-26 12:07:17,961:INFO:[LightGBM] [Info] Start training from score 14.834266
2025-02-26 12:07:18,788:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:18,789:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:18,792:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000452 seconds.
2025-02-26 12:07:18,792:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:07:18,792:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:07:18,792:INFO:[LightGBM] [Info] Total Bins 1767
2025-02-26 12:07:18,792:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-26 12:07:18,793:INFO:[LightGBM] [Info] Start training from score 1965.418992
2025-02-26 12:07:19,196:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:19,196:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:19,201:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000419 seconds.
2025-02-26 12:07:19,201:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:07:19,202:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:07:19,202:INFO:[LightGBM] [Info] Total Bins 1771
2025-02-26 12:07:19,202:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-26 12:07:19,202:INFO:[LightGBM] [Info] Start training from score 1965.251163
2025-02-26 12:07:19,249:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:19,249:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:19,254:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001456 seconds.
2025-02-26 12:07:19,254:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:19,254:INFO:[LightGBM] [Info] Total Bins 2216
2025-02-26 12:07:19,254:INFO:[LightGBM] [Info] Number of data points in the train set: 8523, number of used features: 17
2025-02-26 12:07:19,255:INFO:[LightGBM] [Info] Start training from score 1.615394
2025-02-26 12:07:20,290:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:20,290:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:20,294:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000665 seconds.
2025-02-26 12:07:20,294:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:07:20,294:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:07:20,294:INFO:[LightGBM] [Info] Total Bins 2215
2025-02-26 12:07:20,294:INFO:[LightGBM] [Info] Number of data points in the train set: 8523, number of used features: 17
2025-02-26 12:07:20,295:INFO:[LightGBM] [Info] Start training from score 1.615394
2025-02-26 12:07:20,645:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:20,646:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:20,649:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000527 seconds.
2025-02-26 12:07:20,649:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:07:20,649:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:07:20,649:INFO:[LightGBM] [Info] Total Bins 1926
2025-02-26 12:07:20,650:INFO:[LightGBM] [Info] Number of data points in the train set: 7714, number of used features: 17
2025-02-26 12:07:20,650:INFO:[LightGBM] [Info] Start training from score 14.845735
2025-02-26 12:07:20,779:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:20,779:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:20,786:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001852 seconds.
2025-02-26 12:07:20,786:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:20,786:INFO:[LightGBM] [Info] Total Bins 2215
2025-02-26 12:07:20,787:INFO:[LightGBM] [Info] Number of data points in the train set: 8520, number of used features: 17
2025-02-26 12:07:20,788:INFO:[LightGBM] [Info] Start training from score 1.609038
2025-02-26 12:07:21,952:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:21,952:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:21,956:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000949 seconds.
2025-02-26 12:07:21,956:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:21,956:INFO:[LightGBM] [Info] Total Bins 1925
2025-02-26 12:07:21,957:INFO:[LightGBM] [Info] Number of data points in the train set: 7714, number of used features: 17
2025-02-26 12:07:21,960:INFO:[LightGBM] [Info] Start training from score 14.845735
2025-02-26 12:07:22,115:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:22,115:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:22,122:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000472 seconds.
2025-02-26 12:07:22,122:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:07:22,122:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:07:22,122:INFO:[LightGBM] [Info] Total Bins 1767
2025-02-26 12:07:22,122:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-26 12:07:22,137:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:22,137:INFO:[LightGBM] [Warning] [LightGBM] [Info] Start training from score 1965.418992
2025-02-26 12:07:22,138:INFO:For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:22,216:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000937 seconds.
2025-02-26 12:07:22,216:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:07:22,216:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:07:22,216:INFO:[LightGBM] [Info] Total Bins 1920
2025-02-26 12:07:22,216:INFO:[LightGBM] [Info] Number of data points in the train set: 7681, number of used features: 17
2025-02-26 12:07:22,218:INFO:[LightGBM] [Info] Start training from score 14.834266
2025-02-26 12:07:23,407:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:23,407:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:23,411:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000838 seconds.
2025-02-26 12:07:23,411:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:23,411:INFO:[LightGBM] [Info] Total Bins 1766
2025-02-26 12:07:23,411:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-26 12:07:23,412:INFO:[LightGBM] [Info] Start training from score 1965.418992
2025-02-26 12:07:23,905:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:23,905:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:23,914:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000867 seconds.
2025-02-26 12:07:23,914:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:23,915:INFO:[LightGBM] [Info] Total Bins 1771
2025-02-26 12:07:23,915:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-26 12:07:23,923:INFO:[LightGBM] [Info] Start training from score 1965.251163
2025-02-26 12:07:23,974:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:23,975:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:23,977:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000751 seconds.
2025-02-26 12:07:23,977:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:07:23,977:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:07:23,977:INFO:[LightGBM] [Info] Total Bins 2215
2025-02-26 12:07:23,977:INFO:[LightGBM] [Info] Number of data points in the train set: 8523, number of used features: 17
2025-02-26 12:07:23,978:INFO:[LightGBM] [Info] Start training from score 1.615394
2025-02-26 12:07:24,926:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:24,926:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:24,931:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002215 seconds.
2025-02-26 12:07:24,931:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:24,931:INFO:[LightGBM] [Info] Total Bins 2215
2025-02-26 12:07:24,932:INFO:[LightGBM] [Info] Number of data points in the train set: 8523, number of used features: 17
2025-02-26 12:07:24,933:INFO:[LightGBM] [Info] Start training from score 1.615394
2025-02-26 12:07:25,557:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:25,557:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:25,558:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001086 seconds.
2025-02-26 12:07:25,558:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:25,558:INFO:[LightGBM] [Info] Total Bins 2215
2025-02-26 12:07:25,558:INFO:[LightGBM] [Info] Number of data points in the train set: 8520, number of used features: 17
2025-02-26 12:07:25,559:INFO:[LightGBM] [Info] Start training from score 1.609038
2025-02-26 12:07:26,002:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:26,002:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:26,005:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000823 seconds.
2025-02-26 12:07:26,005:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:26,005:INFO:[LightGBM] [Info] Total Bins 1925
2025-02-26 12:07:26,005:INFO:[LightGBM] [Info] Number of data points in the train set: 7714, number of used features: 17
2025-02-26 12:07:26,006:INFO:[LightGBM] [Info] Start training from score 14.845735
2025-02-26 12:07:26,783:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:26,783:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:26,791:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001995 seconds.
2025-02-26 12:07:26,791:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:26,791:INFO:[LightGBM] [Info] Total Bins 1926
2025-02-26 12:07:26,791:INFO:[LightGBM] [Info] Number of data points in the train set: 7714, number of used features: 17
2025-02-26 12:07:26,792:INFO:[LightGBM] [Info] Start training from score 14.845735
2025-02-26 12:07:27,097:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:27,097:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:27,101:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000601 seconds.
2025-02-26 12:07:27,101:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:07:27,101:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:07:27,101:INFO:[LightGBM] [Info] Total Bins 1920
2025-02-26 12:07:27,101:INFO:[LightGBM] [Info] Number of data points in the train set: 7681, number of used features: 17
2025-02-26 12:07:27,102:INFO:[LightGBM] [Info] Start training from score 14.834266
2025-02-26 12:07:27,442:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:27,442:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:27,445:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000457 seconds.
2025-02-26 12:07:27,445:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:07:27,445:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:07:27,445:INFO:[LightGBM] [Info] Total Bins 1766
2025-02-26 12:07:27,445:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-26 12:07:27,446:INFO:[LightGBM] [Info] Start training from score 1965.418992
2025-02-26 12:07:28,269:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:28,269:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:28,272:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000564 seconds.
2025-02-26 12:07:28,272:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:07:28,272:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:07:28,272:INFO:[LightGBM] [Info] Total Bins 1766
2025-02-26 12:07:28,272:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-26 12:07:28,273:INFO:[LightGBM] [Info] Start training from score 1965.418992
2025-02-26 12:07:28,634:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:28,634:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:28,638:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001011 seconds.
2025-02-26 12:07:28,638:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:28,638:INFO:[LightGBM] [Info] Total Bins 1771
2025-02-26 12:07:28,638:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-26 12:07:28,639:INFO:[LightGBM] [Info] Start training from score 1965.251163
2025-02-26 12:07:28,961:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:28,961:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:28,965:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000553 seconds.
2025-02-26 12:07:28,965:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:07:28,965:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:07:28,965:INFO:[LightGBM] [Info] Total Bins 2215
2025-02-26 12:07:28,965:INFO:[LightGBM] [Info] Number of data points in the train set: 8523, number of used features: 17
2025-02-26 12:07:28,967:INFO:[LightGBM] [Info] Start training from score 1.615394
2025-02-26 12:07:29,742:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:29,742:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:29,746:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001030 seconds.
2025-02-26 12:07:29,746:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:29,746:INFO:[LightGBM] [Info] Total Bins 2214
2025-02-26 12:07:29,746:INFO:[LightGBM] [Info] Number of data points in the train set: 8523, number of used features: 17
2025-02-26 12:07:29,746:INFO:[LightGBM] [Info] Start training from score 1.615394
2025-02-26 12:07:30,447:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:30,447:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:30,452:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000764 seconds.
2025-02-26 12:07:30,452:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:07:30,452:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:07:30,452:INFO:[LightGBM] [Info] Total Bins 1926
2025-02-26 12:07:30,452:INFO:[LightGBM] [Info] Number of data points in the train set: 7714, number of used features: 17
2025-02-26 12:07:30,453:INFO:[LightGBM] [Info] Start training from score 14.845735
2025-02-26 12:07:30,988:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:30,988:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:30,992:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000672 seconds.
2025-02-26 12:07:30,992:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:07:30,993:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:07:30,993:INFO:[LightGBM] [Info] Total Bins 1925
2025-02-26 12:07:30,993:INFO:[LightGBM] [Info] Number of data points in the train set: 7714, number of used features: 17
2025-02-26 12:07:30,994:INFO:[LightGBM] [Info] Start training from score 14.845735
2025-02-26 12:07:31,673:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:31,673:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:31,676:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000917 seconds.
2025-02-26 12:07:31,676:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:31,676:INFO:[LightGBM] [Info] Total Bins 1766
2025-02-26 12:07:31,676:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-26 12:07:31,677:INFO:[LightGBM] [Info] Start training from score 1965.418992
2025-02-26 12:07:32,227:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:32,227:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:32,231:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000482 seconds.
2025-02-26 12:07:32,231:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:07:32,231:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:07:32,231:INFO:[LightGBM] [Info] Total Bins 1766
2025-02-26 12:07:32,232:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-26 12:07:32,232:INFO:[LightGBM] [Info] Start training from score 1965.418992
2025-02-26 12:07:32,731:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:32,732:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:32,737:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000704 seconds.
2025-02-26 12:07:32,737:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:07:32,737:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:07:32,737:INFO:[LightGBM] [Info] Total Bins 2214
2025-02-26 12:07:32,737:INFO:[LightGBM] [Info] Number of data points in the train set: 8523, number of used features: 17
2025-02-26 12:07:32,738:INFO:[LightGBM] [Info] Start training from score 1.615394
2025-02-26 12:07:34,102:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:34,102:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:34,122:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007139 seconds.
2025-02-26 12:07:34,122:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:07:34,123:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:07:34,123:INFO:[LightGBM] [Info] Total Bins 1925
2025-02-26 12:07:34,131:INFO:[LightGBM] [Info] Number of data points in the train set: 7714, number of used features: 17
2025-02-26 12:07:34,133:INFO:[LightGBM] [Info] Start training from score 14.845735
2025-02-26 12:07:34,245:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:34,246:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:34,250:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001166 seconds.
2025-02-26 12:07:34,250:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:07:34,250:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:07:34,250:INFO:[LightGBM] [Info] Total Bins 1845
2025-02-26 12:07:34,251:INFO:[LightGBM] [Info] Number of data points in the train set: 8519, number of used features: 17
2025-02-26 12:07:34,251:INFO:[LightGBM] [Info] Start training from score 1.614861
2025-02-26 12:07:35,452:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:35,452:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:35,456:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001361 seconds.
2025-02-26 12:07:35,456:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:35,456:INFO:[LightGBM] [Info] Total Bins 1766
2025-02-26 12:07:35,456:INFO:[LightGBM] [Info] Number of data points in the train set: 5160, number of used features: 17
2025-02-26 12:07:35,457:INFO:[LightGBM] [Info] Start training from score 1965.418992
2025-02-26 12:07:35,705:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:35,705:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:35,709:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000510 seconds.
2025-02-26 12:07:35,709:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:07:35,709:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:07:35,709:INFO:[LightGBM] [Info] Total Bins 1772
2025-02-26 12:07:35,709:INFO:[LightGBM] [Info] Number of data points in the train set: 7681, number of used features: 17
2025-02-26 12:07:35,709:INFO:[LightGBM] [Info] Start training from score 14.912381
2025-02-26 12:07:36,876:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:36,876:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:36,881:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000594 seconds.
2025-02-26 12:07:36,881:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:07:36,881:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:07:36,881:INFO:[LightGBM] [Info] Total Bins 1749
2025-02-26 12:07:36,881:INFO:[LightGBM] [Info] Number of data points in the train set: 5125, number of used features: 17
2025-02-26 12:07:36,882:INFO:[LightGBM] [Info] Start training from score 1965.302829
2025-02-26 12:07:37,683:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:37,684:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:37,705:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000518 seconds.
2025-02-26 12:07:37,705:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:07:37,705:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:07:37,706:INFO:[LightGBM] [Info] Total Bins 2216
2025-02-26 12:07:37,706:INFO:[LightGBM] [Info] Number of data points in the train set: 8519, number of used features: 17
2025-02-26 12:07:37,706:INFO:[LightGBM] [Info] Start training from score 1.614861
2025-02-26 12:07:38,648:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:38,648:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:38,653:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000783 seconds.
2025-02-26 12:07:38,653:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:07:38,653:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:07:38,653:INFO:[LightGBM] [Info] Total Bins 1927
2025-02-26 12:07:38,653:INFO:[LightGBM] [Info] Number of data points in the train set: 7681, number of used features: 17
2025-02-26 12:07:38,654:INFO:[LightGBM] [Info] Start training from score 14.912381
2025-02-26 12:07:39,430:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:39,431:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:39,434:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000490 seconds.
2025-02-26 12:07:39,434:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:07:39,434:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:07:39,434:INFO:[LightGBM] [Info] Total Bins 1749
2025-02-26 12:07:39,434:INFO:[LightGBM] [Info] Number of data points in the train set: 5125, number of used features: 17
2025-02-26 12:07:39,435:INFO:[LightGBM] [Info] Start training from score 1965.302829
2025-02-26 12:07:40,169:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:40,170:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:40,174:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000735 seconds.
2025-02-26 12:07:40,174:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:07:40,174:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:07:40,174:INFO:[LightGBM] [Info] Total Bins 2216
2025-02-26 12:07:40,174:INFO:[LightGBM] [Info] Number of data points in the train set: 8519, number of used features: 17
2025-02-26 12:07:40,175:INFO:[LightGBM] [Info] Start training from score 1.614861
2025-02-26 12:07:40,623:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:40,623:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:40,623:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000585 seconds.
2025-02-26 12:07:40,623:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:07:40,623:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:07:40,623:INFO:[LightGBM] [Info] Total Bins 1837
2025-02-26 12:07:40,623:INFO:[LightGBM] [Info] Number of data points in the train set: 8516, number of used features: 17
2025-02-26 12:07:40,624:INFO:[LightGBM] [Info] Start training from score 1.610263
2025-02-26 12:07:40,624:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:40,624:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:40,624:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002589 seconds.
2025-02-26 12:07:40,624:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:40,624:INFO:[LightGBM] [Info] Total Bins 1759
2025-02-26 12:07:40,624:INFO:[LightGBM] [Info] Number of data points in the train set: 7685, number of used features: 17
2025-02-26 12:07:40,624:INFO:[LightGBM] [Info] Start training from score 14.835003
2025-02-26 12:07:40,624:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:40,624:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:40,624:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000436 seconds.
2025-02-26 12:07:40,624:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:40,624:INFO:[LightGBM] [Info] Total Bins 1760
2025-02-26 12:07:40,624:INFO:[LightGBM] [Info] Number of data points in the train set: 5140, number of used features: 17
2025-02-26 12:07:40,624:INFO:[LightGBM] [Info] Start training from score 1965.178599
2025-02-26 12:07:40,624:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:40,624:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:40,624:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000736 seconds.
2025-02-26 12:07:40,624:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:40,624:INFO:[LightGBM] [Info] Total Bins 2207
2025-02-26 12:07:40,624:INFO:[LightGBM] [Info] Number of data points in the train set: 8516, number of used features: 17
2025-02-26 12:07:40,624:INFO:[LightGBM] [Info] Start training from score 1.610263
2025-02-26 12:07:40,624:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:40,625:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:40,625:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001245 seconds.
2025-02-26 12:07:40,625:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:40,625:INFO:[LightGBM] [Info] Total Bins 1914
2025-02-26 12:07:40,625:INFO:[LightGBM] [Info] Number of data points in the train set: 7685, number of used features: 17
2025-02-26 12:07:40,625:INFO:[LightGBM] [Info] Start training from score 14.835003
2025-02-26 12:07:40,625:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:40,625:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:40,625:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000462 seconds.
2025-02-26 12:07:40,625:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:07:40,625:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:07:40,625:INFO:[LightGBM] [Info] Total Bins 1760
2025-02-26 12:07:40,625:INFO:[LightGBM] [Info] Number of data points in the train set: 5140, number of used features: 17
2025-02-26 12:07:40,625:INFO:[LightGBM] [Info] Start training from score 1965.178599
2025-02-26 12:07:40,625:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:40,625:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:40,625:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.050338 seconds.
2025-02-26 12:07:40,625:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:07:40,625:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:07:40,625:INFO:[LightGBM] [Info] Total Bins 2206
2025-02-26 12:07:40,626:INFO:[LightGBM] [Info] Number of data points in the train set: 8516, number of used features: 17
2025-02-26 12:07:40,626:INFO:[LightGBM] [Info] Start training from score 1.610263
2025-02-26 12:07:40,626:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:40,626:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:40,626:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005523 seconds.
2025-02-26 12:07:40,626:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:07:40,626:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:07:40,626:INFO:[LightGBM] [Info] Total Bins 1914
2025-02-26 12:07:40,626:INFO:[LightGBM] [Info] Number of data points in the train set: 7685, number of used features: 17
2025-02-26 12:07:40,626:INFO:[LightGBM] [Info] Start training from score 14.835003
2025-02-26 12:07:40,626:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:40,628:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:40,628:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.051419 seconds.
2025-02-26 12:07:40,628:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:40,628:INFO:[LightGBM] [Info] Total Bins 1759
2025-02-26 12:07:40,628:INFO:[LightGBM] [Info] Number of data points in the train set: 5140, number of used features: 17
2025-02-26 12:07:40,628:INFO:[LightGBM] [Info] Start training from score 1965.178599
2025-02-26 12:07:40,628:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:40,628:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:40,628:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.084917 seconds.
2025-02-26 12:07:40,628:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:40,628:INFO:[LightGBM] [Info] Total Bins 2208
2025-02-26 12:07:40,628:INFO:[LightGBM] [Info] Number of data points in the train set: 8516, number of used features: 17
2025-02-26 12:07:40,628:INFO:[LightGBM] [Info] Start training from score 1.610263
2025-02-26 12:07:40,629:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:40,629:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:40,629:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008970 seconds.
2025-02-26 12:07:40,629:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:07:40,629:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:07:40,629:INFO:[LightGBM] [Info] Total Bins 1915
2025-02-26 12:07:40,629:INFO:[LightGBM] [Info] Number of data points in the train set: 7685, number of used features: 17
2025-02-26 12:07:40,629:INFO:[LightGBM] [Info] Start training from score 14.835003
2025-02-26 12:07:40,629:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:40,629:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:40,629:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001911 seconds.
2025-02-26 12:07:40,629:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:40,629:INFO:[LightGBM] [Info] Total Bins 1760
2025-02-26 12:07:40,629:INFO:[LightGBM] [Info] Number of data points in the train set: 5140, number of used features: 17
2025-02-26 12:07:40,629:INFO:[LightGBM] [Info] Start training from score 1965.178599
2025-02-26 12:07:40,629:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:40,629:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:40,629:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001589 seconds.
2025-02-26 12:07:40,629:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:40,629:INFO:[LightGBM] [Info] Total Bins 2205
2025-02-26 12:07:40,629:INFO:[LightGBM] [Info] Number of data points in the train set: 8516, number of used features: 17
2025-02-26 12:07:40,629:INFO:[LightGBM] [Info] Start training from score 1.610263
2025-02-26 12:07:40,629:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:40,629:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:40,629:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001824 seconds.
2025-02-26 12:07:40,629:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:40,629:INFO:[LightGBM] [Info] Total Bins 1914
2025-02-26 12:07:40,629:INFO:[LightGBM] [Info] Number of data points in the train set: 7685, number of used features: 17
2025-02-26 12:07:40,629:INFO:[LightGBM] [Info] Start training from score 14.835003
2025-02-26 12:07:40,629:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:40,629:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:40,629:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000694 seconds.
2025-02-26 12:07:40,629:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:40,629:INFO:[LightGBM] [Info] Total Bins 1760
2025-02-26 12:07:40,629:INFO:[LightGBM] [Info] Number of data points in the train set: 5140, number of used features: 17
2025-02-26 12:07:40,629:INFO:[LightGBM] [Info] Start training from score 1965.178599
2025-02-26 12:07:40,629:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:40,629:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:40,629:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000299 seconds.
2025-02-26 12:07:40,629:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:07:40,629:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:07:40,629:INFO:[LightGBM] [Info] Total Bins 1846
2025-02-26 12:07:40,629:INFO:[LightGBM] [Info] Number of data points in the train set: 8518, number of used features: 17
2025-02-26 12:07:40,629:INFO:[LightGBM] [Info] Start training from score 1.611059
2025-02-26 12:07:40,629:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:40,629:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:40,629:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002307 seconds.
2025-02-26 12:07:40,629:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:40,629:INFO:[LightGBM] [Info] Total Bins 1770
2025-02-26 12:07:40,629:INFO:[LightGBM] [Info] Number of data points in the train set: 7677, number of used features: 17
2025-02-26 12:07:40,629:INFO:[LightGBM] [Info] Start training from score 14.874951
2025-02-26 12:07:40,629:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:40,629:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:40,629:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.178070 seconds.
2025-02-26 12:07:40,629:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:40,629:INFO:[LightGBM] [Info] Total Bins 1770
2025-02-26 12:07:40,629:INFO:[LightGBM] [Info] Number of data points in the train set: 5153, number of used features: 17
2025-02-26 12:07:40,629:INFO:[LightGBM] [Info] Start training from score 1965.193091
2025-02-26 12:07:40,629:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:40,629:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:40,629:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000629 seconds.
2025-02-26 12:07:40,629:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:07:40,629:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:07:40,629:INFO:[LightGBM] [Info] Total Bins 2216
2025-02-26 12:07:40,630:INFO:[LightGBM] [Info] Number of data points in the train set: 8518, number of used features: 17
2025-02-26 12:07:40,630:INFO:[LightGBM] [Info] Start training from score 1.611059
2025-02-26 12:07:40,630:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:40,630:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:40,630:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000726 seconds.
2025-02-26 12:07:40,630:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:07:40,630:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:07:40,630:INFO:[LightGBM] [Info] Total Bins 1923
2025-02-26 12:07:40,630:INFO:[LightGBM] [Info] Number of data points in the train set: 7677, number of used features: 17
2025-02-26 12:07:40,630:INFO:[LightGBM] [Info] Start training from score 14.874951
2025-02-26 12:07:40,630:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:40,630:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:40,630:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001408 seconds.
2025-02-26 12:07:40,630:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:40,630:INFO:[LightGBM] [Info] Total Bins 1770
2025-02-26 12:07:40,630:INFO:[LightGBM] [Info] Number of data points in the train set: 5153, number of used features: 17
2025-02-26 12:07:40,630:INFO:[LightGBM] [Info] Start training from score 1965.193091
2025-02-26 12:07:40,630:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:40,630:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:40,630:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001579 seconds.
2025-02-26 12:07:40,630:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:40,630:INFO:[LightGBM] [Info] Total Bins 2216
2025-02-26 12:07:40,630:INFO:[LightGBM] [Info] Number of data points in the train set: 8518, number of used features: 17
2025-02-26 12:07:40,630:INFO:[LightGBM] [Info] Start training from score 1.611059
2025-02-26 12:07:40,630:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:40,630:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:40,630:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000788 seconds.
2025-02-26 12:07:40,630:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:07:40,630:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:07:40,630:INFO:[LightGBM] [Info] Total Bins 1924
2025-02-26 12:07:40,630:INFO:[LightGBM] [Info] Number of data points in the train set: 7677, number of used features: 17
2025-02-26 12:07:40,630:INFO:[LightGBM] [Info] Start training from score 14.874951
2025-02-26 12:07:40,630:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:40,630:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:40,630:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000641 seconds.
2025-02-26 12:07:40,630:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:40,630:INFO:[LightGBM] [Info] Total Bins 1770
2025-02-26 12:07:40,630:INFO:[LightGBM] [Info] Number of data points in the train set: 5153, number of used features: 17
2025-02-26 12:07:40,630:INFO:[LightGBM] [Info] Start training from score 1965.193091
2025-02-26 12:07:40,630:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:40,630:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:40,630:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000457 seconds.
2025-02-26 12:07:40,630:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:07:40,630:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:07:40,630:INFO:[LightGBM] [Info] Total Bins 2216
2025-02-26 12:07:40,630:INFO:[LightGBM] [Info] Number of data points in the train set: 8518, number of used features: 17
2025-02-26 12:07:40,630:INFO:[LightGBM] [Info] Start training from score 1.611059
2025-02-26 12:07:40,630:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:40,630:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:40,630:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002267 seconds.
2025-02-26 12:07:40,630:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:40,630:INFO:[LightGBM] [Info] Total Bins 1923
2025-02-26 12:07:40,630:INFO:[LightGBM] [Info] Number of data points in the train set: 7677, number of used features: 17
2025-02-26 12:07:40,630:INFO:[LightGBM] [Info] Start training from score 14.874951
2025-02-26 12:07:40,630:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:40,630:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:40,630:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002502 seconds.
2025-02-26 12:07:40,630:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:40,630:INFO:[LightGBM] [Info] Total Bins 1770
2025-02-26 12:07:40,630:INFO:[LightGBM] [Info] Number of data points in the train set: 5153, number of used features: 17
2025-02-26 12:07:40,630:INFO:[LightGBM] [Info] Start training from score 1965.193091
2025-02-26 12:07:40,630:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:40,630:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:40,630:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000802 seconds.
2025-02-26 12:07:40,630:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:40,630:INFO:[LightGBM] [Info] Total Bins 2215
2025-02-26 12:07:40,630:INFO:[LightGBM] [Info] Number of data points in the train set: 8518, number of used features: 17
2025-02-26 12:07:40,630:INFO:[LightGBM] [Info] Start training from score 1.611059
2025-02-26 12:07:40,631:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:40,631:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:40,631:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014743 seconds.
2025-02-26 12:07:40,631:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:40,631:INFO:[LightGBM] [Info] Total Bins 1924
2025-02-26 12:07:40,631:INFO:[LightGBM] [Info] Number of data points in the train set: 7677, number of used features: 17
2025-02-26 12:07:40,631:INFO:[LightGBM] [Info] Start training from score 14.874951
2025-02-26 12:07:40,631:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:40,631:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:40,631:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001226 seconds.
2025-02-26 12:07:40,631:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:40,631:INFO:[LightGBM] [Info] Total Bins 1770
2025-02-26 12:07:40,631:INFO:[LightGBM] [Info] Number of data points in the train set: 5153, number of used features: 17
2025-02-26 12:07:40,631:INFO:[LightGBM] [Info] Start training from score 1965.193091
2025-02-26 12:07:40,631:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:40,631:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:40,631:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004004 seconds.
2025-02-26 12:07:40,631:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:40,631:INFO:[LightGBM] [Info] Total Bins 1839
2025-02-26 12:07:40,631:INFO:[LightGBM] [Info] Number of data points in the train set: 8525, number of used features: 17
2025-02-26 12:07:40,631:INFO:[LightGBM] [Info] Start training from score 1.606921
2025-02-26 12:07:40,631:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:40,631:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:40,631:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000881 seconds.
2025-02-26 12:07:40,631:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:07:40,631:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:07:40,631:INFO:[LightGBM] [Info] Total Bins 1764
2025-02-26 12:07:40,631:INFO:[LightGBM] [Info] Number of data points in the train set: 7690, number of used features: 17
2025-02-26 12:07:40,631:INFO:[LightGBM] [Info] Start training from score 14.915475
2025-02-26 12:07:40,631:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:40,631:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:40,631:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000636 seconds.
2025-02-26 12:07:40,631:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:40,631:INFO:[LightGBM] [Info] Total Bins 1769
2025-02-26 12:07:40,631:INFO:[LightGBM] [Info] Number of data points in the train set: 5167, number of used features: 17
2025-02-26 12:07:40,631:INFO:[LightGBM] [Info] Start training from score 1965.395394
2025-02-26 12:07:40,631:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:40,631:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:40,631:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003159 seconds.
2025-02-26 12:07:40,631:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:40,631:INFO:[LightGBM] [Info] Total Bins 2210
2025-02-26 12:07:40,631:INFO:[LightGBM] [Info] Number of data points in the train set: 8525, number of used features: 17
2025-02-26 12:07:40,631:INFO:[LightGBM] [Info] Start training from score 1.606921
2025-02-26 12:07:40,631:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:40,631:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:40,631:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000996 seconds.
2025-02-26 12:07:40,631:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:40,631:INFO:[LightGBM] [Info] Total Bins 1919
2025-02-26 12:07:40,631:INFO:[LightGBM] [Info] Number of data points in the train set: 7690, number of used features: 17
2025-02-26 12:07:40,631:INFO:[LightGBM] [Info] Start training from score 14.915475
2025-02-26 12:07:40,631:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:40,631:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:40,631:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002770 seconds.
2025-02-26 12:07:40,631:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:40,631:INFO:[LightGBM] [Info] Total Bins 1769
2025-02-26 12:07:40,631:INFO:[LightGBM] [Info] Number of data points in the train set: 5167, number of used features: 17
2025-02-26 12:07:40,631:INFO:[LightGBM] [Info] Start training from score 1965.395394
2025-02-26 12:07:40,631:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:40,631:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:40,631:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002791 seconds.
2025-02-26 12:07:40,631:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:40,631:INFO:[LightGBM] [Info] Total Bins 2209
2025-02-26 12:07:40,631:INFO:[LightGBM] [Info] Number of data points in the train set: 8525, number of used features: 17
2025-02-26 12:07:40,631:INFO:[LightGBM] [Info] Start training from score 1.606921
2025-02-26 12:07:40,631:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:40,631:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:40,631:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001067 seconds.
2025-02-26 12:07:40,631:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:40,632:INFO:[LightGBM] [Info] Total Bins 1921
2025-02-26 12:07:40,632:INFO:[LightGBM] [Info] Number of data points in the train set: 7690, number of used features: 17
2025-02-26 12:07:40,632:INFO:[LightGBM] [Info] Start training from score 14.915475
2025-02-26 12:07:40,632:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:40,632:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:40,632:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001316 seconds.
2025-02-26 12:07:40,632:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:40,632:INFO:[LightGBM] [Info] Total Bins 1770
2025-02-26 12:07:40,632:INFO:[LightGBM] [Info] Number of data points in the train set: 5167, number of used features: 17
2025-02-26 12:07:40,632:INFO:[LightGBM] [Info] Start training from score 1965.395394
2025-02-26 12:07:40,632:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:40,632:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:40,632:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000889 seconds.
2025-02-26 12:07:40,632:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:40,632:INFO:[LightGBM] [Info] Total Bins 2211
2025-02-26 12:07:40,632:INFO:[LightGBM] [Info] Number of data points in the train set: 8525, number of used features: 17
2025-02-26 12:07:40,632:INFO:[LightGBM] [Info] Start training from score 1.606921
2025-02-26 12:07:40,632:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:40,632:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:40,632:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001169 seconds.
2025-02-26 12:07:40,632:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:40,632:INFO:[LightGBM] [Info] Total Bins 1921
2025-02-26 12:07:40,632:INFO:[LightGBM] [Info] Number of data points in the train set: 7690, number of used features: 17
2025-02-26 12:07:40,632:INFO:[LightGBM] [Info] Start training from score 14.915475
2025-02-26 12:07:40,632:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:40,632:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:40,632:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000622 seconds.
2025-02-26 12:07:40,632:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:40,632:INFO:[LightGBM] [Info] Total Bins 1769
2025-02-26 12:07:40,632:INFO:[LightGBM] [Info] Number of data points in the train set: 5167, number of used features: 17
2025-02-26 12:07:40,632:INFO:[LightGBM] [Info] Start training from score 1965.395394
2025-02-26 12:07:40,632:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:40,632:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:40,632:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001679 seconds.
2025-02-26 12:07:40,632:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:40,632:INFO:[LightGBM] [Info] Total Bins 2210
2025-02-26 12:07:40,632:INFO:[LightGBM] [Info] Number of data points in the train set: 8525, number of used features: 17
2025-02-26 12:07:40,632:INFO:[LightGBM] [Info] Start training from score 1.606921
2025-02-26 12:07:40,632:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:40,632:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:40,632:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001018 seconds.
2025-02-26 12:07:40,632:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:40,632:INFO:[LightGBM] [Info] Total Bins 1921
2025-02-26 12:07:40,632:INFO:[LightGBM] [Info] Number of data points in the train set: 7690, number of used features: 17
2025-02-26 12:07:40,632:INFO:[LightGBM] [Info] Start training from score 14.915475
2025-02-26 12:07:40,632:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:40,632:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:40,632:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000768 seconds.
2025-02-26 12:07:40,632:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:40,632:INFO:[LightGBM] [Info] Total Bins 1770
2025-02-26 12:07:40,632:INFO:[LightGBM] [Info] Number of data points in the train set: 5167, number of used features: 17
2025-02-26 12:07:40,632:INFO:[LightGBM] [Info] Start training from score 1965.395394
2025-02-26 12:07:40,632:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:40,632:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:40,632:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003893 seconds.
2025-02-26 12:07:40,632:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:40,632:INFO:[LightGBM] [Info] Total Bins 1843
2025-02-26 12:07:40,632:INFO:[LightGBM] [Info] Number of data points in the train set: 8520, number of used features: 17
2025-02-26 12:07:40,632:INFO:[LightGBM] [Info] Start training from score 1.608099
2025-02-26 12:07:40,632:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:40,632:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:40,632:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000861 seconds.
2025-02-26 12:07:40,632:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:40,632:INFO:[LightGBM] [Info] Total Bins 1766
2025-02-26 12:07:40,632:INFO:[LightGBM] [Info] Number of data points in the train set: 7690, number of used features: 17
2025-02-26 12:07:40,632:INFO:[LightGBM] [Info] Start training from score 14.825618
2025-02-26 12:07:40,632:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:40,633:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:40,633:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.096745 seconds.
2025-02-26 12:07:40,633:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:40,633:INFO:[LightGBM] [Info] Total Bins 1757
2025-02-26 12:07:40,633:INFO:[LightGBM] [Info] Number of data points in the train set: 5152, number of used features: 17
2025-02-26 12:07:40,633:INFO:[LightGBM] [Info] Start training from score 1965.258152
2025-02-26 12:07:40,633:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:40,633:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:40,633:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001079 seconds.
2025-02-26 12:07:40,633:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:40,633:INFO:[LightGBM] [Info] Total Bins 2212
2025-02-26 12:07:40,633:INFO:[LightGBM] [Info] Number of data points in the train set: 8520, number of used features: 17
2025-02-26 12:07:40,633:INFO:[LightGBM] [Info] Start training from score 1.608099
2025-02-26 12:07:40,633:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:40,633:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:40,633:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001740 seconds.
2025-02-26 12:07:40,633:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:40,633:INFO:[LightGBM] [Info] Total Bins 1919
2025-02-26 12:07:40,633:INFO:[LightGBM] [Info] Number of data points in the train set: 7690, number of used features: 17
2025-02-26 12:07:40,633:INFO:[LightGBM] [Info] Start training from score 14.825618
2025-02-26 12:07:40,633:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:40,633:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:40,633:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.142548 seconds.
2025-02-26 12:07:40,633:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:07:40,633:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:07:40,633:INFO:[LightGBM] [Info] Total Bins 1758
2025-02-26 12:07:40,633:INFO:[LightGBM] [Info] Number of data points in the train set: 5152, number of used features: 17
2025-02-26 12:07:40,633:INFO:[LightGBM] [Info] Start training from score 1965.258152
2025-02-26 12:07:40,633:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:40,633:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:40,633:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001500 seconds.
2025-02-26 12:07:40,633:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:40,633:INFO:[LightGBM] [Info] Total Bins 2212
2025-02-26 12:07:40,633:INFO:[LightGBM] [Info] Number of data points in the train set: 8520, number of used features: 17
2025-02-26 12:07:40,633:INFO:[LightGBM] [Info] Start training from score 1.608099
2025-02-26 12:07:40,633:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:40,633:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:40,633:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001203 seconds.
2025-02-26 12:07:40,633:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:40,633:INFO:[LightGBM] [Info] Total Bins 1919
2025-02-26 12:07:40,633:INFO:[LightGBM] [Info] Number of data points in the train set: 7690, number of used features: 17
2025-02-26 12:07:40,633:INFO:[LightGBM] [Info] Start training from score 14.825618
2025-02-26 12:07:40,633:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:40,633:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:40,633:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004097 seconds.
2025-02-26 12:07:40,633:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:40,633:INFO:[LightGBM] [Info] Total Bins 1758
2025-02-26 12:07:40,633:INFO:[LightGBM] [Info] Number of data points in the train set: 5152, number of used features: 17
2025-02-26 12:07:40,633:INFO:[LightGBM] [Info] Start training from score 1965.258152
2025-02-26 12:07:40,633:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:40,633:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:40,633:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001620 seconds.
2025-02-26 12:07:40,633:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:40,633:INFO:[LightGBM] [Info] Total Bins 2212
2025-02-26 12:07:40,633:INFO:[LightGBM] [Info] Number of data points in the train set: 8520, number of used features: 17
2025-02-26 12:07:40,633:INFO:[LightGBM] [Info] Start training from score 1.608099
2025-02-26 12:07:40,633:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:40,633:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:40,633:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001546 seconds.
2025-02-26 12:07:40,633:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:40,633:INFO:[LightGBM] [Info] Total Bins 1919
2025-02-26 12:07:40,633:INFO:[LightGBM] [Info] Number of data points in the train set: 7690, number of used features: 17
2025-02-26 12:07:40,633:INFO:[LightGBM] [Info] Start training from score 14.825618
2025-02-26 12:07:40,633:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:40,633:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:40,633:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004227 seconds.
2025-02-26 12:07:40,634:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:40,634:INFO:[LightGBM] [Info] Total Bins 1757
2025-02-26 12:07:40,634:INFO:[LightGBM] [Info] Number of data points in the train set: 5152, number of used features: 17
2025-02-26 12:07:40,634:INFO:[LightGBM] [Info] Start training from score 1965.258152
2025-02-26 12:07:40,634:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:40,634:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:40,634:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065813 seconds.
2025-02-26 12:07:40,634:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:07:40,634:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:07:40,634:INFO:[LightGBM] [Info] Total Bins 2212
2025-02-26 12:07:40,634:INFO:[LightGBM] [Info] Number of data points in the train set: 8520, number of used features: 17
2025-02-26 12:07:40,634:INFO:[LightGBM] [Info] Start training from score 1.608099
2025-02-26 12:07:40,634:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:40,634:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:40,634:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001539 seconds.
2025-02-26 12:07:40,634:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:40,634:INFO:[LightGBM] [Info] Total Bins 1920
2025-02-26 12:07:40,634:INFO:[LightGBM] [Info] Number of data points in the train set: 7690, number of used features: 17
2025-02-26 12:07:40,634:INFO:[LightGBM] [Info] Start training from score 14.825618
2025-02-26 12:07:40,634:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:40,634:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:40,634:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000835 seconds.
2025-02-26 12:07:40,634:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:40,634:INFO:[LightGBM] [Info] Total Bins 1757
2025-02-26 12:07:40,634:INFO:[LightGBM] [Info] Number of data points in the train set: 5152, number of used features: 17
2025-02-26 12:07:40,634:INFO:[LightGBM] [Info] Start training from score 1965.258152
2025-02-26 12:07:40,634:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:40,634:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:40,634:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002198 seconds.
2025-02-26 12:07:40,634:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:40,634:INFO:[LightGBM] [Info] Total Bins 1836
2025-02-26 12:07:40,634:INFO:[LightGBM] [Info] Number of data points in the train set: 8520, number of used features: 17
2025-02-26 12:07:40,634:INFO:[LightGBM] [Info] Start training from score 1.614906
2025-02-26 12:07:40,634:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:40,634:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:40,634:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035491 seconds.
2025-02-26 12:07:40,634:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:07:40,634:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:07:40,634:INFO:[LightGBM] [Info] Total Bins 1761
2025-02-26 12:07:40,634:INFO:[LightGBM] [Info] Number of data points in the train set: 7687, number of used features: 17
2025-02-26 12:07:40,634:INFO:[LightGBM] [Info] Start training from score 14.905945
2025-02-26 12:07:40,634:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:40,634:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:40,634:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001967 seconds.
2025-02-26 12:07:40,634:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:40,634:INFO:[LightGBM] [Info] Total Bins 1759
2025-02-26 12:07:40,634:INFO:[LightGBM] [Info] Number of data points in the train set: 5176, number of used features: 17
2025-02-26 12:07:40,634:INFO:[LightGBM] [Info] Start training from score 1965.290958
2025-02-26 12:07:40,634:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:40,634:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:40,634:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000906 seconds.
2025-02-26 12:07:40,634:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:40,634:INFO:[LightGBM] [Info] Total Bins 2206
2025-02-26 12:07:40,634:INFO:[LightGBM] [Info] Number of data points in the train set: 8520, number of used features: 17
2025-02-26 12:07:40,634:INFO:[LightGBM] [Info] Start training from score 1.614906
2025-02-26 12:07:40,634:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:40,634:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:40,634:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000721 seconds.
2025-02-26 12:07:40,634:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:40,634:INFO:[LightGBM] [Info] Total Bins 1916
2025-02-26 12:07:40,634:INFO:[LightGBM] [Info] Number of data points in the train set: 7687, number of used features: 17
2025-02-26 12:07:40,645:INFO:[LightGBM] [Info] Start training from score 14.905945
2025-02-26 12:07:40,650:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:40,650:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:40,650:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002315 seconds.
2025-02-26 12:07:40,650:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:40,650:INFO:[LightGBM] [Info] Total Bins 1758
2025-02-26 12:07:40,650:INFO:[LightGBM] [Info] Number of data points in the train set: 5176, number of used features: 17
2025-02-26 12:07:40,650:INFO:[LightGBM] [Info] Start training from score 1965.290958
2025-02-26 12:07:40,650:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:40,650:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:40,650:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019379 seconds.
2025-02-26 12:07:40,650:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:40,651:INFO:[LightGBM] [Info] Total Bins 2206
2025-02-26 12:07:40,651:INFO:[LightGBM] [Info] Number of data points in the train set: 8520, number of used features: 17
2025-02-26 12:07:40,651:INFO:[LightGBM] [Info] Start training from score 1.614906
2025-02-26 12:07:40,651:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:40,651:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:40,651:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002476 seconds.
2025-02-26 12:07:40,651:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:40,651:INFO:[LightGBM] [Info] Total Bins 1916
2025-02-26 12:07:40,651:INFO:[LightGBM] [Info] Number of data points in the train set: 7687, number of used features: 17
2025-02-26 12:07:40,651:INFO:[LightGBM] [Info] Start training from score 14.905945
2025-02-26 12:07:40,651:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:40,651:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:40,651:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000757 seconds.
2025-02-26 12:07:40,651:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:40,651:INFO:[LightGBM] [Info] Total Bins 1759
2025-02-26 12:07:40,651:INFO:[LightGBM] [Info] Number of data points in the train set: 5176, number of used features: 17
2025-02-26 12:07:40,651:INFO:[LightGBM] [Info] Start training from score 1965.290958
2025-02-26 12:07:40,651:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:40,651:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:40,651:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000697 seconds.
2025-02-26 12:07:40,651:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:40,651:INFO:[LightGBM] [Info] Total Bins 2206
2025-02-26 12:07:40,651:INFO:[LightGBM] [Info] Number of data points in the train set: 8520, number of used features: 17
2025-02-26 12:07:40,652:INFO:[LightGBM] [Info] Start training from score 1.614906
2025-02-26 12:07:40,652:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:40,652:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:40,652:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004141 seconds.
2025-02-26 12:07:40,652:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:40,652:INFO:[LightGBM] [Info] Total Bins 1914
2025-02-26 12:07:40,652:INFO:[LightGBM] [Info] Number of data points in the train set: 7687, number of used features: 17
2025-02-26 12:07:40,652:INFO:[LightGBM] [Info] Start training from score 14.905945
2025-02-26 12:07:40,652:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:40,652:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:40,652:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002230 seconds.
2025-02-26 12:07:40,652:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:40,652:INFO:[LightGBM] [Info] Total Bins 1758
2025-02-26 12:07:40,652:INFO:[LightGBM] [Info] Number of data points in the train set: 5176, number of used features: 17
2025-02-26 12:07:40,652:INFO:[LightGBM] [Info] Start training from score 1965.290958
2025-02-26 12:07:40,652:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:40,652:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:40,652:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008360 seconds.
2025-02-26 12:07:40,652:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:40,652:INFO:[LightGBM] [Info] Total Bins 2206
2025-02-26 12:07:40,652:INFO:[LightGBM] [Info] Number of data points in the train set: 8520, number of used features: 17
2025-02-26 12:07:40,652:INFO:[LightGBM] [Info] Start training from score 1.614906
2025-02-26 12:07:40,652:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:40,653:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:40,653:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002113 seconds.
2025-02-26 12:07:40,653:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:40,653:INFO:[LightGBM] [Info] Total Bins 1915
2025-02-26 12:07:40,653:INFO:[LightGBM] [Info] Number of data points in the train set: 7687, number of used features: 17
2025-02-26 12:07:40,653:INFO:[LightGBM] [Info] Start training from score 14.905945
2025-02-26 12:07:40,653:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:07:40,653:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:07:40,653:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000719 seconds.
2025-02-26 12:07:40,653:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:07:40,653:INFO:[LightGBM] [Info] Total Bins 1758
2025-02-26 12:07:40,653:INFO:[LightGBM] [Info] Number of data points in the train set: 5176, number of used features: 17
2025-02-26 12:07:40,653:INFO:[LightGBM] [Info] Start training from score 1965.290958
2025-02-26 12:07:54,107:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-26 12:07:54,107:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-26 12:07:54,107:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-26 12:07:54,107:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-26 12:07:55,407:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_29592/2838843215.py:2: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.boxplot(data=df, x="Method", y="Price", palette="viridis")

2025-02-26 12:07:55,523:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_29592/2297183518.py:4: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.barplot(x=avg_price_by_seller.index, y=avg_price_by_seller.values, palette="viridis")

2025-02-26 12:07:56,666:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_29592/1639960328.py:2: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.boxplot(

2025-02-26 12:07:57,120:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_29592/2157425422.py:3: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.violinplot(data=df, x="Bedroom2", y="Price", palette="viridis", ax=axes[0])

2025-02-26 12:07:57,253:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_29592/2157425422.py:8: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.violinplot(data=df, x="Bathroom", y="Price", palette="viridis", ax=axes[1])

2025-02-26 12:07:57,340:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_29592/2157425422.py:13: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.violinplot(data=df, x="Car", y="Price", palette="viridis", ax=axes[2])

2025-02-26 12:07:58,609:INFO:PyCaret RegressionExperiment
2025-02-26 12:07:58,609:INFO:Logging name: house_pricing
2025-02-26 12:07:58,609:INFO:ML Usecase: MLUsecase.REGRESSION
2025-02-26 12:07:58,609:INFO:version 3.3.2
2025-02-26 12:07:58,609:INFO:Initializing setup()
2025-02-26 12:07:58,609:INFO:self.USI: 98f1
2025-02-26 12:07:58,609:INFO:self._variable_keys: {'transform_target_param', 'memory', 'idx', 'n_jobs_param', 'gpu_param', 'exp_id', 'html_param', 'target_param', 'pipeline', 'USI', 'data', 'y', 'fold_groups_param', '_ml_usecase', 'y_test', 'log_plots_param', 'X_train', 'fold_generator', 'logging_param', 'exp_name_log', 'gpu_n_jobs_param', '_available_plots', 'y_train', 'seed', 'X_test', 'X', 'fold_shuffle_param'}
2025-02-26 12:07:58,609:INFO:Checking environment
2025-02-26 12:07:58,609:INFO:python_version: 3.10.15
2025-02-26 12:07:58,609:INFO:python_build: ('main', 'Oct  3 2024 02:24:49')
2025-02-26 12:07:58,609:INFO:machine: arm64
2025-02-26 12:07:58,609:INFO:platform: macOS-14.1-arm64-arm-64bit
2025-02-26 12:07:58,609:INFO:Memory: svmem(total=17179869184, available=6308478976, percent=63.3, used=4986634240, free=2974597120, active=2988310528, inactive=3220930560, wired=1998323712)
2025-02-26 12:07:58,609:INFO:Physical Core: 8
2025-02-26 12:07:58,609:INFO:Logical Core: 8
2025-02-26 12:07:58,609:INFO:Checking libraries
2025-02-26 12:07:58,609:INFO:System:
2025-02-26 12:07:58,609:INFO:    python: 3.10.15 (main, Oct  3 2024, 02:24:49) [Clang 14.0.6 ]
2025-02-26 12:07:58,609:INFO:executable: /Users/daaa/opt/miniconda3/envs/mlops/bin/python
2025-02-26 12:07:58,609:INFO:   machine: macOS-14.1-arm64-arm-64bit
2025-02-26 12:07:58,609:INFO:PyCaret required dependencies:
2025-02-26 12:07:59,029:INFO:                 pip: 24.2
2025-02-26 12:07:59,029:INFO:          setuptools: 75.1.0
2025-02-26 12:07:59,029:INFO:             pycaret: 3.3.2
2025-02-26 12:07:59,029:INFO:             IPython: 8.30.0
2025-02-26 12:07:59,029:INFO:          ipywidgets: 8.1.5
2025-02-26 12:07:59,029:INFO:                tqdm: 4.67.1
2025-02-26 12:07:59,029:INFO:               numpy: 1.26.4
2025-02-26 12:07:59,029:INFO:              pandas: 2.1.4
2025-02-26 12:07:59,029:INFO:              jinja2: 3.1.4
2025-02-26 12:07:59,029:INFO:               scipy: 1.11.4
2025-02-26 12:07:59,029:INFO:              joblib: 1.3.2
2025-02-26 12:07:59,029:INFO:             sklearn: 1.4.2
2025-02-26 12:07:59,029:INFO:                pyod: 2.0.2
2025-02-26 12:07:59,030:INFO:            imblearn: 0.12.4
2025-02-26 12:07:59,030:INFO:   category_encoders: 2.6.4
2025-02-26 12:07:59,030:INFO:            lightgbm: 4.5.0
2025-02-26 12:07:59,030:INFO:               numba: 0.60.0
2025-02-26 12:07:59,030:INFO:            requests: 2.32.3
2025-02-26 12:07:59,030:INFO:          matplotlib: 3.10.0
2025-02-26 12:07:59,030:INFO:          scikitplot: 0.3.7
2025-02-26 12:07:59,030:INFO:         yellowbrick: 1.5
2025-02-26 12:07:59,030:INFO:              plotly: 5.24.1
2025-02-26 12:07:59,030:INFO:    plotly-resampler: Not installed
2025-02-26 12:07:59,030:INFO:             kaleido: 0.2.1
2025-02-26 12:07:59,030:INFO:           schemdraw: 0.15
2025-02-26 12:07:59,030:INFO:         statsmodels: 0.14.4
2025-02-26 12:07:59,030:INFO:              sktime: 0.26.0
2025-02-26 12:07:59,030:INFO:               tbats: 1.1.3
2025-02-26 12:07:59,030:INFO:            pmdarima: 2.0.4
2025-02-26 12:07:59,030:INFO:              psutil: 6.1.0
2025-02-26 12:07:59,030:INFO:          markupsafe: 2.1.5
2025-02-26 12:07:59,030:INFO:             pickle5: Not installed
2025-02-26 12:07:59,030:INFO:         cloudpickle: 3.1.0
2025-02-26 12:07:59,030:INFO:         deprecation: 2.1.0
2025-02-26 12:07:59,030:INFO:              xxhash: 3.5.0
2025-02-26 12:07:59,030:INFO:           wurlitzer: 3.1.1
2025-02-26 12:07:59,030:INFO:PyCaret optional dependencies:
2025-02-26 12:08:00,326:INFO:                shap: 0.44.1
2025-02-26 12:08:00,326:INFO:           interpret: 0.6.9
2025-02-26 12:08:00,326:INFO:                umap: 0.5.7
2025-02-26 12:08:00,326:INFO:     ydata_profiling: 4.12.1
2025-02-26 12:08:00,326:INFO:  explainerdashboard: 0.4.8
2025-02-26 12:08:00,326:INFO:             autoviz: Not installed
2025-02-26 12:08:00,326:INFO:           fairlearn: 0.7.0
2025-02-26 12:08:00,326:INFO:          deepchecks: Not installed
2025-02-26 12:08:00,326:INFO:             xgboost: 2.1.3
2025-02-26 12:08:00,326:INFO:            catboost: 1.1.1
2025-02-26 12:08:00,326:INFO:              kmodes: 0.12.2
2025-02-26 12:08:00,326:INFO:             mlxtend: 0.23.3
2025-02-26 12:08:00,326:INFO:       statsforecast: 1.5.0
2025-02-26 12:08:00,326:INFO:        tune_sklearn: 0.5.0
2025-02-26 12:08:00,326:INFO:                 ray: 2.40.0
2025-02-26 12:08:00,326:INFO:            hyperopt: 0.2.7
2025-02-26 12:08:00,326:INFO:              optuna: 4.1.0
2025-02-26 12:08:00,326:INFO:               skopt: 0.10.2
2025-02-26 12:08:00,326:INFO:              mlflow: 2.16.0
2025-02-26 12:08:00,326:INFO:              gradio: 5.12.0
2025-02-26 12:08:00,326:INFO:             fastapi: 0.115.6
2025-02-26 12:08:00,326:INFO:             uvicorn: 0.34.0
2025-02-26 12:08:00,326:INFO:              m2cgen: 0.10.0
2025-02-26 12:08:00,326:INFO:           evidently: 0.4.40
2025-02-26 12:08:00,326:INFO:               fugue: 0.8.7
2025-02-26 12:08:00,326:INFO:           streamlit: Not installed
2025-02-26 12:08:00,326:INFO:             prophet: Not installed
2025-02-26 12:08:00,326:INFO:None
2025-02-26 12:08:00,326:INFO:Set up data.
2025-02-26 12:08:00,347:INFO:Set up folding strategy.
2025-02-26 12:08:00,348:INFO:Set up train/test split.
2025-02-26 12:08:00,355:INFO:Set up index.
2025-02-26 12:08:00,355:INFO:Assigning column types.
2025-02-26 12:08:00,359:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-02-26 12:08:00,359:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2025-02-26 12:08:00,362:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-26 12:08:00,365:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-26 12:08:00,399:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-26 12:08:00,423:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-26 12:08:00,423:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-26 12:08:00,424:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-26 12:08:00,439:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2025-02-26 12:08:00,442:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-26 12:08:00,444:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-26 12:08:00,477:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-26 12:08:00,500:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-26 12:08:00,501:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-26 12:08:00,502:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-26 12:08:00,502:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2025-02-26 12:08:00,505:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-26 12:08:00,507:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-26 12:08:00,539:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-26 12:08:00,562:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-26 12:08:00,563:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-26 12:08:00,564:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-26 12:08:00,567:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-26 12:08:00,569:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-26 12:08:00,602:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-26 12:08:00,626:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-26 12:08:00,626:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-26 12:08:00,628:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-26 12:08:00,628:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2025-02-26 12:08:00,633:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-26 12:08:00,666:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-26 12:08:00,690:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-26 12:08:00,691:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-26 12:08:00,692:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-26 12:08:00,697:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-26 12:08:00,730:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-26 12:08:00,754:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-26 12:08:00,754:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-26 12:08:00,756:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-26 12:08:00,756:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2025-02-26 12:08:00,796:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-26 12:08:00,820:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-26 12:08:00,820:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-26 12:08:00,822:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-26 12:08:00,862:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-26 12:08:00,887:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-26 12:08:00,887:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-26 12:08:00,889:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-26 12:08:00,889:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-02-26 12:08:00,930:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-26 12:08:00,954:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-26 12:08:00,956:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-26 12:08:00,995:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-26 12:08:01,020:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-26 12:08:01,021:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-26 12:08:01,021:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2025-02-26 12:08:01,083:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-26 12:08:01,085:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-26 12:08:01,149:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-26 12:08:01,151:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-26 12:08:01,152:INFO:Preparing preprocessing pipeline...
2025-02-26 12:08:01,152:INFO:Set up date feature engineering.
2025-02-26 12:08:01,152:INFO:Set up iterative imputation.
2025-02-26 12:08:01,214:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-26 12:08:01,216:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-26 12:08:01,260:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-26 12:08:01,280:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-26 12:08:01,282:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-26 12:08:01,299:INFO:Set up encoding of ordinal features.
2025-02-26 12:08:01,303:WARNING:The number of classes passed to feature Method in the ordinal_features parameter (11) don't match with the number of classes in the data (5).
2025-02-26 12:08:01,303:INFO:Set up encoding of categorical features.
2025-02-26 12:08:01,304:INFO:Set up polynomial features.
2025-02-26 12:08:01,304:INFO:Set up removing multicollinearity.
2025-02-26 12:08:01,304:INFO:Set up feature normalization.
2025-02-26 12:08:01,304:INFO:Set up feature selection.
2025-02-26 12:08:01,365:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-26 12:08:01,367:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-26 12:08:01,772:INFO:Finished creating preprocessing pipeline.
2025-02-26 12:08:01,816:INFO:Pipeline: Pipeline(memory=FastMemory(location=/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/joblib),
         steps=[('date_feature_extractor',
                 TransformerWrapper(include=['Date'],
                                    transformer=ExtractDateTimeFeatures())),
                ('iterative_imputer',
                 TransformerWrapper(transformer=IterativeImputer(cat_estimator=LGBMClassifier(n_jobs=-1,
                                                                                              random_state=123),
                                                                 cat_estimator_prepare_...
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('remove_multicollinearity',
                 TransformerWrapper(exclude=[],
                                    transformer=RemoveMulticollinearity(threshold=0.55))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('feature_selection',
                 TransformerWrapper(exclude=[],
                                    transformer=SelectFromModel(estimator=RandomForestRegressor(),
                                                                max_features=12,
                                                                threshold=-inf)))])
2025-02-26 12:08:01,816:INFO:Creating final display dataframe.
2025-02-26 12:08:03,312:INFO:Setup _display_container:                         Description          Value
0                        Session id            123
1                            Target          Price
2                       Target type     Regression
3               Original data shape    (13580, 21)
4            Transformed data shape    (13580, 13)
5       Transformed train set shape     (9506, 13)
6        Transformed test set shape     (4074, 13)
7                   Ignore features              4
8                  Ordinal features              3
9                  Numeric features              8
10                    Date features              1
11             Categorical features              6
12         Rows with missing values          54.4%
13                       Preprocess           True
14                  Imputation type      iterative
15  Iterative imputation iterations              5
16        Numeric iterative imputer       lightgbm
17    Categorical iterative imputer       lightgbm
18         Maximum one-hot encoding             25
19                  Encoding method           None
20              Polynomial features           True
21                Polynomial degree              2
22         Remove multicollinearity           True
23      Multicollinearity threshold           0.55
24                        Normalize           True
25                 Normalize method         zscore
26                Feature selection           True
27         Feature selection method        classic
28      Feature selection estimator             rf
29      Number of features selected            0.8
30                   Fold Generator          KFold
31                      Fold Number             10
32                         CPU Jobs             -1
33                          Use GPU          False
34                   Log Experiment  DagshubLogger
35                  Experiment Name  house_pricing
36                              USI           98f1
2025-02-26 12:08:03,380:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-26 12:08:03,382:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-26 12:08:03,446:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-26 12:08:03,447:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-26 12:08:03,448:INFO:Logging experiment in loggers
2025-02-26 12:09:15,849:INFO:SubProcess save_model() called ==================================
2025-02-26 12:09:15,961:INFO:Initializing save_model()
2025-02-26 12:09:15,961:INFO:save_model(model=Pipeline(memory=FastMemory(location=/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/joblib),
         steps=[('date_feature_extractor',
                 TransformerWrapper(include=['Date'],
                                    transformer=ExtractDateTimeFeatures())),
                ('iterative_imputer',
                 TransformerWrapper(transformer=IterativeImputer(cat_estimator=LGBMClassifier(n_jobs=-1,
                                                                                              random_state=123),
                                                                 cat_estimator_prepare_...
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('remove_multicollinearity',
                 TransformerWrapper(exclude=[],
                                    transformer=RemoveMulticollinearity(threshold=0.55))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('feature_selection',
                 TransformerWrapper(exclude=[],
                                    transformer=SelectFromModel(estimator=RandomForestRegressor(),
                                                                max_features=12,
                                                                threshold=-inf)))]), model_name=/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/tmpi6oxzprg/Transformation Pipeline, prep_pipe_=Pipeline(memory=FastMemory(location=/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/joblib),
         steps=[('date_feature_extractor',
                 TransformerWrapper(include=['Date'],
                                    transformer=ExtractDateTimeFeatures())),
                ('iterative_imputer',
                 TransformerWrapper(transformer=IterativeImputer(cat_estimator=LGBMClassifier(n_jobs=-1,
                                                                                              random_state=123),
                                                                 cat_estimator_prepare_...
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('remove_multicollinearity',
                 TransformerWrapper(exclude=[],
                                    transformer=RemoveMulticollinearity(threshold=0.55))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('feature_selection',
                 TransformerWrapper(exclude=[],
                                    transformer=SelectFromModel(estimator=RandomForestRegressor(),
                                                                max_features=12,
                                                                threshold=-inf)))]), verbose=False, use_case=MLUsecase.REGRESSION, kwargs={})
2025-02-26 12:09:15,961:INFO:Adding model into prep_pipe
2025-02-26 12:09:15,961:WARNING:Only Model saved as it was a pipeline.
2025-02-26 12:09:16,158:INFO:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/tmpi6oxzprg/Transformation Pipeline.pkl saved in current working directory
2025-02-26 12:09:16,201:INFO:Pipeline(memory=FastMemory(location=/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/joblib),
         steps=[('date_feature_extractor',
                 TransformerWrapper(include=['Date'],
                                    transformer=ExtractDateTimeFeatures())),
                ('iterative_imputer',
                 TransformerWrapper(transformer=IterativeImputer(cat_estimator=LGBMClassifier(n_jobs=-1,
                                                                                              random_state=123),
                                                                 cat_estimator_prepare_...
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('remove_multicollinearity',
                 TransformerWrapper(exclude=[],
                                    transformer=RemoveMulticollinearity(threshold=0.55))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('feature_selection',
                 TransformerWrapper(exclude=[],
                                    transformer=SelectFromModel(estimator=RandomForestRegressor(),
                                                                max_features=12,
                                                                threshold=-inf)))])
2025-02-26 12:09:16,201:INFO:save_model() successfully completed......................................
2025-02-26 12:09:16,294:INFO:SubProcess save_model() end ==================================
2025-02-26 12:09:28,565:INFO:setup() successfully completed in 4.85s...............
2025-02-26 12:09:28,882:INFO:Initializing compare_models()
2025-02-26 12:09:28,883:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x287f0a7a0>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x287f0a7a0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2025-02-26 12:09:28,883:INFO:Checking exceptions
2025-02-26 12:09:28,886:INFO:Preparing display monitor
2025-02-26 12:09:28,921:INFO:Initializing Linear Regression
2025-02-26 12:09:28,921:INFO:Total runtime is 7.430712381998698e-06 minutes
2025-02-26 12:09:28,923:INFO:SubProcess create_model() called ==================================
2025-02-26 12:09:28,924:INFO:Initializing create_model()
2025-02-26 12:09:28,924:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x287f0a7a0>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x282f3be20>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-26 12:09:28,924:INFO:Checking exceptions
2025-02-26 12:09:28,924:INFO:Importing libraries
2025-02-26 12:09:28,924:INFO:Copying training dataset
2025-02-26 12:09:28,933:INFO:Defining folds
2025-02-26 12:09:28,933:INFO:Declaring metric variables
2025-02-26 12:09:28,935:INFO:Importing untrained model
2025-02-26 12:09:28,937:INFO:Linear Regression Imported successfully
2025-02-26 12:09:28,940:INFO:Starting cross validation
2025-02-26 12:09:29,064:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-26 12:10:13,291:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:10:13,396:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:10:13,519:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:10:13,647:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:10:13,699:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:10:13,712:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:10:13,771:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:10:13,796:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:10:45,330:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:10:45,541:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:11:03,144:INFO:Calculating mean and std
2025-02-26 12:11:03,147:INFO:Creating metrics dataframe
2025-02-26 12:11:03,162:INFO:Uploading results into container
2025-02-26 12:11:03,162:INFO:Uploading model into container now
2025-02-26 12:11:03,163:INFO:_master_model_container: 1
2025-02-26 12:11:03,163:INFO:_display_container: 2
2025-02-26 12:11:03,163:INFO:LinearRegression(n_jobs=-1)
2025-02-26 12:11:03,163:INFO:create_model() successfully completed......................................
2025-02-26 12:11:03,292:INFO:SubProcess create_model() end ==================================
2025-02-26 12:11:03,292:INFO:Creating metrics dataframe
2025-02-26 12:11:03,296:INFO:Initializing Lasso Regression
2025-02-26 12:11:03,296:INFO:Total runtime is 1.5729221423467001 minutes
2025-02-26 12:11:03,298:INFO:SubProcess create_model() called ==================================
2025-02-26 12:11:03,298:INFO:Initializing create_model()
2025-02-26 12:11:03,298:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x287f0a7a0>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x282f3be20>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-26 12:11:03,298:INFO:Checking exceptions
2025-02-26 12:11:03,298:INFO:Importing libraries
2025-02-26 12:11:03,298:INFO:Copying training dataset
2025-02-26 12:11:03,307:INFO:Defining folds
2025-02-26 12:11:03,307:INFO:Declaring metric variables
2025-02-26 12:11:03,310:INFO:Importing untrained model
2025-02-26 12:11:03,312:INFO:Lasso Regression Imported successfully
2025-02-26 12:11:03,316:INFO:Starting cross validation
2025-02-26 12:11:03,466:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-26 12:11:43,613:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:11:43,875:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:11:43,990:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:11:44,066:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:11:44,146:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:11:44,363:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:11:44,691:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:11:44,758:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:12:04,413:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.829e+12, tolerance: 3.335e+11
  model = cd_fast.enet_coordinate_descent(

2025-02-26 12:12:04,839:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.795e+12, tolerance: 3.340e+11
  model = cd_fast.enet_coordinate_descent(

2025-02-26 12:12:04,843:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.076e+12, tolerance: 3.332e+11
  model = cd_fast.enet_coordinate_descent(

2025-02-26 12:12:05,047:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.356e+12, tolerance: 3.328e+11
  model = cd_fast.enet_coordinate_descent(

2025-02-26 12:12:05,107:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.696e+12, tolerance: 3.306e+11
  model = cd_fast.enet_coordinate_descent(

2025-02-26 12:12:05,430:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.001e+13, tolerance: 3.323e+11
  model = cd_fast.enet_coordinate_descent(

2025-02-26 12:12:05,873:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.236e+13, tolerance: 3.361e+11
  model = cd_fast.enet_coordinate_descent(

2025-02-26 12:12:05,972:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.665e+12, tolerance: 3.411e+11
  model = cd_fast.enet_coordinate_descent(

2025-02-26 12:12:16,307:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:12:16,810:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:12:33,968:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.994e+12, tolerance: 3.363e+11
  model = cd_fast.enet_coordinate_descent(

2025-02-26 12:12:34,212:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.074e+12, tolerance: 3.294e+11
  model = cd_fast.enet_coordinate_descent(

2025-02-26 12:12:34,321:INFO:Calculating mean and std
2025-02-26 12:12:34,322:INFO:Creating metrics dataframe
2025-02-26 12:12:34,335:INFO:Uploading results into container
2025-02-26 12:12:34,336:INFO:Uploading model into container now
2025-02-26 12:12:34,336:INFO:_master_model_container: 2
2025-02-26 12:12:34,336:INFO:_display_container: 2
2025-02-26 12:12:34,336:INFO:Lasso(random_state=123)
2025-02-26 12:12:34,336:INFO:create_model() successfully completed......................................
2025-02-26 12:12:34,424:INFO:SubProcess create_model() end ==================================
2025-02-26 12:12:34,425:INFO:Creating metrics dataframe
2025-02-26 12:12:34,428:INFO:Initializing Ridge Regression
2025-02-26 12:12:34,428:INFO:Total runtime is 3.0917957425117493 minutes
2025-02-26 12:12:34,430:INFO:SubProcess create_model() called ==================================
2025-02-26 12:12:34,430:INFO:Initializing create_model()
2025-02-26 12:12:34,430:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x287f0a7a0>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x282f3be20>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-26 12:12:34,430:INFO:Checking exceptions
2025-02-26 12:12:34,430:INFO:Importing libraries
2025-02-26 12:12:34,430:INFO:Copying training dataset
2025-02-26 12:12:34,438:INFO:Defining folds
2025-02-26 12:12:34,438:INFO:Declaring metric variables
2025-02-26 12:12:34,440:INFO:Importing untrained model
2025-02-26 12:12:34,442:INFO:Ridge Regression Imported successfully
2025-02-26 12:12:34,446:INFO:Starting cross validation
2025-02-26 12:12:34,565:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-26 12:13:17,470:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:13:17,806:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:13:17,827:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:13:18,196:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:13:18,221:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:13:18,449:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:13:18,510:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:13:18,578:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:13:51,813:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:13:52,090:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:14:09,472:INFO:Calculating mean and std
2025-02-26 12:14:09,474:INFO:Creating metrics dataframe
2025-02-26 12:14:09,487:INFO:Uploading results into container
2025-02-26 12:14:09,487:INFO:Uploading model into container now
2025-02-26 12:14:09,488:INFO:_master_model_container: 3
2025-02-26 12:14:09,488:INFO:_display_container: 2
2025-02-26 12:14:09,488:INFO:Ridge(random_state=123)
2025-02-26 12:14:09,488:INFO:create_model() successfully completed......................................
2025-02-26 12:14:09,620:INFO:SubProcess create_model() end ==================================
2025-02-26 12:14:09,621:INFO:Creating metrics dataframe
2025-02-26 12:14:09,625:INFO:Initializing Elastic Net
2025-02-26 12:14:09,625:INFO:Total runtime is 4.678405265013377 minutes
2025-02-26 12:14:09,627:INFO:SubProcess create_model() called ==================================
2025-02-26 12:14:09,627:INFO:Initializing create_model()
2025-02-26 12:14:09,627:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x287f0a7a0>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x282f3be20>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-26 12:14:09,627:INFO:Checking exceptions
2025-02-26 12:14:09,627:INFO:Importing libraries
2025-02-26 12:14:09,627:INFO:Copying training dataset
2025-02-26 12:14:09,634:INFO:Defining folds
2025-02-26 12:14:09,634:INFO:Declaring metric variables
2025-02-26 12:14:09,636:INFO:Importing untrained model
2025-02-26 12:14:09,638:INFO:Elastic Net Imported successfully
2025-02-26 12:14:09,642:INFO:Starting cross validation
2025-02-26 12:14:09,800:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-26 12:14:51,202:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:14:51,662:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:14:51,727:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:14:52,203:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:14:52,274:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:14:52,415:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:14:52,631:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:14:52,749:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:15:25,110:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:15:25,466:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:15:43,445:INFO:Calculating mean and std
2025-02-26 12:15:43,446:INFO:Creating metrics dataframe
2025-02-26 12:15:43,459:INFO:Uploading results into container
2025-02-26 12:15:43,460:INFO:Uploading model into container now
2025-02-26 12:15:43,460:INFO:_master_model_container: 4
2025-02-26 12:15:43,460:INFO:_display_container: 2
2025-02-26 12:15:43,460:INFO:ElasticNet(random_state=123)
2025-02-26 12:15:43,460:INFO:create_model() successfully completed......................................
2025-02-26 12:15:43,544:INFO:SubProcess create_model() end ==================================
2025-02-26 12:15:43,544:INFO:Creating metrics dataframe
2025-02-26 12:15:43,548:INFO:Initializing Least Angle Regression
2025-02-26 12:15:43,548:INFO:Total runtime is 6.243789378801981 minutes
2025-02-26 12:15:43,550:INFO:SubProcess create_model() called ==================================
2025-02-26 12:15:43,550:INFO:Initializing create_model()
2025-02-26 12:15:43,550:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x287f0a7a0>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x282f3be20>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-26 12:15:43,550:INFO:Checking exceptions
2025-02-26 12:15:43,550:INFO:Importing libraries
2025-02-26 12:15:43,550:INFO:Copying training dataset
2025-02-26 12:15:43,556:INFO:Defining folds
2025-02-26 12:15:43,556:INFO:Declaring metric variables
2025-02-26 12:15:43,558:INFO:Importing untrained model
2025-02-26 12:15:43,560:INFO:Least Angle Regression Imported successfully
2025-02-26 12:15:43,564:INFO:Starting cross validation
2025-02-26 12:15:43,683:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-26 12:16:25,833:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:16:26,406:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:16:26,453:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:16:26,481:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:16:26,772:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:16:26,819:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:16:27,094:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:16:27,430:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:16:58,440:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:16:58,575:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:17:15,992:INFO:Calculating mean and std
2025-02-26 12:17:15,994:INFO:Creating metrics dataframe
2025-02-26 12:17:16,005:INFO:Uploading results into container
2025-02-26 12:17:16,005:INFO:Uploading model into container now
2025-02-26 12:17:16,006:INFO:_master_model_container: 5
2025-02-26 12:17:16,006:INFO:_display_container: 2
2025-02-26 12:17:16,006:INFO:Lars(random_state=123)
2025-02-26 12:17:16,006:INFO:create_model() successfully completed......................................
2025-02-26 12:17:16,124:INFO:SubProcess create_model() end ==================================
2025-02-26 12:17:16,124:INFO:Creating metrics dataframe
2025-02-26 12:17:16,128:INFO:Initializing Lasso Least Angle Regression
2025-02-26 12:17:16,129:INFO:Total runtime is 7.78679891427358 minutes
2025-02-26 12:17:16,130:INFO:SubProcess create_model() called ==================================
2025-02-26 12:17:16,130:INFO:Initializing create_model()
2025-02-26 12:17:16,130:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x287f0a7a0>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x282f3be20>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-26 12:17:16,130:INFO:Checking exceptions
2025-02-26 12:17:16,131:INFO:Importing libraries
2025-02-26 12:17:16,131:INFO:Copying training dataset
2025-02-26 12:17:16,137:INFO:Defining folds
2025-02-26 12:17:16,137:INFO:Declaring metric variables
2025-02-26 12:17:16,139:INFO:Importing untrained model
2025-02-26 12:17:16,141:INFO:Lasso Least Angle Regression Imported successfully
2025-02-26 12:17:16,146:INFO:Starting cross validation
2025-02-26 12:17:16,268:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-26 12:17:56,826:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:17:57,764:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:17:57,861:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:17:58,058:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:17:58,267:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:17:58,288:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:17:58,289:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:17:58,343:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:18:31,402:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:18:31,686:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:18:49,497:INFO:Calculating mean and std
2025-02-26 12:18:49,499:INFO:Creating metrics dataframe
2025-02-26 12:18:49,511:INFO:Uploading results into container
2025-02-26 12:18:49,511:INFO:Uploading model into container now
2025-02-26 12:18:49,511:INFO:_master_model_container: 6
2025-02-26 12:18:49,511:INFO:_display_container: 2
2025-02-26 12:18:49,511:INFO:LassoLars(random_state=123)
2025-02-26 12:18:49,511:INFO:create_model() successfully completed......................................
2025-02-26 12:18:49,599:INFO:SubProcess create_model() end ==================================
2025-02-26 12:18:49,599:INFO:Creating metrics dataframe
2025-02-26 12:18:49,603:INFO:Initializing Orthogonal Matching Pursuit
2025-02-26 12:18:49,603:INFO:Total runtime is 9.344709912935892 minutes
2025-02-26 12:18:49,605:INFO:SubProcess create_model() called ==================================
2025-02-26 12:18:49,605:INFO:Initializing create_model()
2025-02-26 12:18:49,605:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x287f0a7a0>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x282f3be20>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-26 12:18:49,605:INFO:Checking exceptions
2025-02-26 12:18:49,605:INFO:Importing libraries
2025-02-26 12:18:49,605:INFO:Copying training dataset
2025-02-26 12:18:49,611:INFO:Defining folds
2025-02-26 12:18:49,611:INFO:Declaring metric variables
2025-02-26 12:18:49,613:INFO:Importing untrained model
2025-02-26 12:18:49,615:INFO:Orthogonal Matching Pursuit Imported successfully
2025-02-26 12:18:49,619:INFO:Starting cross validation
2025-02-26 12:18:49,740:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-26 12:19:30,799:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:19:30,955:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:19:30,963:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:19:30,977:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:19:31,190:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:19:31,204:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:19:31,320:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:19:31,560:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:20:04,203:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:20:04,320:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:20:21,915:INFO:Calculating mean and std
2025-02-26 12:20:21,921:INFO:Creating metrics dataframe
2025-02-26 12:20:21,946:INFO:Uploading results into container
2025-02-26 12:20:21,946:INFO:Uploading model into container now
2025-02-26 12:20:21,947:INFO:_master_model_container: 7
2025-02-26 12:20:21,947:INFO:_display_container: 2
2025-02-26 12:20:21,947:INFO:OrthogonalMatchingPursuit()
2025-02-26 12:20:21,947:INFO:create_model() successfully completed......................................
2025-02-26 12:20:22,065:INFO:SubProcess create_model() end ==================================
2025-02-26 12:20:22,065:INFO:Creating metrics dataframe
2025-02-26 12:20:22,069:INFO:Initializing Bayesian Ridge
2025-02-26 12:20:22,070:INFO:Total runtime is 10.885815779368082 minutes
2025-02-26 12:20:22,071:INFO:SubProcess create_model() called ==================================
2025-02-26 12:20:22,071:INFO:Initializing create_model()
2025-02-26 12:20:22,071:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x287f0a7a0>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x282f3be20>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-26 12:20:22,071:INFO:Checking exceptions
2025-02-26 12:20:22,071:INFO:Importing libraries
2025-02-26 12:20:22,071:INFO:Copying training dataset
2025-02-26 12:20:22,078:INFO:Defining folds
2025-02-26 12:20:22,079:INFO:Declaring metric variables
2025-02-26 12:20:22,080:INFO:Importing untrained model
2025-02-26 12:20:22,082:INFO:Bayesian Ridge Imported successfully
2025-02-26 12:20:22,086:INFO:Starting cross validation
2025-02-26 12:20:22,228:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-26 12:21:02,767:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:21:03,074:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:21:03,108:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:21:03,443:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:21:03,648:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:21:03,710:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:21:03,830:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:21:04,156:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:21:35,116:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:21:35,400:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:21:52,817:INFO:Calculating mean and std
2025-02-26 12:21:52,818:INFO:Creating metrics dataframe
2025-02-26 12:21:52,830:INFO:Uploading results into container
2025-02-26 12:21:52,830:INFO:Uploading model into container now
2025-02-26 12:21:52,831:INFO:_master_model_container: 8
2025-02-26 12:21:52,831:INFO:_display_container: 2
2025-02-26 12:21:52,831:INFO:BayesianRidge()
2025-02-26 12:21:52,831:INFO:create_model() successfully completed......................................
2025-02-26 12:21:52,915:INFO:SubProcess create_model() end ==================================
2025-02-26 12:21:52,915:INFO:Creating metrics dataframe
2025-02-26 12:21:52,919:INFO:Initializing Passive Aggressive Regressor
2025-02-26 12:21:52,919:INFO:Total runtime is 12.399979444344838 minutes
2025-02-26 12:21:52,921:INFO:SubProcess create_model() called ==================================
2025-02-26 12:21:52,921:INFO:Initializing create_model()
2025-02-26 12:21:52,921:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x287f0a7a0>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x282f3be20>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-26 12:21:52,921:INFO:Checking exceptions
2025-02-26 12:21:52,922:INFO:Importing libraries
2025-02-26 12:21:52,922:INFO:Copying training dataset
2025-02-26 12:21:52,928:INFO:Defining folds
2025-02-26 12:21:52,928:INFO:Declaring metric variables
2025-02-26 12:21:52,930:INFO:Importing untrained model
2025-02-26 12:21:52,932:INFO:Passive Aggressive Regressor Imported successfully
2025-02-26 12:21:52,955:INFO:Starting cross validation
2025-02-26 12:21:53,066:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-26 12:22:32,603:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:22:33,170:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:22:33,209:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:22:33,229:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:22:33,430:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:22:33,626:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:22:33,638:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:22:33,900:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:22:53,924:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py:1575: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
  warnings.warn(

2025-02-26 12:22:54,259:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py:1575: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
  warnings.warn(

2025-02-26 12:22:54,591:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py:1575: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
  warnings.warn(

2025-02-26 12:22:54,726:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py:1575: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
  warnings.warn(

2025-02-26 12:22:54,737:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py:1575: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
  warnings.warn(

2025-02-26 12:22:55,106:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py:1575: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
  warnings.warn(

2025-02-26 12:22:55,222:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py:1575: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
  warnings.warn(

2025-02-26 12:22:55,278:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py:1575: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
  warnings.warn(

2025-02-26 12:23:06,268:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:23:06,567:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:23:23,964:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py:1575: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
  warnings.warn(

2025-02-26 12:23:24,112:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py:1575: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
  warnings.warn(

2025-02-26 12:23:24,220:INFO:Calculating mean and std
2025-02-26 12:23:24,221:INFO:Creating metrics dataframe
2025-02-26 12:23:24,230:INFO:Uploading results into container
2025-02-26 12:23:24,231:INFO:Uploading model into container now
2025-02-26 12:23:24,231:INFO:_master_model_container: 9
2025-02-26 12:23:24,231:INFO:_display_container: 2
2025-02-26 12:23:24,231:INFO:PassiveAggressiveRegressor(random_state=123)
2025-02-26 12:23:24,232:INFO:create_model() successfully completed......................................
2025-02-26 12:23:24,318:INFO:SubProcess create_model() end ==================================
2025-02-26 12:23:24,318:INFO:Creating metrics dataframe
2025-02-26 12:23:24,323:INFO:Initializing Huber Regressor
2025-02-26 12:23:24,323:INFO:Total runtime is 13.923379762967427 minutes
2025-02-26 12:23:24,325:INFO:SubProcess create_model() called ==================================
2025-02-26 12:23:24,325:INFO:Initializing create_model()
2025-02-26 12:23:24,325:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x287f0a7a0>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x282f3be20>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-26 12:23:24,325:INFO:Checking exceptions
2025-02-26 12:23:24,325:INFO:Importing libraries
2025-02-26 12:23:24,325:INFO:Copying training dataset
2025-02-26 12:23:24,332:INFO:Defining folds
2025-02-26 12:23:24,332:INFO:Declaring metric variables
2025-02-26 12:23:24,335:INFO:Importing untrained model
2025-02-26 12:23:24,337:INFO:Huber Regressor Imported successfully
2025-02-26 12:23:24,341:INFO:Starting cross validation
2025-02-26 12:23:24,448:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-26 12:24:05,228:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:24:05,403:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:24:05,468:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:24:05,523:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:24:05,562:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:24:05,618:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:24:05,917:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:24:05,928:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:24:36,470:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:24:36,824:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:24:54,022:INFO:Calculating mean and std
2025-02-26 12:24:54,023:INFO:Creating metrics dataframe
2025-02-26 12:24:54,037:INFO:Uploading results into container
2025-02-26 12:24:54,037:INFO:Uploading model into container now
2025-02-26 12:24:54,038:INFO:_master_model_container: 10
2025-02-26 12:24:54,038:INFO:_display_container: 2
2025-02-26 12:24:54,038:INFO:HuberRegressor()
2025-02-26 12:24:54,038:INFO:create_model() successfully completed......................................
2025-02-26 12:24:54,163:INFO:SubProcess create_model() end ==================================
2025-02-26 12:24:54,163:INFO:Creating metrics dataframe
2025-02-26 12:24:54,168:INFO:Initializing K Neighbors Regressor
2025-02-26 12:24:54,168:INFO:Total runtime is 15.420795027414957 minutes
2025-02-26 12:24:54,170:INFO:SubProcess create_model() called ==================================
2025-02-26 12:24:54,170:INFO:Initializing create_model()
2025-02-26 12:24:54,170:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x287f0a7a0>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x282f3be20>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-26 12:24:54,170:INFO:Checking exceptions
2025-02-26 12:24:54,170:INFO:Importing libraries
2025-02-26 12:24:54,171:INFO:Copying training dataset
2025-02-26 12:24:54,177:INFO:Defining folds
2025-02-26 12:24:54,177:INFO:Declaring metric variables
2025-02-26 12:24:54,179:INFO:Importing untrained model
2025-02-26 12:24:54,181:INFO:K Neighbors Regressor Imported successfully
2025-02-26 12:24:54,184:INFO:Starting cross validation
2025-02-26 12:24:54,318:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-26 12:25:33,726:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:25:34,242:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:25:34,299:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:25:34,417:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:25:34,464:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:25:34,479:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:25:34,725:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:25:34,866:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:26:04,986:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:26:05,201:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:26:22,585:INFO:Calculating mean and std
2025-02-26 12:26:22,587:INFO:Creating metrics dataframe
2025-02-26 12:26:22,594:INFO:Uploading results into container
2025-02-26 12:26:22,595:INFO:Uploading model into container now
2025-02-26 12:26:22,595:INFO:_master_model_container: 11
2025-02-26 12:26:22,595:INFO:_display_container: 2
2025-02-26 12:26:22,595:INFO:KNeighborsRegressor(n_jobs=-1)
2025-02-26 12:26:22,595:INFO:create_model() successfully completed......................................
2025-02-26 12:26:22,678:INFO:SubProcess create_model() end ==================================
2025-02-26 12:26:22,678:INFO:Creating metrics dataframe
2025-02-26 12:26:22,682:INFO:Initializing Decision Tree Regressor
2025-02-26 12:26:22,682:INFO:Total runtime is 16.8960311293602 minutes
2025-02-26 12:26:22,684:INFO:SubProcess create_model() called ==================================
2025-02-26 12:26:22,684:INFO:Initializing create_model()
2025-02-26 12:26:22,684:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x287f0a7a0>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x282f3be20>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-26 12:26:22,684:INFO:Checking exceptions
2025-02-26 12:26:22,684:INFO:Importing libraries
2025-02-26 12:26:22,684:INFO:Copying training dataset
2025-02-26 12:26:22,690:INFO:Defining folds
2025-02-26 12:26:22,690:INFO:Declaring metric variables
2025-02-26 12:26:22,692:INFO:Importing untrained model
2025-02-26 12:26:22,694:INFO:Decision Tree Regressor Imported successfully
2025-02-26 12:26:22,698:INFO:Starting cross validation
2025-02-26 12:26:22,802:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-26 12:27:02,499:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:27:02,612:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:27:02,712:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:27:02,815:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:27:03,016:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:27:03,263:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:27:03,523:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:27:03,608:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:27:34,464:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:27:34,482:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:27:52,347:INFO:Calculating mean and std
2025-02-26 12:27:52,348:INFO:Creating metrics dataframe
2025-02-26 12:27:52,362:INFO:Uploading results into container
2025-02-26 12:27:52,363:INFO:Uploading model into container now
2025-02-26 12:27:52,363:INFO:_master_model_container: 12
2025-02-26 12:27:52,363:INFO:_display_container: 2
2025-02-26 12:27:52,363:INFO:DecisionTreeRegressor(random_state=123)
2025-02-26 12:27:52,363:INFO:create_model() successfully completed......................................
2025-02-26 12:27:52,453:INFO:SubProcess create_model() end ==================================
2025-02-26 12:27:52,453:INFO:Creating metrics dataframe
2025-02-26 12:27:52,459:INFO:Initializing Random Forest Regressor
2025-02-26 12:27:52,459:INFO:Total runtime is 18.392302044232686 minutes
2025-02-26 12:27:52,461:INFO:SubProcess create_model() called ==================================
2025-02-26 12:27:52,461:INFO:Initializing create_model()
2025-02-26 12:27:52,461:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x287f0a7a0>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x282f3be20>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-26 12:27:52,461:INFO:Checking exceptions
2025-02-26 12:27:52,461:INFO:Importing libraries
2025-02-26 12:27:52,461:INFO:Copying training dataset
2025-02-26 12:27:52,468:INFO:Defining folds
2025-02-26 12:27:52,468:INFO:Declaring metric variables
2025-02-26 12:27:52,470:INFO:Importing untrained model
2025-02-26 12:27:52,473:INFO:Random Forest Regressor Imported successfully
2025-02-26 12:27:52,476:INFO:Starting cross validation
2025-02-26 12:27:52,592:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-26 12:28:34,054:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:28:34,452:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:28:35,128:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:28:35,256:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:28:35,277:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:28:35,303:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:28:35,438:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:28:35,546:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:29:15,371:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:29:15,822:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:29:36,514:INFO:Calculating mean and std
2025-02-26 12:29:36,516:INFO:Creating metrics dataframe
2025-02-26 12:29:36,529:INFO:Uploading results into container
2025-02-26 12:29:36,530:INFO:Uploading model into container now
2025-02-26 12:29:36,530:INFO:_master_model_container: 13
2025-02-26 12:29:36,530:INFO:_display_container: 2
2025-02-26 12:29:36,531:INFO:RandomForestRegressor(n_jobs=-1, random_state=123)
2025-02-26 12:29:36,531:INFO:create_model() successfully completed......................................
2025-02-26 12:29:36,648:INFO:SubProcess create_model() end ==================================
2025-02-26 12:29:36,649:INFO:Creating metrics dataframe
2025-02-26 12:29:36,655:INFO:Initializing Extra Trees Regressor
2025-02-26 12:29:36,655:INFO:Total runtime is 20.12890229622523 minutes
2025-02-26 12:29:36,657:INFO:SubProcess create_model() called ==================================
2025-02-26 12:29:36,657:INFO:Initializing create_model()
2025-02-26 12:29:36,657:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x287f0a7a0>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x282f3be20>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-26 12:29:36,657:INFO:Checking exceptions
2025-02-26 12:29:36,657:INFO:Importing libraries
2025-02-26 12:29:36,657:INFO:Copying training dataset
2025-02-26 12:29:36,664:INFO:Defining folds
2025-02-26 12:29:36,664:INFO:Declaring metric variables
2025-02-26 12:29:36,666:INFO:Importing untrained model
2025-02-26 12:29:36,668:INFO:Extra Trees Regressor Imported successfully
2025-02-26 12:29:36,672:INFO:Starting cross validation
2025-02-26 12:29:36,808:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-26 12:30:16,407:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:30:16,984:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:30:17,313:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:30:17,362:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:30:17,613:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:30:17,784:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:30:17,847:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:30:17,928:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:30:50,605:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:30:50,893:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:31:08,814:INFO:Calculating mean and std
2025-02-26 12:31:08,816:INFO:Creating metrics dataframe
2025-02-26 12:31:08,829:INFO:Uploading results into container
2025-02-26 12:31:08,829:INFO:Uploading model into container now
2025-02-26 12:31:08,830:INFO:_master_model_container: 14
2025-02-26 12:31:08,830:INFO:_display_container: 2
2025-02-26 12:31:08,830:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=123)
2025-02-26 12:31:08,830:INFO:create_model() successfully completed......................................
2025-02-26 12:31:08,919:INFO:SubProcess create_model() end ==================================
2025-02-26 12:31:08,919:INFO:Creating metrics dataframe
2025-02-26 12:31:08,925:INFO:Initializing AdaBoost Regressor
2025-02-26 12:31:08,925:INFO:Total runtime is 21.66673534711202 minutes
2025-02-26 12:31:08,927:INFO:SubProcess create_model() called ==================================
2025-02-26 12:31:08,927:INFO:Initializing create_model()
2025-02-26 12:31:08,927:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x287f0a7a0>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x282f3be20>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-26 12:31:08,927:INFO:Checking exceptions
2025-02-26 12:31:08,927:INFO:Importing libraries
2025-02-26 12:31:08,927:INFO:Copying training dataset
2025-02-26 12:31:08,934:INFO:Defining folds
2025-02-26 12:31:08,934:INFO:Declaring metric variables
2025-02-26 12:31:08,936:INFO:Importing untrained model
2025-02-26 12:31:08,938:INFO:AdaBoost Regressor Imported successfully
2025-02-26 12:31:08,942:INFO:Starting cross validation
2025-02-26 12:31:09,117:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-26 12:31:48,672:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:31:48,727:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:31:49,080:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:31:49,176:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:31:49,306:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:31:49,561:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:31:50,071:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:31:50,723:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:32:20,893:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:32:20,988:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:32:39,124:INFO:Calculating mean and std
2025-02-26 12:32:39,126:INFO:Creating metrics dataframe
2025-02-26 12:32:39,141:INFO:Uploading results into container
2025-02-26 12:32:39,141:INFO:Uploading model into container now
2025-02-26 12:32:39,141:INFO:_master_model_container: 15
2025-02-26 12:32:39,141:INFO:_display_container: 2
2025-02-26 12:32:39,142:INFO:AdaBoostRegressor(random_state=123)
2025-02-26 12:32:39,142:INFO:create_model() successfully completed......................................
2025-02-26 12:32:39,231:INFO:SubProcess create_model() end ==================================
2025-02-26 12:32:39,231:INFO:Creating metrics dataframe
2025-02-26 12:32:39,237:INFO:Initializing Gradient Boosting Regressor
2025-02-26 12:32:39,237:INFO:Total runtime is 23.171936360994977 minutes
2025-02-26 12:32:39,238:INFO:SubProcess create_model() called ==================================
2025-02-26 12:32:39,238:INFO:Initializing create_model()
2025-02-26 12:32:39,238:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x287f0a7a0>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x282f3be20>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-26 12:32:39,239:INFO:Checking exceptions
2025-02-26 12:32:39,239:INFO:Importing libraries
2025-02-26 12:32:39,239:INFO:Copying training dataset
2025-02-26 12:32:39,245:INFO:Defining folds
2025-02-26 12:32:39,245:INFO:Declaring metric variables
2025-02-26 12:32:39,247:INFO:Importing untrained model
2025-02-26 12:32:39,249:INFO:Gradient Boosting Regressor Imported successfully
2025-02-26 12:32:39,252:INFO:Starting cross validation
2025-02-26 12:32:39,364:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-26 12:33:18,762:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:33:19,037:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:33:19,192:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:33:19,253:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:33:19,409:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:33:19,568:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:33:19,820:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:33:19,936:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:33:53,786:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:33:53,975:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:34:14,081:INFO:Calculating mean and std
2025-02-26 12:34:14,082:INFO:Creating metrics dataframe
2025-02-26 12:34:14,095:INFO:Uploading results into container
2025-02-26 12:34:14,095:INFO:Uploading model into container now
2025-02-26 12:34:14,096:INFO:_master_model_container: 16
2025-02-26 12:34:14,096:INFO:_display_container: 2
2025-02-26 12:34:14,096:INFO:GradientBoostingRegressor(random_state=123)
2025-02-26 12:34:14,096:INFO:create_model() successfully completed......................................
2025-02-26 12:34:14,190:INFO:SubProcess create_model() end ==================================
2025-02-26 12:34:14,190:INFO:Creating metrics dataframe
2025-02-26 12:34:14,195:INFO:Initializing Extreme Gradient Boosting
2025-02-26 12:34:14,196:INFO:Total runtime is 24.754582178592685 minutes
2025-02-26 12:34:14,197:INFO:SubProcess create_model() called ==================================
2025-02-26 12:34:14,197:INFO:Initializing create_model()
2025-02-26 12:34:14,197:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x287f0a7a0>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x282f3be20>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-26 12:34:14,197:INFO:Checking exceptions
2025-02-26 12:34:14,197:INFO:Importing libraries
2025-02-26 12:34:14,198:INFO:Copying training dataset
2025-02-26 12:34:14,203:INFO:Defining folds
2025-02-26 12:34:14,204:INFO:Declaring metric variables
2025-02-26 12:34:14,205:INFO:Importing untrained model
2025-02-26 12:34:14,207:INFO:Extreme Gradient Boosting Imported successfully
2025-02-26 12:34:14,210:INFO:Starting cross validation
2025-02-26 12:34:14,322:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-26 12:34:53,736:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:34:53,957:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:34:54,150:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:34:54,318:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:34:54,319:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:34:54,342:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:34:54,605:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:34:54,649:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:35:27,540:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:35:27,577:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:35:45,476:INFO:Calculating mean and std
2025-02-26 12:35:45,478:INFO:Creating metrics dataframe
2025-02-26 12:35:45,486:INFO:Uploading results into container
2025-02-26 12:35:45,486:INFO:Uploading model into container now
2025-02-26 12:35:45,487:INFO:_master_model_container: 17
2025-02-26 12:35:45,487:INFO:_display_container: 2
2025-02-26 12:35:45,487:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device='cpu', early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, random_state=123, ...)
2025-02-26 12:35:45,487:INFO:create_model() successfully completed......................................
2025-02-26 12:35:45,572:INFO:SubProcess create_model() end ==================================
2025-02-26 12:35:45,572:INFO:Creating metrics dataframe
2025-02-26 12:35:45,578:INFO:Initializing Light Gradient Boosting Machine
2025-02-26 12:35:45,578:INFO:Total runtime is 26.27762131293615 minutes
2025-02-26 12:35:45,579:INFO:SubProcess create_model() called ==================================
2025-02-26 12:35:45,580:INFO:Initializing create_model()
2025-02-26 12:35:45,580:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x287f0a7a0>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x282f3be20>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-26 12:35:45,580:INFO:Checking exceptions
2025-02-26 12:35:45,580:INFO:Importing libraries
2025-02-26 12:35:45,580:INFO:Copying training dataset
2025-02-26 12:35:45,586:INFO:Defining folds
2025-02-26 12:35:45,586:INFO:Declaring metric variables
2025-02-26 12:35:45,588:INFO:Importing untrained model
2025-02-26 12:35:45,590:INFO:Light Gradient Boosting Machine Imported successfully
2025-02-26 12:35:45,595:INFO:Starting cross validation
2025-02-26 12:35:45,711:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-26 12:36:25,671:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:36:26,181:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:36:26,184:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:36:26,232:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:36:26,275:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:36:26,333:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:36:26,529:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:36:26,788:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:36:59,217:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:36:59,421:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:37:17,442:INFO:Calculating mean and std
2025-02-26 12:37:17,443:INFO:Creating metrics dataframe
2025-02-26 12:37:17,461:INFO:Uploading results into container
2025-02-26 12:37:17,461:INFO:Uploading model into container now
2025-02-26 12:37:17,462:INFO:_master_model_container: 18
2025-02-26 12:37:17,462:INFO:_display_container: 2
2025-02-26 12:37:17,462:INFO:LGBMRegressor(n_jobs=-1, random_state=123)
2025-02-26 12:37:17,462:INFO:create_model() successfully completed......................................
2025-02-26 12:37:17,574:INFO:SubProcess create_model() end ==================================
2025-02-26 12:37:17,575:INFO:Creating metrics dataframe
2025-02-26 12:37:17,581:INFO:Initializing CatBoost Regressor
2025-02-26 12:37:17,581:INFO:Total runtime is 27.811001678307857 minutes
2025-02-26 12:37:17,582:INFO:SubProcess create_model() called ==================================
2025-02-26 12:37:17,583:INFO:Initializing create_model()
2025-02-26 12:37:17,583:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x287f0a7a0>, estimator=catboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x282f3be20>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-26 12:37:17,583:INFO:Checking exceptions
2025-02-26 12:37:17,583:INFO:Importing libraries
2025-02-26 12:37:17,583:INFO:Copying training dataset
2025-02-26 12:37:17,588:INFO:Defining folds
2025-02-26 12:37:17,588:INFO:Declaring metric variables
2025-02-26 12:37:17,590:INFO:Importing untrained model
2025-02-26 12:37:17,592:INFO:CatBoost Regressor Imported successfully
2025-02-26 12:37:17,596:INFO:Starting cross validation
2025-02-26 12:37:17,751:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-26 12:37:57,154:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:37:58,348:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:37:58,386:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:37:58,439:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:37:58,671:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:37:58,713:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:37:58,900:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:37:59,050:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:38:32,797:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:38:33,001:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:38:52,260:INFO:Calculating mean and std
2025-02-26 12:38:52,261:INFO:Creating metrics dataframe
2025-02-26 12:38:52,274:INFO:Uploading results into container
2025-02-26 12:38:52,275:INFO:Uploading model into container now
2025-02-26 12:38:52,275:INFO:_master_model_container: 19
2025-02-26 12:38:52,275:INFO:_display_container: 2
2025-02-26 12:38:52,275:INFO:<catboost.core.CatBoostRegressor object at 0x28301a3b0>
2025-02-26 12:38:52,275:INFO:create_model() successfully completed......................................
2025-02-26 12:38:52,361:INFO:SubProcess create_model() end ==================================
2025-02-26 12:38:52,361:INFO:Creating metrics dataframe
2025-02-26 12:38:52,368:INFO:Initializing Dummy Regressor
2025-02-26 12:38:52,368:INFO:Total runtime is 29.39078469673793 minutes
2025-02-26 12:38:52,370:INFO:SubProcess create_model() called ==================================
2025-02-26 12:38:52,370:INFO:Initializing create_model()
2025-02-26 12:38:52,370:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x287f0a7a0>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x282f3be20>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-26 12:38:52,370:INFO:Checking exceptions
2025-02-26 12:38:52,370:INFO:Importing libraries
2025-02-26 12:38:52,370:INFO:Copying training dataset
2025-02-26 12:38:52,376:INFO:Defining folds
2025-02-26 12:38:52,376:INFO:Declaring metric variables
2025-02-26 12:38:52,378:INFO:Importing untrained model
2025-02-26 12:38:52,380:INFO:Dummy Regressor Imported successfully
2025-02-26 12:38:52,384:INFO:Starting cross validation
2025-02-26 12:38:52,497:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-26 12:39:31,492:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:39:31,814:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:39:31,870:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:39:31,930:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:39:32,259:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:39:32,593:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:39:32,808:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:39:33,276:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:40:03,149:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:40:03,389:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:40:20,793:INFO:Calculating mean and std
2025-02-26 12:40:20,794:INFO:Creating metrics dataframe
2025-02-26 12:40:20,805:INFO:Uploading results into container
2025-02-26 12:40:20,806:INFO:Uploading model into container now
2025-02-26 12:40:20,806:INFO:_master_model_container: 20
2025-02-26 12:40:20,806:INFO:_display_container: 2
2025-02-26 12:40:20,806:INFO:DummyRegressor()
2025-02-26 12:40:20,806:INFO:create_model() successfully completed......................................
2025-02-26 12:40:20,897:INFO:SubProcess create_model() end ==================================
2025-02-26 12:40:20,897:INFO:Creating metrics dataframe
2025-02-26 12:40:20,907:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/pycaret_experiment/supervised_experiment.py:323: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  master_display_.apply(

2025-02-26 12:40:20,911:INFO:Initializing create_model()
2025-02-26 12:40:20,911:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x287f0a7a0>, estimator=<catboost.core.CatBoostRegressor object at 0x28301a3b0>, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-26 12:40:20,911:INFO:Checking exceptions
2025-02-26 12:40:20,912:INFO:Importing libraries
2025-02-26 12:40:20,912:INFO:Copying training dataset
2025-02-26 12:40:20,919:INFO:Defining folds
2025-02-26 12:40:20,919:INFO:Declaring metric variables
2025-02-26 12:40:20,919:INFO:Importing untrained model
2025-02-26 12:40:20,919:INFO:Declaring custom model
2025-02-26 12:40:20,919:INFO:CatBoost Regressor Imported successfully
2025-02-26 12:40:21,034:INFO:Cross validation set to False
2025-02-26 12:40:21,034:INFO:Fitting Model
2025-02-26 12:40:21,065:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:40:21,065:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:40:21,068:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000277 seconds.
2025-02-26 12:40:21,068:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:40:21,068:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:40:21,068:INFO:[LightGBM] [Info] Total Bins 1877
2025-02-26 12:40:21,068:INFO:[LightGBM] [Info] Number of data points in the train set: 9467, number of used features: 17
2025-02-26 12:40:21,069:INFO:[LightGBM] [Info] Start training from score 1.610964
2025-02-26 12:40:21,582:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:40:21,582:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:40:21,585:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000504 seconds.
2025-02-26 12:40:21,585:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:40:21,585:INFO:[LightGBM] [Info] Total Bins 1810
2025-02-26 12:40:21,585:INFO:[LightGBM] [Info] Number of data points in the train set: 8543, number of used features: 17
2025-02-26 12:40:21,586:INFO:[LightGBM] [Info] Start training from score 14.875219
2025-02-26 12:40:21,948:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:40:21,948:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:40:21,950:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000417 seconds.
2025-02-26 12:40:21,950:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:40:21,950:INFO:[LightGBM] [Info] Total Bins 1813
2025-02-26 12:40:21,950:INFO:[LightGBM] [Info] Number of data points in the train set: 5729, number of used features: 17
2025-02-26 12:40:21,951:INFO:[LightGBM] [Info] Start training from score 1965.296038
2025-02-26 12:40:22,357:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:40:22,357:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:40:22,359:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000468 seconds.
2025-02-26 12:40:22,359:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:40:22,359:INFO:[LightGBM] [Info] Total Bins 2245
2025-02-26 12:40:22,360:INFO:[LightGBM] [Info] Number of data points in the train set: 9467, number of used features: 17
2025-02-26 12:40:22,360:INFO:[LightGBM] [Info] Start training from score 1.610964
2025-02-26 12:40:22,917:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:40:22,917:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:40:22,920:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000225 seconds.
2025-02-26 12:40:22,920:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:40:22,920:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:40:22,920:INFO:[LightGBM] [Info] Total Bins 1962
2025-02-26 12:40:22,920:INFO:[LightGBM] [Info] Number of data points in the train set: 8543, number of used features: 17
2025-02-26 12:40:22,920:INFO:[LightGBM] [Info] Start training from score 14.875219
2025-02-26 12:40:23,333:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:40:23,333:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:40:23,335:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000363 seconds.
2025-02-26 12:40:23,335:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:40:23,335:INFO:[LightGBM] [Info] Total Bins 1812
2025-02-26 12:40:23,335:INFO:[LightGBM] [Info] Number of data points in the train set: 5729, number of used features: 17
2025-02-26 12:40:23,335:INFO:[LightGBM] [Info] Start training from score 1965.296038
2025-02-26 12:40:23,711:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:40:23,711:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:40:23,714:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000263 seconds.
2025-02-26 12:40:23,714:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:40:23,714:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:40:23,714:INFO:[LightGBM] [Info] Total Bins 2244
2025-02-26 12:40:23,715:INFO:[LightGBM] [Info] Number of data points in the train set: 9467, number of used features: 17
2025-02-26 12:40:23,715:INFO:[LightGBM] [Info] Start training from score 1.610964
2025-02-26 12:40:24,148:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:40:24,148:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:40:24,150:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000498 seconds.
2025-02-26 12:40:24,150:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:40:24,150:INFO:[LightGBM] [Info] Total Bins 1962
2025-02-26 12:40:24,150:INFO:[LightGBM] [Info] Number of data points in the train set: 8543, number of used features: 17
2025-02-26 12:40:24,151:INFO:[LightGBM] [Info] Start training from score 14.875219
2025-02-26 12:40:24,654:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:40:24,654:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:40:24,656:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000363 seconds.
2025-02-26 12:40:24,656:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:40:24,656:INFO:[LightGBM] [Info] Total Bins 1813
2025-02-26 12:40:24,656:INFO:[LightGBM] [Info] Number of data points in the train set: 5729, number of used features: 17
2025-02-26 12:40:24,656:INFO:[LightGBM] [Info] Start training from score 1965.296038
2025-02-26 12:40:25,175:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:40:25,175:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:40:25,178:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000281 seconds.
2025-02-26 12:40:25,178:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:40:25,178:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:40:25,178:INFO:[LightGBM] [Info] Total Bins 2243
2025-02-26 12:40:25,178:INFO:[LightGBM] [Info] Number of data points in the train set: 9467, number of used features: 17
2025-02-26 12:40:25,179:INFO:[LightGBM] [Info] Start training from score 1.610964
2025-02-26 12:40:25,809:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:40:25,809:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:40:25,811:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000438 seconds.
2025-02-26 12:40:25,811:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:40:25,811:INFO:[LightGBM] [Info] Total Bins 1961
2025-02-26 12:40:25,811:INFO:[LightGBM] [Info] Number of data points in the train set: 8543, number of used features: 17
2025-02-26 12:40:25,812:INFO:[LightGBM] [Info] Start training from score 14.875219
2025-02-26 12:40:26,164:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:40:26,164:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:40:26,167:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000382 seconds.
2025-02-26 12:40:26,167:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:40:26,167:INFO:[LightGBM] [Info] Total Bins 1813
2025-02-26 12:40:26,167:INFO:[LightGBM] [Info] Number of data points in the train set: 5729, number of used features: 17
2025-02-26 12:40:26,167:INFO:[LightGBM] [Info] Start training from score 1965.296038
2025-02-26 12:40:26,522:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:40:26,522:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:40:26,525:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000528 seconds.
2025-02-26 12:40:26,525:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:40:26,525:INFO:[LightGBM] [Info] Total Bins 2244
2025-02-26 12:40:26,525:INFO:[LightGBM] [Info] Number of data points in the train set: 9467, number of used features: 17
2025-02-26 12:40:26,526:INFO:[LightGBM] [Info] Start training from score 1.610964
2025-02-26 12:40:26,925:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:40:26,925:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:40:26,928:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000279 seconds.
2025-02-26 12:40:26,928:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:40:26,928:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:40:26,928:INFO:[LightGBM] [Info] Total Bins 1962
2025-02-26 12:40:26,928:INFO:[LightGBM] [Info] Number of data points in the train set: 8543, number of used features: 17
2025-02-26 12:40:26,928:INFO:[LightGBM] [Info] Start training from score 14.875219
2025-02-26 12:40:27,405:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:40:27,405:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:40:27,406:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000395 seconds.
2025-02-26 12:40:27,406:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:40:27,407:INFO:[LightGBM] [Info] Total Bins 1813
2025-02-26 12:40:27,407:INFO:[LightGBM] [Info] Number of data points in the train set: 5729, number of used features: 17
2025-02-26 12:40:27,407:INFO:[LightGBM] [Info] Start training from score 1965.296038
2025-02-26 12:40:27,849:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:40:49,067:INFO:<catboost.core.CatBoostRegressor object at 0x295290be0>
2025-02-26 12:40:49,067:INFO:create_model() successfully completed......................................
2025-02-26 12:40:49,152:INFO:Creating Dashboard logs
2025-02-26 12:40:49,154:INFO:Model: CatBoost Regressor
2025-02-26 12:40:49,915:INFO:Logged params: {'nan_mode': 'Min', 'eval_metric': 'RMSE', 'iterations': 1000, 'sampling_frequency': 'PerTree', 'leaf_estimation_method': 'Newton', 'grow_policy': 'SymmetricTree', 'penalties_coefficient': 1, 'boosting_type': 'Plain', 'model_shrink_mode': 'Constant', 'feature_border_type': 'GreedyLogSum', 'bayesian_matrix_reg': 0.10000000149011612, 'eval_fraction': 0, 'force_unit_auto_pair_weights': False, 'l2_leaf_reg': 3, 'random_strength': 1, 'rsm': 1, 'boost_from_average': True, 'model_size_reg': 0.5, 'pool_metainfo_options': {'tags': {}}, 'subsample': 0.800000011920929, 'use_best_model': False, 'random_seed': 123, 'depth': 6, 'posterior_sampling': False, 'border_count': 254, 'classes_count': 0, 'auto_class_weights': 'None', 'sparse_features_conflict_fraction': 0, 'leaf_estimation_backtracking': 'AnyImprovement', 'best_model_min_trees': 1, 'model_shrink_rate': 0, 'min_data_in_leaf': 1, 'loss_function': 'RMSE', 'learning_rate': 0.05843900144100189, 'score_function': 'Cosine', 'task_type': 'CPU', 'leaf_estimation_iterations': 1, 'bootstrap_type': 'MVS', 'max_leaves': 64}
2025-02-26 12:40:57,288:INFO:Initializing predict_model()
2025-02-26 12:40:57,288:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x287f0a7a0>, estimator=<catboost.core.CatBoostRegressor object at 0x295290be0>, probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=False, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x287e0beb0>)
2025-02-26 12:40:57,288:INFO:Checking exceptions
2025-02-26 12:40:57,289:INFO:Preloading libraries
2025-02-26 12:40:57,610:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.
  warnings.warn(

2025-02-26 12:41:17,986:INFO:Creating Dashboard logs
2025-02-26 12:41:17,989:INFO:Model: Light Gradient Boosting Machine
2025-02-26 12:41:18,656:INFO:Logged params: {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'num_leaves': 31, 'objective': None, 'random_state': 123, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0}
2025-02-26 12:41:45,524:INFO:Creating Dashboard logs
2025-02-26 12:41:45,527:INFO:Model: Extra Trees Regressor
2025-02-26 12:41:46,541:INFO:Logged params: {'bootstrap': False, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False}
2025-02-26 12:42:17,070:INFO:Creating Dashboard logs
2025-02-26 12:42:17,073:INFO:Model: Extreme Gradient Boosting
2025-02-26 12:42:17,804:INFO:Logged params: {'objective': 'reg:squarederror', 'base_score': None, 'booster': 'gbtree', 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': None, 'device': 'cpu', 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': None, 'feature_types': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': None, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': None, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': None, 'n_jobs': -1, 'num_parallel_tree': None, 'random_state': 123, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': None, 'tree_method': 'auto', 'validate_parameters': None, 'verbosity': 0}
2025-02-26 12:42:45,062:INFO:Creating Dashboard logs
2025-02-26 12:42:45,065:INFO:Model: Random Forest Regressor
2025-02-26 12:42:46,037:INFO:Logged params: {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False}
2025-02-26 12:43:07,760:INFO:Creating Dashboard logs
2025-02-26 12:43:07,763:INFO:Model: Gradient Boosting Regressor
2025-02-26 12:43:08,564:INFO:Logged params: {'alpha': 0.9, 'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.1, 'loss': 'squared_error', 'max_depth': 3, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_iter_no_change': None, 'random_state': 123, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}
2025-02-26 12:43:31,006:INFO:Creating Dashboard logs
2025-02-26 12:43:31,009:INFO:Model: K Neighbors Regressor
2025-02-26 12:43:31,809:INFO:Logged params: {'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': -1, 'n_neighbors': 5, 'p': 2, 'weights': 'uniform'}
2025-02-26 12:43:54,351:INFO:Creating Dashboard logs
2025-02-26 12:43:54,355:INFO:Model: Huber Regressor
2025-02-26 12:43:55,057:INFO:Logged params: {'alpha': 0.0001, 'epsilon': 1.35, 'fit_intercept': True, 'max_iter': 100, 'tol': 1e-05, 'warm_start': False}
2025-02-26 12:44:16,987:INFO:Creating Dashboard logs
2025-02-26 12:44:16,990:INFO:Model: Bayesian Ridge
2025-02-26 12:44:17,792:INFO:Logged params: {'alpha_1': 1e-06, 'alpha_2': 1e-06, 'alpha_init': None, 'compute_score': False, 'copy_X': True, 'fit_intercept': True, 'lambda_1': 1e-06, 'lambda_2': 1e-06, 'lambda_init': None, 'max_iter': None, 'n_iter': 'deprecated', 'tol': 0.001, 'verbose': False}
2025-02-26 12:44:40,021:INFO:Creating Dashboard logs
2025-02-26 12:44:40,024:INFO:Model: Linear Regression
2025-02-26 12:44:40,930:INFO:Logged params: {'copy_X': True, 'fit_intercept': True, 'n_jobs': -1, 'positive': False}
2025-02-26 12:45:01,526:INFO:Creating Dashboard logs
2025-02-26 12:45:01,528:INFO:Model: Lasso Regression
2025-02-26 12:45:02,333:INFO:Logged params: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': 1000, 'positive': False, 'precompute': False, 'random_state': 123, 'selection': 'cyclic', 'tol': 0.0001, 'warm_start': False}
2025-02-26 12:45:24,366:INFO:Creating Dashboard logs
2025-02-26 12:45:24,368:INFO:Model: Ridge Regression
2025-02-26 12:45:25,168:INFO:Logged params: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'positive': False, 'random_state': 123, 'solver': 'auto', 'tol': 0.0001}
2025-02-26 12:45:48,852:INFO:Creating Dashboard logs
2025-02-26 12:45:48,854:INFO:Model: Least Angle Regression
2025-02-26 12:45:49,581:INFO:Logged params: {'copy_X': True, 'eps': 2.220446049250313e-16, 'fit_intercept': True, 'fit_path': True, 'jitter': None, 'n_nonzero_coefs': 500, 'precompute': 'auto', 'random_state': 123, 'verbose': False}
2025-02-26 12:46:10,437:INFO:Creating Dashboard logs
2025-02-26 12:46:10,439:INFO:Model: Lasso Least Angle Regression
2025-02-26 12:46:11,349:INFO:Logged params: {'alpha': 1.0, 'copy_X': True, 'eps': 2.220446049250313e-16, 'fit_intercept': True, 'fit_path': True, 'jitter': None, 'max_iter': 500, 'positive': False, 'precompute': 'auto', 'random_state': 123, 'verbose': False}
2025-02-26 12:46:33,684:INFO:Creating Dashboard logs
2025-02-26 12:46:33,686:INFO:Model: Passive Aggressive Regressor
2025-02-26 12:46:34,386:INFO:Logged params: {'C': 1.0, 'average': False, 'early_stopping': False, 'epsilon': 0.1, 'fit_intercept': True, 'loss': 'epsilon_insensitive', 'max_iter': 1000, 'n_iter_no_change': 5, 'random_state': 123, 'shuffle': True, 'tol': 0.001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}
2025-02-26 12:46:56,728:INFO:Creating Dashboard logs
2025-02-26 12:46:56,730:INFO:Model: Elastic Net
2025-02-26 12:46:57,634:INFO:Logged params: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'l1_ratio': 0.5, 'max_iter': 1000, 'positive': False, 'precompute': False, 'random_state': 123, 'selection': 'cyclic', 'tol': 0.0001, 'warm_start': False}
2025-02-26 12:47:19,455:INFO:Creating Dashboard logs
2025-02-26 12:47:19,457:INFO:Model: Decision Tree Regressor
2025-02-26 12:47:20,262:INFO:Logged params: {'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 123, 'splitter': 'best'}
2025-02-26 12:47:42,395:INFO:Creating Dashboard logs
2025-02-26 12:47:42,397:INFO:Model: Orthogonal Matching Pursuit
2025-02-26 12:47:43,038:INFO:Logged params: {'fit_intercept': True, 'n_nonzero_coefs': None, 'precompute': 'auto', 'tol': None}
2025-02-26 12:48:05,738:INFO:Creating Dashboard logs
2025-02-26 12:48:05,740:INFO:Model: AdaBoost Regressor
2025-02-26 12:48:06,753:INFO:Logged params: {'estimator': None, 'learning_rate': 1.0, 'loss': 'linear', 'n_estimators': 50, 'random_state': 123}
2025-02-26 12:48:28,579:INFO:Creating Dashboard logs
2025-02-26 12:48:28,581:INFO:Model: Dummy Regressor
2025-02-26 12:48:29,384:INFO:Logged params: {'constant': None, 'quantile': None, 'strategy': 'mean'}
2025-02-26 12:48:51,321:INFO:_master_model_container: 20
2025-02-26 12:48:51,321:INFO:_display_container: 2
2025-02-26 12:48:51,321:INFO:<catboost.core.CatBoostRegressor object at 0x295290be0>
2025-02-26 12:48:51,321:INFO:compare_models() successfully completed......................................
2025-02-26 12:48:51,324:INFO:Initializing create_model()
2025-02-26 12:48:51,324:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x287f0a7a0>, estimator=et, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-26 12:48:51,325:INFO:Checking exceptions
2025-02-26 12:48:51,333:INFO:Importing libraries
2025-02-26 12:48:51,333:INFO:Copying training dataset
2025-02-26 12:48:51,342:INFO:Defining folds
2025-02-26 12:48:51,342:INFO:Declaring metric variables
2025-02-26 12:48:51,345:INFO:Importing untrained model
2025-02-26 12:48:51,348:INFO:Extra Trees Regressor Imported successfully
2025-02-26 12:48:51,355:INFO:Starting cross validation
2025-02-26 12:48:51,464:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-26 12:49:33,460:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:49:34,029:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:49:34,053:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:49:34,053:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:49:34,268:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:49:34,414:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:49:34,480:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:49:34,604:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:50:07,054:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:50:07,218:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:50:25,166:INFO:Calculating mean and std
2025-02-26 12:50:25,168:INFO:Creating metrics dataframe
2025-02-26 12:50:25,173:INFO:Finalizing model
2025-02-26 12:50:25,212:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:50:25,213:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:50:25,216:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000447 seconds.
2025-02-26 12:50:25,216:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:50:25,216:INFO:[LightGBM] [Info] Total Bins 1877
2025-02-26 12:50:25,216:INFO:[LightGBM] [Info] Number of data points in the train set: 9467, number of used features: 17
2025-02-26 12:50:25,217:INFO:[LightGBM] [Info] Start training from score 1.610964
2025-02-26 12:50:25,592:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:50:25,592:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:50:25,594:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000479 seconds.
2025-02-26 12:50:25,594:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:50:25,594:INFO:[LightGBM] [Info] Total Bins 1810
2025-02-26 12:50:25,594:INFO:[LightGBM] [Info] Number of data points in the train set: 8543, number of used features: 17
2025-02-26 12:50:25,595:INFO:[LightGBM] [Info] Start training from score 14.875219
2025-02-26 12:50:25,990:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:50:25,990:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:50:25,992:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000382 seconds.
2025-02-26 12:50:25,992:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:50:25,992:INFO:[LightGBM] [Info] Total Bins 1813
2025-02-26 12:50:25,992:INFO:[LightGBM] [Info] Number of data points in the train set: 5729, number of used features: 17
2025-02-26 12:50:25,992:INFO:[LightGBM] [Info] Start training from score 1965.296038
2025-02-26 12:50:26,493:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:50:26,494:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:50:26,496:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000506 seconds.
2025-02-26 12:50:26,496:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:50:26,496:INFO:[LightGBM] [Info] Total Bins 2245
2025-02-26 12:50:26,496:INFO:[LightGBM] [Info] Number of data points in the train set: 9467, number of used features: 17
2025-02-26 12:50:26,497:INFO:[LightGBM] [Info] Start training from score 1.610964
2025-02-26 12:50:26,953:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:50:26,954:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:50:26,956:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000245 seconds.
2025-02-26 12:50:26,956:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:50:26,956:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:50:26,956:INFO:[LightGBM] [Info] Total Bins 1962
2025-02-26 12:50:26,956:INFO:[LightGBM] [Info] Number of data points in the train set: 8543, number of used features: 17
2025-02-26 12:50:26,957:INFO:[LightGBM] [Info] Start training from score 14.875219
2025-02-26 12:50:27,390:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:50:27,390:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:50:27,392:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000397 seconds.
2025-02-26 12:50:27,392:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:50:27,392:INFO:[LightGBM] [Info] Total Bins 1812
2025-02-26 12:50:27,392:INFO:[LightGBM] [Info] Number of data points in the train set: 5729, number of used features: 17
2025-02-26 12:50:27,392:INFO:[LightGBM] [Info] Start training from score 1965.296038
2025-02-26 12:50:27,912:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:50:27,912:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:50:27,914:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000554 seconds.
2025-02-26 12:50:27,914:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:50:27,915:INFO:[LightGBM] [Info] Total Bins 2244
2025-02-26 12:50:27,915:INFO:[LightGBM] [Info] Number of data points in the train set: 9467, number of used features: 17
2025-02-26 12:50:27,915:INFO:[LightGBM] [Info] Start training from score 1.610964
2025-02-26 12:50:28,358:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:50:28,358:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:50:28,361:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000268 seconds.
2025-02-26 12:50:28,361:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:50:28,361:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:50:28,361:INFO:[LightGBM] [Info] Total Bins 1962
2025-02-26 12:50:28,361:INFO:[LightGBM] [Info] Number of data points in the train set: 8543, number of used features: 17
2025-02-26 12:50:28,361:INFO:[LightGBM] [Info] Start training from score 14.875219
2025-02-26 12:50:28,832:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:50:28,832:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:50:28,834:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000384 seconds.
2025-02-26 12:50:28,834:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:50:28,834:INFO:[LightGBM] [Info] Total Bins 1813
2025-02-26 12:50:28,834:INFO:[LightGBM] [Info] Number of data points in the train set: 5729, number of used features: 17
2025-02-26 12:50:28,835:INFO:[LightGBM] [Info] Start training from score 1965.296038
2025-02-26 12:50:29,228:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:50:29,228:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:50:29,231:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000282 seconds.
2025-02-26 12:50:29,231:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:50:29,231:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:50:29,231:INFO:[LightGBM] [Info] Total Bins 2243
2025-02-26 12:50:29,231:INFO:[LightGBM] [Info] Number of data points in the train set: 9467, number of used features: 17
2025-02-26 12:50:29,231:INFO:[LightGBM] [Info] Start training from score 1.610964
2025-02-26 12:50:29,850:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:50:29,850:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:50:29,853:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000250 seconds.
2025-02-26 12:50:29,853:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:50:29,853:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:50:29,853:INFO:[LightGBM] [Info] Total Bins 1961
2025-02-26 12:50:29,853:INFO:[LightGBM] [Info] Number of data points in the train set: 8543, number of used features: 17
2025-02-26 12:50:29,853:INFO:[LightGBM] [Info] Start training from score 14.875219
2025-02-26 12:50:30,469:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:50:30,469:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:50:30,471:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000384 seconds.
2025-02-26 12:50:30,471:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 12:50:30,471:INFO:[LightGBM] [Info] Total Bins 1813
2025-02-26 12:50:30,471:INFO:[LightGBM] [Info] Number of data points in the train set: 5729, number of used features: 17
2025-02-26 12:50:30,471:INFO:[LightGBM] [Info] Start training from score 1965.296038
2025-02-26 12:50:30,918:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:50:30,918:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:50:30,921:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000264 seconds.
2025-02-26 12:50:30,921:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:50:30,921:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:50:30,921:INFO:[LightGBM] [Info] Total Bins 2244
2025-02-26 12:50:30,921:INFO:[LightGBM] [Info] Number of data points in the train set: 9467, number of used features: 17
2025-02-26 12:50:30,921:INFO:[LightGBM] [Info] Start training from score 1.610964
2025-02-26 12:50:31,375:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:50:31,375:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:50:31,378:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000252 seconds.
2025-02-26 12:50:31,378:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:50:31,378:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:50:31,378:INFO:[LightGBM] [Info] Total Bins 1962
2025-02-26 12:50:31,378:INFO:[LightGBM] [Info] Number of data points in the train set: 8543, number of used features: 17
2025-02-26 12:50:31,378:INFO:[LightGBM] [Info] Start training from score 14.875219
2025-02-26 12:50:31,859:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 12:50:31,859:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 12:50:31,861:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000195 seconds.
2025-02-26 12:50:31,861:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 12:50:31,861:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 12:50:31,861:INFO:[LightGBM] [Info] Total Bins 1813
2025-02-26 12:50:31,861:INFO:[LightGBM] [Info] Number of data points in the train set: 5729, number of used features: 17
2025-02-26 12:50:31,861:INFO:[LightGBM] [Info] Start training from score 1965.296038
2025-02-26 12:50:32,445:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:50:52,405:INFO:Creating Dashboard logs
2025-02-26 12:50:52,408:INFO:Model: Extra Trees Regressor
2025-02-26 12:50:53,357:INFO:Logged params: {'bootstrap': False, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False}
2025-02-26 12:51:01,140:INFO:Initializing predict_model()
2025-02-26 12:51:01,140:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x287f0a7a0>, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=123), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=False, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x296915d80>)
2025-02-26 12:51:01,140:INFO:Checking exceptions
2025-02-26 12:51:01,141:INFO:Preloading libraries
2025-02-26 12:51:01,508:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.
  warnings.warn(

2025-02-26 12:51:30,144:INFO:Uploading results into container
2025-02-26 12:51:30,145:INFO:Uploading model into container now
2025-02-26 12:51:30,150:INFO:_master_model_container: 21
2025-02-26 12:51:30,150:INFO:_display_container: 3
2025-02-26 12:51:30,151:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=123)
2025-02-26 12:51:30,151:INFO:create_model() successfully completed......................................
2025-02-26 12:51:30,245:INFO:Initializing tune_model()
2025-02-26 12:51:30,245:INFO:tune_model(estimator=ExtraTreesRegressor(n_jobs=-1, random_state=123), fold=None, round=4, n_iter=25, custom_grid=None, optimize=RMSLE, custom_scorer=None, search_library=optuna, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=True, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.regression.oop.RegressionExperiment object at 0x287f0a7a0>)
2025-02-26 12:51:30,245:INFO:Checking exceptions
2025-02-26 12:51:30,245:INFO:Soft dependency imported: optuna: 4.1.0
2025-02-26 12:51:30,289:INFO:Copying training dataset
2025-02-26 12:51:30,295:INFO:Checking base model
2025-02-26 12:51:30,295:INFO:Base model : Extra Trees Regressor
2025-02-26 12:51:30,297:INFO:Declaring metric variables
2025-02-26 12:51:30,299:INFO:Defining Hyperparameters
2025-02-26 12:51:30,497:INFO:Tuning with n_jobs=-1
2025-02-26 12:51:30,498:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/optuna/_experimental.py:31: ExperimentalWarning: Argument ``multivariate`` is an experimental feature. The interface can change in the future.
  warnings.warn(

2025-02-26 12:51:30,498:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/optuna/_experimental.py:31: ExperimentalWarning: Argument ``constant_liar`` is an experimental feature. The interface can change in the future.
  warnings.warn(

2025-02-26 12:51:30,499:INFO:Initializing optuna.integration.OptunaSearchCV
2025-02-26 12:51:30,505:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/pycaret_experiment/supervised_experiment.py:2458: ExperimentalWarning: OptunaSearchCV is experimental (supported from v0.17.0). The interface can change in the future.
  model_grid = optuna.integration.OptunaSearchCV(  # type: ignore

2025-02-26 12:52:19,122:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:52:19,166:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:52:19,677:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:52:19,948:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:52:19,986:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:52:20,205:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:52:20,496:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 12:52:20,604:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 13:26:32,981:INFO:best_params: {'actual_estimator__n_estimators': 122, 'actual_estimator__max_depth': 11, 'actual_estimator__min_samples_split': 5, 'actual_estimator__min_samples_leaf': 3, 'actual_estimator__max_features': 0.8762623921980849, 'actual_estimator__min_impurity_decrease': 3.553384467507892e-09, 'actual_estimator__criterion': 'squared_error', 'actual_estimator__bootstrap': True}
2025-02-26 13:26:32,984:INFO:Hyperparameter search completed
2025-02-26 13:26:32,984:INFO:SubProcess create_model() called ==================================
2025-02-26 13:26:32,985:INFO:Initializing create_model()
2025-02-26 13:26:32,985:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x287f0a7a0>, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=123), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x28301a380>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'n_estimators': 122, 'max_depth': 11, 'min_samples_split': 5, 'min_samples_leaf': 3, 'max_features': 0.8762623921980849, 'min_impurity_decrease': 3.553384467507892e-09, 'criterion': 'squared_error', 'bootstrap': True})
2025-02-26 13:26:32,985:INFO:Checking exceptions
2025-02-26 13:26:32,985:INFO:Importing libraries
2025-02-26 13:26:32,985:INFO:Copying training dataset
2025-02-26 13:26:32,995:INFO:Defining folds
2025-02-26 13:26:32,995:INFO:Declaring metric variables
2025-02-26 13:26:32,997:INFO:Importing untrained model
2025-02-26 13:26:32,998:INFO:Declaring custom model
2025-02-26 13:26:33,000:INFO:Extra Trees Regressor Imported successfully
2025-02-26 13:26:33,007:INFO:Starting cross validation
2025-02-26 13:26:33,236:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-26 13:27:20,031:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 13:27:20,411:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 13:27:20,585:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 13:27:20,870:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 13:27:21,089:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 13:27:21,251:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 13:27:21,422:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 13:27:21,574:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 13:27:55,283:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 13:27:55,296:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 13:28:13,133:INFO:Calculating mean and std
2025-02-26 13:28:13,136:INFO:Creating metrics dataframe
2025-02-26 13:28:13,142:INFO:Finalizing model
2025-02-26 13:28:13,180:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 13:28:13,180:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 13:28:13,183:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000596 seconds.
2025-02-26 13:28:13,183:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 13:28:13,183:INFO:[LightGBM] [Info] Total Bins 1877
2025-02-26 13:28:13,183:INFO:[LightGBM] [Info] Number of data points in the train set: 9467, number of used features: 17
2025-02-26 13:28:13,184:INFO:[LightGBM] [Info] Start training from score 1.610964
2025-02-26 13:28:13,754:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 13:28:13,754:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 13:28:13,757:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000654 seconds.
2025-02-26 13:28:13,757:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 13:28:13,758:INFO:[LightGBM] [Info] Total Bins 1810
2025-02-26 13:28:13,758:INFO:[LightGBM] [Info] Number of data points in the train set: 8543, number of used features: 17
2025-02-26 13:28:13,758:INFO:[LightGBM] [Info] Start training from score 14.875219
2025-02-26 13:28:14,286:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 13:28:14,286:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 13:28:14,288:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000380 seconds.
2025-02-26 13:28:14,288:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 13:28:14,288:INFO:[LightGBM] [Info] Total Bins 1813
2025-02-26 13:28:14,289:INFO:[LightGBM] [Info] Number of data points in the train set: 5729, number of used features: 17
2025-02-26 13:28:14,289:INFO:[LightGBM] [Info] Start training from score 1965.296038
2025-02-26 13:28:14,860:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 13:28:14,860:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 13:28:14,863:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000252 seconds.
2025-02-26 13:28:14,863:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 13:28:14,863:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 13:28:14,863:INFO:[LightGBM] [Info] Total Bins 2245
2025-02-26 13:28:14,863:INFO:[LightGBM] [Info] Number of data points in the train set: 9467, number of used features: 17
2025-02-26 13:28:14,863:INFO:[LightGBM] [Info] Start training from score 1.610964
2025-02-26 13:28:15,546:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 13:28:15,547:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 13:28:15,549:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000287 seconds.
2025-02-26 13:28:15,549:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 13:28:15,549:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 13:28:15,549:INFO:[LightGBM] [Info] Total Bins 1962
2025-02-26 13:28:15,549:INFO:[LightGBM] [Info] Number of data points in the train set: 8543, number of used features: 17
2025-02-26 13:28:15,550:INFO:[LightGBM] [Info] Start training from score 14.875219
2025-02-26 13:28:16,077:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 13:28:16,077:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 13:28:16,079:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000380 seconds.
2025-02-26 13:28:16,079:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 13:28:16,079:INFO:[LightGBM] [Info] Total Bins 1812
2025-02-26 13:28:16,079:INFO:[LightGBM] [Info] Number of data points in the train set: 5729, number of used features: 17
2025-02-26 13:28:16,079:INFO:[LightGBM] [Info] Start training from score 1965.296038
2025-02-26 13:28:16,625:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 13:28:16,625:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 13:28:16,627:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000236 seconds.
2025-02-26 13:28:16,628:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 13:28:16,628:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 13:28:16,628:INFO:[LightGBM] [Info] Total Bins 2244
2025-02-26 13:28:16,628:INFO:[LightGBM] [Info] Number of data points in the train set: 9467, number of used features: 17
2025-02-26 13:28:16,628:INFO:[LightGBM] [Info] Start training from score 1.610964
2025-02-26 13:28:17,340:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 13:28:17,340:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 13:28:17,342:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000289 seconds.
2025-02-26 13:28:17,342:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 13:28:17,342:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 13:28:17,342:INFO:[LightGBM] [Info] Total Bins 1962
2025-02-26 13:28:17,343:INFO:[LightGBM] [Info] Number of data points in the train set: 8543, number of used features: 17
2025-02-26 13:28:17,343:INFO:[LightGBM] [Info] Start training from score 14.875219
2025-02-26 13:28:17,965:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 13:28:17,965:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 13:28:17,967:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000408 seconds.
2025-02-26 13:28:17,967:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 13:28:17,967:INFO:[LightGBM] [Info] Total Bins 1813
2025-02-26 13:28:17,968:INFO:[LightGBM] [Info] Number of data points in the train set: 5729, number of used features: 17
2025-02-26 13:28:17,968:INFO:[LightGBM] [Info] Start training from score 1965.296038
2025-02-26 13:28:18,391:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 13:28:18,391:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 13:28:18,394:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000302 seconds.
2025-02-26 13:28:18,394:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 13:28:18,395:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 13:28:18,395:INFO:[LightGBM] [Info] Total Bins 2243
2025-02-26 13:28:18,395:INFO:[LightGBM] [Info] Number of data points in the train set: 9467, number of used features: 17
2025-02-26 13:28:18,395:INFO:[LightGBM] [Info] Start training from score 1.610964
2025-02-26 13:28:19,081:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 13:28:19,081:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 13:28:19,085:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000278 seconds.
2025-02-26 13:28:19,085:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 13:28:19,085:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 13:28:19,085:INFO:[LightGBM] [Info] Total Bins 1961
2025-02-26 13:28:19,085:INFO:[LightGBM] [Info] Number of data points in the train set: 8543, number of used features: 17
2025-02-26 13:28:19,085:INFO:[LightGBM] [Info] Start training from score 14.875219
2025-02-26 13:28:19,762:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 13:28:19,762:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 13:28:19,764:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000461 seconds.
2025-02-26 13:28:19,764:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 13:28:19,764:INFO:[LightGBM] [Info] Total Bins 1813
2025-02-26 13:28:19,765:INFO:[LightGBM] [Info] Number of data points in the train set: 5729, number of used features: 17
2025-02-26 13:28:19,765:INFO:[LightGBM] [Info] Start training from score 1965.296038
2025-02-26 13:28:20,218:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 13:28:20,219:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 13:28:20,221:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000518 seconds.
2025-02-26 13:28:20,221:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 13:28:20,222:INFO:[LightGBM] [Info] Total Bins 2244
2025-02-26 13:28:20,222:INFO:[LightGBM] [Info] Number of data points in the train set: 9467, number of used features: 17
2025-02-26 13:28:20,222:INFO:[LightGBM] [Info] Start training from score 1.610964
2025-02-26 13:28:20,816:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 13:28:20,816:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 13:28:20,819:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000478 seconds.
2025-02-26 13:28:20,819:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 13:28:20,819:INFO:[LightGBM] [Info] Total Bins 1962
2025-02-26 13:28:20,819:INFO:[LightGBM] [Info] Number of data points in the train set: 8543, number of used features: 17
2025-02-26 13:28:20,820:INFO:[LightGBM] [Info] Start training from score 14.875219
2025-02-26 13:28:21,400:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 13:28:21,401:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 13:28:21,403:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000377 seconds.
2025-02-26 13:28:21,403:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 13:28:21,403:INFO:[LightGBM] [Info] Total Bins 1813
2025-02-26 13:28:21,404:INFO:[LightGBM] [Info] Number of data points in the train set: 5729, number of used features: 17
2025-02-26 13:28:21,404:INFO:[LightGBM] [Info] Start training from score 1965.296038
2025-02-26 13:28:42,150:INFO:Uploading results into container
2025-02-26 13:28:42,151:INFO:Uploading model into container now
2025-02-26 13:28:42,151:INFO:_master_model_container: 22
2025-02-26 13:28:42,151:INFO:_display_container: 4
2025-02-26 13:28:42,152:INFO:ExtraTreesRegressor(bootstrap=True, max_depth=11,
                    max_features=0.8762623921980849,
                    min_impurity_decrease=3.553384467507892e-09,
                    min_samples_leaf=3, min_samples_split=5, n_estimators=122,
                    n_jobs=-1, random_state=123)
2025-02-26 13:28:42,152:INFO:create_model() successfully completed......................................
2025-02-26 13:28:42,284:INFO:SubProcess create_model() end ==================================
2025-02-26 13:28:42,284:INFO:choose_better activated
2025-02-26 13:28:42,286:INFO:SubProcess create_model() called ==================================
2025-02-26 13:28:42,286:INFO:Initializing create_model()
2025-02-26 13:28:42,286:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x287f0a7a0>, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=123), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-26 13:28:42,286:INFO:Checking exceptions
2025-02-26 13:28:42,287:INFO:Importing libraries
2025-02-26 13:28:42,287:INFO:Copying training dataset
2025-02-26 13:28:42,293:INFO:Defining folds
2025-02-26 13:28:42,293:INFO:Declaring metric variables
2025-02-26 13:28:42,293:INFO:Importing untrained model
2025-02-26 13:28:42,293:INFO:Declaring custom model
2025-02-26 13:28:42,293:INFO:Extra Trees Regressor Imported successfully
2025-02-26 13:28:42,293:INFO:Starting cross validation
2025-02-26 13:28:42,427:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-26 13:29:24,523:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 13:29:24,689:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 13:29:25,256:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 13:29:25,306:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 13:29:25,340:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 13:29:25,583:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 13:29:25,940:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 13:29:25,969:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 13:30:01,065:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 13:30:01,543:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 13:30:20,299:INFO:Calculating mean and std
2025-02-26 13:30:20,300:INFO:Creating metrics dataframe
2025-02-26 13:30:20,303:INFO:Finalizing model
2025-02-26 13:30:20,337:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 13:30:20,337:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 13:30:20,340:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000260 seconds.
2025-02-26 13:30:20,340:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 13:30:20,340:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 13:30:20,340:INFO:[LightGBM] [Info] Total Bins 1877
2025-02-26 13:30:20,340:INFO:[LightGBM] [Info] Number of data points in the train set: 9467, number of used features: 17
2025-02-26 13:30:20,341:INFO:[LightGBM] [Info] Start training from score 1.610964
2025-02-26 13:30:20,927:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 13:30:20,927:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 13:30:20,929:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000535 seconds.
2025-02-26 13:30:20,929:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 13:30:20,929:INFO:[LightGBM] [Info] Total Bins 1810
2025-02-26 13:30:20,930:INFO:[LightGBM] [Info] Number of data points in the train set: 8543, number of used features: 17
2025-02-26 13:30:20,930:INFO:[LightGBM] [Info] Start training from score 14.875219
2025-02-26 13:30:21,418:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 13:30:21,418:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 13:30:21,420:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000418 seconds.
2025-02-26 13:30:21,420:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 13:30:21,420:INFO:[LightGBM] [Info] Total Bins 1813
2025-02-26 13:30:21,420:INFO:[LightGBM] [Info] Number of data points in the train set: 5729, number of used features: 17
2025-02-26 13:30:21,421:INFO:[LightGBM] [Info] Start training from score 1965.296038
2025-02-26 13:30:21,959:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 13:30:21,959:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 13:30:21,961:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000482 seconds.
2025-02-26 13:30:21,961:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 13:30:21,962:INFO:[LightGBM] [Info] Total Bins 2245
2025-02-26 13:30:21,962:INFO:[LightGBM] [Info] Number of data points in the train set: 9467, number of used features: 17
2025-02-26 13:30:21,962:INFO:[LightGBM] [Info] Start training from score 1.610964
2025-02-26 13:30:22,478:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 13:30:22,478:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 13:30:22,480:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000277 seconds.
2025-02-26 13:30:22,480:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 13:30:22,480:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 13:30:22,480:INFO:[LightGBM] [Info] Total Bins 1962
2025-02-26 13:30:22,481:INFO:[LightGBM] [Info] Number of data points in the train set: 8543, number of used features: 17
2025-02-26 13:30:22,481:INFO:[LightGBM] [Info] Start training from score 14.875219
2025-02-26 13:30:23,057:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 13:30:23,057:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 13:30:23,059:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000438 seconds.
2025-02-26 13:30:23,059:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 13:30:23,059:INFO:[LightGBM] [Info] Total Bins 1812
2025-02-26 13:30:23,059:INFO:[LightGBM] [Info] Number of data points in the train set: 5729, number of used features: 17
2025-02-26 13:30:23,060:INFO:[LightGBM] [Info] Start training from score 1965.296038
2025-02-26 13:30:23,603:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 13:30:23,604:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 13:30:23,606:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000565 seconds.
2025-02-26 13:30:23,606:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 13:30:23,606:INFO:[LightGBM] [Info] Total Bins 2244
2025-02-26 13:30:23,606:INFO:[LightGBM] [Info] Number of data points in the train set: 9467, number of used features: 17
2025-02-26 13:30:23,607:INFO:[LightGBM] [Info] Start training from score 1.610964
2025-02-26 13:30:24,175:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 13:30:24,175:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 13:30:24,178:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000516 seconds.
2025-02-26 13:30:24,178:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 13:30:24,178:INFO:[LightGBM] [Info] Total Bins 1962
2025-02-26 13:30:24,178:INFO:[LightGBM] [Info] Number of data points in the train set: 8543, number of used features: 17
2025-02-26 13:30:24,179:INFO:[LightGBM] [Info] Start training from score 14.875219
2025-02-26 13:30:24,718:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 13:30:24,718:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 13:30:24,720:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000417 seconds.
2025-02-26 13:30:24,720:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 13:30:24,720:INFO:[LightGBM] [Info] Total Bins 1813
2025-02-26 13:30:24,720:INFO:[LightGBM] [Info] Number of data points in the train set: 5729, number of used features: 17
2025-02-26 13:30:24,721:INFO:[LightGBM] [Info] Start training from score 1965.296038
2025-02-26 13:30:25,282:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 13:30:25,283:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 13:30:25,285:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000523 seconds.
2025-02-26 13:30:25,285:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 13:30:25,286:INFO:[LightGBM] [Info] Total Bins 2243
2025-02-26 13:30:25,286:INFO:[LightGBM] [Info] Number of data points in the train set: 9467, number of used features: 17
2025-02-26 13:30:25,286:INFO:[LightGBM] [Info] Start training from score 1.610964
2025-02-26 13:30:25,857:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 13:30:25,857:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 13:30:25,859:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000510 seconds.
2025-02-26 13:30:25,860:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 13:30:25,860:INFO:[LightGBM] [Info] Total Bins 1961
2025-02-26 13:30:25,860:INFO:[LightGBM] [Info] Number of data points in the train set: 8543, number of used features: 17
2025-02-26 13:30:25,860:INFO:[LightGBM] [Info] Start training from score 14.875219
2025-02-26 13:30:26,406:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 13:30:26,406:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 13:30:26,408:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000385 seconds.
2025-02-26 13:30:26,408:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 13:30:26,408:INFO:[LightGBM] [Info] Total Bins 1813
2025-02-26 13:30:26,408:INFO:[LightGBM] [Info] Number of data points in the train set: 5729, number of used features: 17
2025-02-26 13:30:26,408:INFO:[LightGBM] [Info] Start training from score 1965.296038
2025-02-26 13:30:26,966:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 13:30:26,966:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 13:30:26,969:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000261 seconds.
2025-02-26 13:30:26,969:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 13:30:26,969:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 13:30:26,969:INFO:[LightGBM] [Info] Total Bins 2244
2025-02-26 13:30:26,969:INFO:[LightGBM] [Info] Number of data points in the train set: 9467, number of used features: 17
2025-02-26 13:30:26,970:INFO:[LightGBM] [Info] Start training from score 1.610964
2025-02-26 13:30:27,649:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 13:30:27,649:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 13:30:27,652:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000249 seconds.
2025-02-26 13:30:27,652:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 13:30:27,652:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 13:30:27,652:INFO:[LightGBM] [Info] Total Bins 1962
2025-02-26 13:30:27,652:INFO:[LightGBM] [Info] Number of data points in the train set: 8543, number of used features: 17
2025-02-26 13:30:27,653:INFO:[LightGBM] [Info] Start training from score 14.875219
2025-02-26 13:30:28,418:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 13:30:28,418:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 13:30:28,420:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000394 seconds.
2025-02-26 13:30:28,420:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 13:30:28,420:INFO:[LightGBM] [Info] Total Bins 1813
2025-02-26 13:30:28,420:INFO:[LightGBM] [Info] Number of data points in the train set: 5729, number of used features: 17
2025-02-26 13:30:28,421:INFO:[LightGBM] [Info] Start training from score 1965.296038
2025-02-26 13:30:50,489:INFO:Uploading results into container
2025-02-26 13:30:50,490:INFO:Uploading model into container now
2025-02-26 13:30:50,490:INFO:_master_model_container: 23
2025-02-26 13:30:50,490:INFO:_display_container: 5
2025-02-26 13:30:50,491:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=123)
2025-02-26 13:30:50,491:INFO:create_model() successfully completed......................................
2025-02-26 13:30:50,614:INFO:SubProcess create_model() end ==================================
2025-02-26 13:30:50,614:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=123) result for RMSLE is 0.2257
2025-02-26 13:30:50,615:INFO:ExtraTreesRegressor(bootstrap=True, max_depth=11,
                    max_features=0.8762623921980849,
                    min_impurity_decrease=3.553384467507892e-09,
                    min_samples_leaf=3, min_samples_split=5, n_estimators=122,
                    n_jobs=-1, random_state=123) result for RMSLE is 0.2418
2025-02-26 13:30:50,615:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=123) is best model
2025-02-26 13:30:50,615:INFO:choose_better completed
2025-02-26 13:30:50,615:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2025-02-26 13:30:50,616:INFO:Creating Dashboard logs
2025-02-26 13:30:50,619:INFO:Model: Extra Trees Regressor
2025-02-26 13:30:51,594:INFO:Logged params: {'bootstrap': False, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False}
2025-02-26 13:30:56,917:INFO:Initializing predict_model()
2025-02-26 13:30:56,917:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x287f0a7a0>, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=123), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=False, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x29b87ba30>)
2025-02-26 13:30:56,917:INFO:Checking exceptions
2025-02-26 13:30:56,917:INFO:Preloading libraries
2025-02-26 13:31:27,563:INFO:_master_model_container: 23
2025-02-26 13:31:27,564:INFO:_display_container: 4
2025-02-26 13:31:27,565:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=123)
2025-02-26 13:31:27,565:INFO:tune_model() successfully completed......................................
2025-02-26 13:31:27,658:INFO:Initializing predict_model()
2025-02-26 13:31:27,659:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x287f0a7a0>, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=123), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x29696db40>)
2025-02-26 13:31:27,659:INFO:Checking exceptions
2025-02-26 13:31:27,659:INFO:Preloading libraries
2025-02-26 13:31:28,099:INFO:Initializing evaluate_model()
2025-02-26 13:31:28,099:INFO:evaluate_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x287f0a7a0>, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=123), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2025-02-26 13:31:28,111:INFO:Initializing plot_model()
2025-02-26 13:31:28,111:INFO:plot_model(plot=pipeline, fold=KFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=123), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x287f0a7a0>, system=True)
2025-02-26 13:31:28,111:INFO:Checking exceptions
2025-02-26 13:31:28,129:INFO:Preloading libraries
2025-02-26 13:31:28,199:INFO:Copying training dataset
2025-02-26 13:31:28,199:INFO:Plot type: pipeline
2025-02-26 13:31:28,340:INFO:Visual Rendered Successfully
2025-02-26 13:31:28,431:INFO:plot_model() successfully completed......................................
2025-02-26 13:31:28,443:INFO:Initializing plot_model()
2025-02-26 13:31:28,443:INFO:plot_model(plot=residuals, fold=None, verbose=True, display=None, display_format=None, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=123), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs={'observed': True, 'color': 'blue'}, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x287f0a7a0>, system=True)
2025-02-26 13:31:28,443:INFO:Checking exceptions
2025-02-26 13:31:28,463:INFO:Preloading libraries
2025-02-26 13:31:28,529:INFO:Copying training dataset
2025-02-26 13:31:28,529:INFO:Plot type: residuals
2025-02-26 13:31:29,142:INFO:Fitting Model
2025-02-26 13:31:29,239:INFO:Scoring test/hold-out set
2025-02-26 13:31:29,544:INFO:Visual Rendered Successfully
2025-02-26 13:31:29,644:INFO:plot_model() successfully completed......................................
2025-02-26 13:31:29,657:INFO:Initializing plot_model()
2025-02-26 13:31:29,658:INFO:plot_model(plot=error, fold=None, verbose=True, display=None, display_format=None, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=123), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs={'alpha': 0.5, 'title': 'Actual vs Predicted'}, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x287f0a7a0>, system=True)
2025-02-26 13:31:29,658:INFO:Checking exceptions
2025-02-26 13:31:29,676:INFO:Preloading libraries
2025-02-26 13:31:29,744:INFO:Copying training dataset
2025-02-26 13:31:29,744:INFO:Plot type: error
2025-02-26 13:31:30,305:INFO:Fitting Model
2025-02-26 13:31:30,305:INFO:Scoring test/hold-out set
2025-02-26 13:31:30,477:INFO:Visual Rendered Successfully
2025-02-26 13:31:30,570:INFO:plot_model() successfully completed......................................
2025-02-26 13:31:30,578:INFO:Initializing plot_model()
2025-02-26 13:31:30,578:INFO:plot_model(plot=cooks, fold=None, verbose=True, display=None, display_format=None, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=123), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs={'draw_threshold': True, 'linefmt': 'r--'}, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x287f0a7a0>, system=True)
2025-02-26 13:31:30,578:INFO:Checking exceptions
2025-02-26 13:31:30,597:INFO:Preloading libraries
2025-02-26 13:31:30,664:INFO:Copying training dataset
2025-02-26 13:31:30,664:INFO:Plot type: cooks
2025-02-26 13:31:31,227:INFO:Fitting Model
2025-02-26 13:34:41,478:INFO:Initializing plot_model()
2025-02-26 13:34:41,479:INFO:plot_model(plot=cooks, fold=None, verbose=True, display=None, display_format=None, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=123), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x287f0a7a0>, system=True)
2025-02-26 13:34:41,479:INFO:Checking exceptions
2025-02-26 13:34:41,506:INFO:Preloading libraries
2025-02-26 13:34:41,577:INFO:Copying training dataset
2025-02-26 13:34:41,577:INFO:Plot type: cooks
2025-02-26 13:34:42,137:INFO:Fitting Model
2025-02-26 13:35:44,447:INFO:Initializing plot_model()
2025-02-26 13:35:44,447:INFO:plot_model(plot=cooks, fold=None, verbose=True, display=None, display_format=None, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=123), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x287f0a7a0>, system=True)
2025-02-26 13:35:44,447:INFO:Checking exceptions
2025-02-26 13:35:44,511:INFO:Preloading libraries
2025-02-26 13:35:44,579:INFO:Copying training dataset
2025-02-26 13:35:44,579:INFO:Plot type: cooks
2025-02-26 13:35:45,116:INFO:Fitting Model
2025-02-26 13:35:52,082:INFO:Initializing plot_model()
2025-02-26 13:35:52,082:INFO:plot_model(plot=feature, fold=None, verbose=True, display=None, display_format=None, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=123), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs={'top_n': 15, 'figsize': (10, 6)}, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x287f0a7a0>, system=True)
2025-02-26 13:35:52,083:INFO:Checking exceptions
2025-02-26 13:35:52,113:INFO:Preloading libraries
2025-02-26 13:35:52,181:INFO:Copying training dataset
2025-02-26 13:35:52,181:INFO:Plot type: feature
2025-02-26 13:35:52,182:WARNING:No coef_ found. Trying feature_importances_
2025-02-26 13:35:52,438:INFO:Visual Rendered Successfully
2025-02-26 13:35:52,553:INFO:plot_model() successfully completed......................................
2025-02-26 13:35:56,889:INFO:Initializing plot_model()
2025-02-26 13:35:56,889:INFO:plot_model(plot=feature, fold=None, verbose=True, display=None, display_format=None, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=123), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs={'top_n': 15, 'figsize': (10, 6)}, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x287f0a7a0>, system=True)
2025-02-26 13:35:56,889:INFO:Checking exceptions
2025-02-26 13:35:56,920:INFO:Preloading libraries
2025-02-26 13:35:56,979:INFO:Copying training dataset
2025-02-26 13:35:56,979:INFO:Plot type: feature
2025-02-26 13:35:56,979:WARNING:No coef_ found. Trying feature_importances_
2025-02-26 13:35:57,202:INFO:Visual Rendered Successfully
2025-02-26 13:35:57,314:INFO:plot_model() successfully completed......................................
2025-02-26 13:36:19,629:INFO:Initializing plot_model()
2025-02-26 13:36:19,630:INFO:plot_model(plot=learning, fold=None, verbose=True, display=None, display_format=None, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=123), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs={'train_sizes': array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])}, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x287f0a7a0>, system=True)
2025-02-26 13:36:19,631:INFO:Checking exceptions
2025-02-26 13:36:19,658:INFO:Preloading libraries
2025-02-26 13:36:19,714:INFO:Copying training dataset
2025-02-26 13:36:19,714:INFO:Plot type: learning
2025-02-26 13:36:20,264:INFO:Fitting Model
2025-02-26 13:36:58,087:INFO:Initializing plot_model()
2025-02-26 13:36:58,090:INFO:plot_model(plot=learning, fold=None, verbose=True, display=None, display_format=None, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=123), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x287f0a7a0>, system=True)
2025-02-26 13:36:58,090:INFO:Checking exceptions
2025-02-26 13:36:58,142:INFO:Preloading libraries
2025-02-26 13:36:58,231:INFO:Copying training dataset
2025-02-26 13:36:58,231:INFO:Plot type: learning
2025-02-26 13:36:58,803:INFO:Fitting Model
2025-02-26 13:38:10,266:INFO:Visual Rendered Successfully
2025-02-26 13:38:10,468:INFO:plot_model() successfully completed......................................
2025-02-26 13:39:42,632:INFO:Initializing evaluate_model()
2025-02-26 13:39:42,633:INFO:evaluate_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x287f0a7a0>, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=123), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2025-02-26 13:39:42,650:INFO:Initializing plot_model()
2025-02-26 13:39:42,650:INFO:plot_model(plot=pipeline, fold=KFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=123), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x287f0a7a0>, system=True)
2025-02-26 13:39:42,650:INFO:Checking exceptions
2025-02-26 13:39:42,684:INFO:Preloading libraries
2025-02-26 13:39:42,782:INFO:Copying training dataset
2025-02-26 13:39:42,782:INFO:Plot type: pipeline
2025-02-26 13:39:42,907:INFO:Visual Rendered Successfully
2025-02-26 13:39:43,033:INFO:plot_model() successfully completed......................................
2025-02-26 13:39:51,527:INFO:Initializing evaluate_model()
2025-02-26 13:39:51,527:INFO:evaluate_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x287f0a7a0>, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=123), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2025-02-26 13:39:51,546:INFO:Initializing plot_model()
2025-02-26 13:39:51,546:INFO:plot_model(plot=pipeline, fold=KFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=123), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x287f0a7a0>, system=True)
2025-02-26 13:39:51,546:INFO:Checking exceptions
2025-02-26 13:39:51,566:INFO:Preloading libraries
2025-02-26 13:39:51,626:INFO:Copying training dataset
2025-02-26 13:39:51,626:INFO:Plot type: pipeline
2025-02-26 13:39:51,747:INFO:Visual Rendered Successfully
2025-02-26 13:39:51,871:INFO:plot_model() successfully completed......................................
2025-02-26 13:39:59,483:INFO:Initializing interpret_model()
2025-02-26 13:39:59,484:INFO:interpret_model(estimator=ExtraTreesRegressor(n_jobs=-1, random_state=123), use_train_data=False, X_new_sample=None, y_new_sample=None, feature=None, kwargs={}, observation=None, plot=summary, save=False, self=<pycaret.regression.oop.RegressionExperiment object at 0x287f0a7a0>)
2025-02-26 13:39:59,484:INFO:Checking exceptions
2025-02-26 13:39:59,484:INFO:Soft dependency imported: shap: 0.44.1
2025-02-26 13:40:01,147:INFO:plot type: summary
2025-02-26 13:40:01,147:INFO:Creating TreeExplainer
2025-02-26 13:40:01,185:INFO:Compiling shap values
2025-02-26 13:59:38,017:INFO:Initializing interpret_model()
2025-02-26 13:59:38,019:INFO:interpret_model(estimator=ExtraTreesRegressor(n_jobs=-1, random_state=123), use_train_data=False, X_new_sample=None, y_new_sample=None, feature=None, kwargs={}, observation=1, plot=reason, save=False, self=<pycaret.regression.oop.RegressionExperiment object at 0x287f0a7a0>)
2025-02-26 13:59:38,019:INFO:Checking exceptions
2025-02-26 13:59:38,019:INFO:Soft dependency imported: shap: 0.44.1
2025-02-26 13:59:38,175:INFO:plot type: reason
2025-02-26 13:59:38,175:INFO:model type detected: type 2
2025-02-26 13:59:38,175:INFO:Creating TreeExplainer
2025-02-26 13:59:38,214:INFO:Compiling shap values
2025-02-26 14:10:16,348:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-26 14:10:16,348:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-26 14:10:16,348:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-26 14:10:16,348:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-26 14:10:26,659:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-26 14:10:26,659:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-26 14:10:26,659:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-26 14:10:26,659:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-26 14:10:27,783:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_30792/2838843215.py:2: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.boxplot(data=df, x="Method", y="Price", palette="viridis")

2025-02-26 14:10:27,874:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_30792/2297183518.py:4: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.barplot(x=avg_price_by_seller.index, y=avg_price_by_seller.values, palette="viridis")

2025-02-26 14:10:28,900:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_30792/1639960328.py:2: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.boxplot(

2025-02-26 14:10:29,345:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_30792/2157425422.py:3: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.violinplot(data=df, x="Bedroom2", y="Price", palette="viridis", ax=axes[0])

2025-02-26 14:10:29,558:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_30792/2157425422.py:8: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.violinplot(data=df, x="Bathroom", y="Price", palette="viridis", ax=axes[1])

2025-02-26 14:10:29,631:WARNING:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/ipykernel_30792/2157425422.py:13: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.violinplot(data=df, x="Car", y="Price", palette="viridis", ax=axes[2])

2025-02-26 14:10:30,653:INFO:PyCaret RegressionExperiment
2025-02-26 14:10:30,653:INFO:Logging name: house_pricing
2025-02-26 14:10:30,653:INFO:ML Usecase: MLUsecase.REGRESSION
2025-02-26 14:10:30,653:INFO:version 3.3.2
2025-02-26 14:10:30,653:INFO:Initializing setup()
2025-02-26 14:10:30,653:INFO:self.USI: b658
2025-02-26 14:10:30,653:INFO:self._variable_keys: {'logging_param', 'idx', 'X_test', 'fold_generator', 'X_train', 'seed', 'target_param', 'y_test', 'fold_groups_param', 'gpu_n_jobs_param', 'X', '_ml_usecase', 'data', 'fold_shuffle_param', 'USI', '_available_plots', 'transform_target_param', 'memory', 'n_jobs_param', 'log_plots_param', 'exp_id', 'y_train', 'y', 'pipeline', 'exp_name_log', 'html_param', 'gpu_param'}
2025-02-26 14:10:30,653:INFO:Checking environment
2025-02-26 14:10:30,653:INFO:python_version: 3.10.15
2025-02-26 14:10:30,653:INFO:python_build: ('main', 'Oct  3 2024 02:24:49')
2025-02-26 14:10:30,653:INFO:machine: arm64
2025-02-26 14:10:30,653:INFO:platform: macOS-14.1-arm64-arm-64bit
2025-02-26 14:10:30,653:INFO:Memory: svmem(total=17179869184, available=6543523840, percent=61.9, used=7089504256, free=1408991232, active=5229936640, inactive=4820189184, wired=1859567616)
2025-02-26 14:10:30,653:INFO:Physical Core: 8
2025-02-26 14:10:30,653:INFO:Logical Core: 8
2025-02-26 14:10:30,653:INFO:Checking libraries
2025-02-26 14:10:30,653:INFO:System:
2025-02-26 14:10:30,653:INFO:    python: 3.10.15 (main, Oct  3 2024, 02:24:49) [Clang 14.0.6 ]
2025-02-26 14:10:30,653:INFO:executable: /Users/daaa/opt/miniconda3/envs/mlops/bin/python
2025-02-26 14:10:30,653:INFO:   machine: macOS-14.1-arm64-arm-64bit
2025-02-26 14:10:30,653:INFO:PyCaret required dependencies:
2025-02-26 14:10:30,975:INFO:                 pip: 24.2
2025-02-26 14:10:30,975:INFO:          setuptools: 75.1.0
2025-02-26 14:10:30,975:INFO:             pycaret: 3.3.2
2025-02-26 14:10:30,975:INFO:             IPython: 8.30.0
2025-02-26 14:10:30,975:INFO:          ipywidgets: 8.1.5
2025-02-26 14:10:30,975:INFO:                tqdm: 4.67.1
2025-02-26 14:10:30,975:INFO:               numpy: 1.26.4
2025-02-26 14:10:30,975:INFO:              pandas: 2.1.4
2025-02-26 14:10:30,975:INFO:              jinja2: 3.1.4
2025-02-26 14:10:30,975:INFO:               scipy: 1.11.4
2025-02-26 14:10:30,975:INFO:              joblib: 1.3.2
2025-02-26 14:10:30,975:INFO:             sklearn: 1.4.2
2025-02-26 14:10:30,975:INFO:                pyod: 2.0.2
2025-02-26 14:10:30,975:INFO:            imblearn: 0.12.4
2025-02-26 14:10:30,975:INFO:   category_encoders: 2.6.4
2025-02-26 14:10:30,975:INFO:            lightgbm: 4.5.0
2025-02-26 14:10:30,975:INFO:               numba: 0.60.0
2025-02-26 14:10:30,975:INFO:            requests: 2.32.3
2025-02-26 14:10:30,975:INFO:          matplotlib: 3.10.0
2025-02-26 14:10:30,975:INFO:          scikitplot: 0.3.7
2025-02-26 14:10:30,975:INFO:         yellowbrick: 1.5
2025-02-26 14:10:30,975:INFO:              plotly: 5.24.1
2025-02-26 14:10:30,975:INFO:    plotly-resampler: Not installed
2025-02-26 14:10:30,975:INFO:             kaleido: 0.2.1
2025-02-26 14:10:30,975:INFO:           schemdraw: 0.15
2025-02-26 14:10:30,975:INFO:         statsmodels: 0.14.4
2025-02-26 14:10:30,975:INFO:              sktime: 0.26.0
2025-02-26 14:10:30,976:INFO:               tbats: 1.1.3
2025-02-26 14:10:30,976:INFO:            pmdarima: 2.0.4
2025-02-26 14:10:30,976:INFO:              psutil: 6.1.0
2025-02-26 14:10:30,976:INFO:          markupsafe: 2.1.5
2025-02-26 14:10:30,976:INFO:             pickle5: Not installed
2025-02-26 14:10:30,976:INFO:         cloudpickle: 3.1.0
2025-02-26 14:10:30,976:INFO:         deprecation: 2.1.0
2025-02-26 14:10:30,976:INFO:              xxhash: 3.5.0
2025-02-26 14:10:30,976:INFO:           wurlitzer: 3.1.1
2025-02-26 14:10:30,976:INFO:PyCaret optional dependencies:
2025-02-26 14:10:31,983:INFO:                shap: 0.44.1
2025-02-26 14:10:31,983:INFO:           interpret: 0.6.9
2025-02-26 14:10:31,983:INFO:                umap: 0.5.7
2025-02-26 14:10:31,983:INFO:     ydata_profiling: 4.12.1
2025-02-26 14:10:31,983:INFO:  explainerdashboard: 0.4.8
2025-02-26 14:10:31,983:INFO:             autoviz: Not installed
2025-02-26 14:10:31,984:INFO:           fairlearn: 0.7.0
2025-02-26 14:10:31,984:INFO:          deepchecks: Not installed
2025-02-26 14:10:31,984:INFO:             xgboost: 2.1.3
2025-02-26 14:10:31,984:INFO:            catboost: 1.1.1
2025-02-26 14:10:31,984:INFO:              kmodes: 0.12.2
2025-02-26 14:10:31,984:INFO:             mlxtend: 0.23.3
2025-02-26 14:10:31,984:INFO:       statsforecast: 1.5.0
2025-02-26 14:10:31,984:INFO:        tune_sklearn: 0.5.0
2025-02-26 14:10:31,984:INFO:                 ray: 2.40.0
2025-02-26 14:10:31,984:INFO:            hyperopt: 0.2.7
2025-02-26 14:10:31,984:INFO:              optuna: 4.1.0
2025-02-26 14:10:31,984:INFO:               skopt: 0.10.2
2025-02-26 14:10:31,984:INFO:              mlflow: 2.16.0
2025-02-26 14:10:31,984:INFO:              gradio: 5.12.0
2025-02-26 14:10:31,984:INFO:             fastapi: 0.115.6
2025-02-26 14:10:31,984:INFO:             uvicorn: 0.34.0
2025-02-26 14:10:31,984:INFO:              m2cgen: 0.10.0
2025-02-26 14:10:31,984:INFO:           evidently: 0.4.40
2025-02-26 14:10:31,984:INFO:               fugue: 0.8.7
2025-02-26 14:10:31,984:INFO:           streamlit: Not installed
2025-02-26 14:10:31,984:INFO:             prophet: Not installed
2025-02-26 14:10:31,984:INFO:None
2025-02-26 14:10:31,984:INFO:Set up data.
2025-02-26 14:10:32,001:INFO:Set up folding strategy.
2025-02-26 14:10:32,001:INFO:Set up train/test split.
2025-02-26 14:10:32,008:INFO:Set up index.
2025-02-26 14:10:32,008:INFO:Assigning column types.
2025-02-26 14:10:32,011:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-02-26 14:10:32,011:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2025-02-26 14:10:32,013:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-26 14:10:32,015:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-26 14:10:32,042:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-26 14:10:32,060:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-26 14:10:32,060:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-26 14:10:32,061:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-26 14:10:32,074:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2025-02-26 14:10:32,076:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-26 14:10:32,078:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-26 14:10:32,104:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-26 14:10:32,122:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-26 14:10:32,123:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-26 14:10:32,124:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-26 14:10:32,124:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2025-02-26 14:10:32,126:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-26 14:10:32,128:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-26 14:10:32,153:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-26 14:10:32,172:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-26 14:10:32,172:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-26 14:10:32,173:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-26 14:10:32,175:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-26 14:10:32,177:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-26 14:10:32,204:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-26 14:10:32,222:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-26 14:10:32,222:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-26 14:10:32,223:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-26 14:10:32,223:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2025-02-26 14:10:32,227:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-26 14:10:32,253:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-26 14:10:32,272:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-26 14:10:32,272:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-26 14:10:32,273:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-26 14:10:32,277:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-26 14:10:32,302:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-26 14:10:32,320:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-26 14:10:32,321:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-26 14:10:32,322:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-26 14:10:32,322:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2025-02-26 14:10:32,352:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-26 14:10:32,369:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-26 14:10:32,370:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-26 14:10:32,371:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-26 14:10:32,400:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-26 14:10:32,419:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-26 14:10:32,419:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-26 14:10:32,420:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-26 14:10:32,421:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-02-26 14:10:32,451:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-26 14:10:32,469:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-26 14:10:32,470:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-26 14:10:32,499:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-26 14:10:32,518:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-26 14:10:32,519:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-26 14:10:32,519:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2025-02-26 14:10:32,566:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-26 14:10:32,567:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-26 14:10:32,613:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-26 14:10:32,614:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-26 14:10:32,616:INFO:Preparing preprocessing pipeline...
2025-02-26 14:10:32,616:INFO:Set up date feature engineering.
2025-02-26 14:10:32,616:INFO:Set up iterative imputation.
2025-02-26 14:10:32,661:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-26 14:10:32,662:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-26 14:10:32,682:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-26 14:10:32,696:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-26 14:10:32,697:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-26 14:10:32,711:INFO:Set up encoding of ordinal features.
2025-02-26 14:10:32,714:WARNING:The number of classes passed to feature Method in the ordinal_features parameter (11) don't match with the number of classes in the data (5).
2025-02-26 14:10:32,714:INFO:Set up encoding of categorical features.
2025-02-26 14:10:32,714:INFO:Set up polynomial features.
2025-02-26 14:10:32,714:INFO:Set up removing multicollinearity.
2025-02-26 14:10:32,714:INFO:Set up feature normalization.
2025-02-26 14:10:32,714:INFO:Set up feature selection.
2025-02-26 14:10:32,762:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-26 14:10:32,763:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-26 14:10:33,118:INFO:Finished creating preprocessing pipeline.
2025-02-26 14:10:33,151:INFO:Pipeline: Pipeline(memory=FastMemory(location=/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/joblib),
         steps=[('date_feature_extractor',
                 TransformerWrapper(include=['Date'],
                                    transformer=ExtractDateTimeFeatures())),
                ('iterative_imputer',
                 TransformerWrapper(transformer=IterativeImputer(cat_estimator=LGBMClassifier(n_jobs=-1,
                                                                                              random_state=123),
                                                                 cat_estimator_prepare_...
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('remove_multicollinearity',
                 TransformerWrapper(exclude=[],
                                    transformer=RemoveMulticollinearity(threshold=0.55))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('feature_selection',
                 TransformerWrapper(exclude=[],
                                    transformer=SelectFromModel(estimator=RandomForestRegressor(),
                                                                max_features=12,
                                                                threshold=-inf)))])
2025-02-26 14:10:33,151:INFO:Creating final display dataframe.
2025-02-26 14:10:34,376:INFO:Setup _display_container:                         Description          Value
0                        Session id            123
1                            Target          Price
2                       Target type     Regression
3               Original data shape    (13580, 21)
4            Transformed data shape    (13580, 13)
5       Transformed train set shape     (9506, 13)
6        Transformed test set shape     (4074, 13)
7                   Ignore features              4
8                  Ordinal features              3
9                  Numeric features              8
10                    Date features              1
11             Categorical features              6
12         Rows with missing values          54.4%
13                       Preprocess           True
14                  Imputation type      iterative
15  Iterative imputation iterations              5
16        Numeric iterative imputer       lightgbm
17    Categorical iterative imputer       lightgbm
18         Maximum one-hot encoding             25
19                  Encoding method           None
20              Polynomial features           True
21                Polynomial degree              2
22         Remove multicollinearity           True
23      Multicollinearity threshold           0.55
24                        Normalize           True
25                 Normalize method         zscore
26                Feature selection           True
27         Feature selection method        classic
28      Feature selection estimator             rf
29      Number of features selected            0.8
30                   Fold Generator          KFold
31                      Fold Number             10
32                         CPU Jobs             -1
33                          Use GPU          False
34                   Log Experiment  DagshubLogger
35                  Experiment Name  house_pricing
36                              USI           b658
2025-02-26 14:10:34,428:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-26 14:10:34,430:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-26 14:10:34,479:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-26 14:10:34,480:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-26 14:10:34,481:INFO:Logging experiment in loggers
2025-02-26 14:13:04,607:INFO:SubProcess save_model() called ==================================
2025-02-26 14:13:04,704:INFO:Initializing save_model()
2025-02-26 14:13:04,705:INFO:save_model(model=Pipeline(memory=FastMemory(location=/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/joblib),
         steps=[('date_feature_extractor',
                 TransformerWrapper(include=['Date'],
                                    transformer=ExtractDateTimeFeatures())),
                ('iterative_imputer',
                 TransformerWrapper(transformer=IterativeImputer(cat_estimator=LGBMClassifier(n_jobs=-1,
                                                                                              random_state=123),
                                                                 cat_estimator_prepare_...
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('remove_multicollinearity',
                 TransformerWrapper(exclude=[],
                                    transformer=RemoveMulticollinearity(threshold=0.55))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('feature_selection',
                 TransformerWrapper(exclude=[],
                                    transformer=SelectFromModel(estimator=RandomForestRegressor(),
                                                                max_features=12,
                                                                threshold=-inf)))]), model_name=/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/tmp2q2p3csh/Transformation Pipeline, prep_pipe_=Pipeline(memory=FastMemory(location=/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/joblib),
         steps=[('date_feature_extractor',
                 TransformerWrapper(include=['Date'],
                                    transformer=ExtractDateTimeFeatures())),
                ('iterative_imputer',
                 TransformerWrapper(transformer=IterativeImputer(cat_estimator=LGBMClassifier(n_jobs=-1,
                                                                                              random_state=123),
                                                                 cat_estimator_prepare_...
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('remove_multicollinearity',
                 TransformerWrapper(exclude=[],
                                    transformer=RemoveMulticollinearity(threshold=0.55))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('feature_selection',
                 TransformerWrapper(exclude=[],
                                    transformer=SelectFromModel(estimator=RandomForestRegressor(),
                                                                max_features=12,
                                                                threshold=-inf)))]), verbose=False, use_case=MLUsecase.REGRESSION, kwargs={})
2025-02-26 14:13:04,705:INFO:Adding model into prep_pipe
2025-02-26 14:13:04,705:WARNING:Only Model saved as it was a pipeline.
2025-02-26 14:13:04,922:INFO:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/tmp2q2p3csh/Transformation Pipeline.pkl saved in current working directory
2025-02-26 14:13:04,956:INFO:Pipeline(memory=FastMemory(location=/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/joblib),
         steps=[('date_feature_extractor',
                 TransformerWrapper(include=['Date'],
                                    transformer=ExtractDateTimeFeatures())),
                ('iterative_imputer',
                 TransformerWrapper(transformer=IterativeImputer(cat_estimator=LGBMClassifier(n_jobs=-1,
                                                                                              random_state=123),
                                                                 cat_estimator_prepare_...
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('remove_multicollinearity',
                 TransformerWrapper(exclude=[],
                                    transformer=RemoveMulticollinearity(threshold=0.55))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('feature_selection',
                 TransformerWrapper(exclude=[],
                                    transformer=SelectFromModel(estimator=RandomForestRegressor(),
                                                                max_features=12,
                                                                threshold=-inf)))])
2025-02-26 14:13:04,956:INFO:save_model() successfully completed......................................
2025-02-26 14:13:05,037:INFO:SubProcess save_model() end ==================================
2025-02-26 14:13:16,489:INFO:setup() successfully completed in 3.84s...............
2025-02-26 14:13:16,756:INFO:Initializing compare_models()
2025-02-26 14:13:16,757:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x2822888e0>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x2822888e0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2025-02-26 14:13:16,757:INFO:Checking exceptions
2025-02-26 14:13:16,759:INFO:Preparing display monitor
2025-02-26 14:13:16,794:INFO:Initializing Linear Regression
2025-02-26 14:13:16,794:INFO:Total runtime is 5.718072255452474e-06 minutes
2025-02-26 14:13:16,797:INFO:SubProcess create_model() called ==================================
2025-02-26 14:13:16,797:INFO:Initializing create_model()
2025-02-26 14:13:16,797:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x2822888e0>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x29abe3280>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-26 14:13:16,797:INFO:Checking exceptions
2025-02-26 14:13:16,797:INFO:Importing libraries
2025-02-26 14:13:16,797:INFO:Copying training dataset
2025-02-26 14:13:16,807:INFO:Defining folds
2025-02-26 14:13:16,807:INFO:Declaring metric variables
2025-02-26 14:13:16,809:INFO:Importing untrained model
2025-02-26 14:13:16,811:INFO:Linear Regression Imported successfully
2025-02-26 14:13:16,815:INFO:Starting cross validation
2025-02-26 14:13:16,914:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-26 14:14:02,203:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:14:02,277:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:14:02,896:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:14:03,001:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:14:03,234:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:14:03,295:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:14:03,388:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:14:03,518:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:14:28,861:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:14:29,121:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:14:43,494:INFO:Calculating mean and std
2025-02-26 14:14:43,497:INFO:Creating metrics dataframe
2025-02-26 14:14:43,510:INFO:Uploading results into container
2025-02-26 14:14:43,511:INFO:Uploading model into container now
2025-02-26 14:14:43,511:INFO:_master_model_container: 1
2025-02-26 14:14:43,511:INFO:_display_container: 2
2025-02-26 14:14:43,511:INFO:LinearRegression(n_jobs=-1)
2025-02-26 14:14:43,511:INFO:create_model() successfully completed......................................
2025-02-26 14:14:43,615:INFO:SubProcess create_model() end ==================================
2025-02-26 14:14:43,615:INFO:Creating metrics dataframe
2025-02-26 14:14:43,619:INFO:Initializing Lasso Regression
2025-02-26 14:14:43,619:INFO:Total runtime is 1.447088118394216 minutes
2025-02-26 14:14:43,620:INFO:SubProcess create_model() called ==================================
2025-02-26 14:14:43,621:INFO:Initializing create_model()
2025-02-26 14:14:43,621:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x2822888e0>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x29abe3280>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-26 14:14:43,621:INFO:Checking exceptions
2025-02-26 14:14:43,621:INFO:Importing libraries
2025-02-26 14:14:43,621:INFO:Copying training dataset
2025-02-26 14:14:43,628:INFO:Defining folds
2025-02-26 14:14:43,628:INFO:Declaring metric variables
2025-02-26 14:14:43,630:INFO:Importing untrained model
2025-02-26 14:14:43,632:INFO:Lasso Regression Imported successfully
2025-02-26 14:14:43,636:INFO:Starting cross validation
2025-02-26 14:14:43,753:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-26 14:15:26,503:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:15:26,506:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:15:26,690:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:15:26,906:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:15:26,985:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:15:27,470:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:15:27,481:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:15:27,626:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:15:46,403:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.382e+12, tolerance: 3.335e+11
  model = cd_fast.enet_coordinate_descent(

2025-02-26 14:15:46,768:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.076e+12, tolerance: 3.332e+11
  model = cd_fast.enet_coordinate_descent(

2025-02-26 14:15:47,099:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.236e+13, tolerance: 3.361e+11
  model = cd_fast.enet_coordinate_descent(

2025-02-26 14:15:47,337:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.665e+12, tolerance: 3.411e+11
  model = cd_fast.enet_coordinate_descent(

2025-02-26 14:15:47,451:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.696e+12, tolerance: 3.306e+11
  model = cd_fast.enet_coordinate_descent(

2025-02-26 14:15:47,451:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.795e+12, tolerance: 3.340e+11
  model = cd_fast.enet_coordinate_descent(

2025-02-26 14:15:47,577:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.001e+13, tolerance: 3.323e+11
  model = cd_fast.enet_coordinate_descent(

2025-02-26 14:15:47,647:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.356e+12, tolerance: 3.328e+11
  model = cd_fast.enet_coordinate_descent(

2025-02-26 14:15:56,053:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:15:56,082:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:16:10,456:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.994e+12, tolerance: 3.363e+11
  model = cd_fast.enet_coordinate_descent(

2025-02-26 14:16:10,556:INFO:Calculating mean and std
2025-02-26 14:16:10,558:INFO:Creating metrics dataframe
2025-02-26 14:16:10,569:INFO:Uploading results into container
2025-02-26 14:16:10,570:INFO:Uploading model into container now
2025-02-26 14:16:10,570:INFO:_master_model_container: 2
2025-02-26 14:16:10,570:INFO:_display_container: 2
2025-02-26 14:16:10,571:INFO:Lasso(random_state=123)
2025-02-26 14:16:10,571:INFO:create_model() successfully completed......................................
2025-02-26 14:16:10,689:INFO:SubProcess create_model() end ==================================
2025-02-26 14:16:10,689:INFO:Creating metrics dataframe
2025-02-26 14:16:10,693:INFO:Initializing Ridge Regression
2025-02-26 14:16:10,693:INFO:Total runtime is 2.898323400815328 minutes
2025-02-26 14:16:10,694:INFO:SubProcess create_model() called ==================================
2025-02-26 14:16:10,695:INFO:Initializing create_model()
2025-02-26 14:16:10,695:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x2822888e0>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x29abe3280>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-26 14:16:10,695:INFO:Checking exceptions
2025-02-26 14:16:10,695:INFO:Importing libraries
2025-02-26 14:16:10,695:INFO:Copying training dataset
2025-02-26 14:16:10,702:INFO:Defining folds
2025-02-26 14:16:10,702:INFO:Declaring metric variables
2025-02-26 14:16:10,704:INFO:Importing untrained model
2025-02-26 14:16:10,706:INFO:Ridge Regression Imported successfully
2025-02-26 14:16:10,709:INFO:Starting cross validation
2025-02-26 14:16:10,808:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-26 14:16:49,781:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:16:49,826:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:16:50,358:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:16:50,364:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:16:50,407:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:16:50,610:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:16:50,786:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:16:50,935:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:17:23,660:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:17:23,773:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:17:38,562:INFO:Calculating mean and std
2025-02-26 14:17:38,565:INFO:Creating metrics dataframe
2025-02-26 14:17:38,583:INFO:Uploading results into container
2025-02-26 14:17:38,584:INFO:Uploading model into container now
2025-02-26 14:17:38,584:INFO:_master_model_container: 3
2025-02-26 14:17:38,584:INFO:_display_container: 2
2025-02-26 14:17:38,585:INFO:Ridge(random_state=123)
2025-02-26 14:17:38,585:INFO:create_model() successfully completed......................................
2025-02-26 14:17:38,713:INFO:SubProcess create_model() end ==================================
2025-02-26 14:17:38,713:INFO:Creating metrics dataframe
2025-02-26 14:17:38,718:INFO:Initializing Elastic Net
2025-02-26 14:17:38,718:INFO:Total runtime is 4.365407065550486 minutes
2025-02-26 14:17:38,720:INFO:SubProcess create_model() called ==================================
2025-02-26 14:17:38,720:INFO:Initializing create_model()
2025-02-26 14:17:38,721:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x2822888e0>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x29abe3280>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-26 14:17:38,721:INFO:Checking exceptions
2025-02-26 14:17:38,721:INFO:Importing libraries
2025-02-26 14:17:38,721:INFO:Copying training dataset
2025-02-26 14:17:38,727:INFO:Defining folds
2025-02-26 14:17:38,727:INFO:Declaring metric variables
2025-02-26 14:17:38,730:INFO:Importing untrained model
2025-02-26 14:17:38,732:INFO:Elastic Net Imported successfully
2025-02-26 14:17:38,736:INFO:Starting cross validation
2025-02-26 14:17:38,868:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-26 14:18:23,399:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:18:24,219:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:18:24,597:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:18:25,125:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:18:25,210:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:18:25,252:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:18:25,482:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:18:25,642:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:18:56,319:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:18:56,358:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:19:11,165:INFO:Calculating mean and std
2025-02-26 14:19:11,166:INFO:Creating metrics dataframe
2025-02-26 14:19:11,177:INFO:Uploading results into container
2025-02-26 14:19:11,178:INFO:Uploading model into container now
2025-02-26 14:19:11,179:INFO:_master_model_container: 4
2025-02-26 14:19:11,179:INFO:_display_container: 2
2025-02-26 14:19:11,179:INFO:ElasticNet(random_state=123)
2025-02-26 14:19:11,179:INFO:create_model() successfully completed......................................
2025-02-26 14:19:11,266:INFO:SubProcess create_model() end ==================================
2025-02-26 14:19:11,266:INFO:Creating metrics dataframe
2025-02-26 14:19:11,269:INFO:Initializing Least Angle Regression
2025-02-26 14:19:11,270:INFO:Total runtime is 5.9079331159591675 minutes
2025-02-26 14:19:11,271:INFO:SubProcess create_model() called ==================================
2025-02-26 14:19:11,272:INFO:Initializing create_model()
2025-02-26 14:19:11,272:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x2822888e0>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x29abe3280>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-26 14:19:11,272:INFO:Checking exceptions
2025-02-26 14:19:11,272:INFO:Importing libraries
2025-02-26 14:19:11,272:INFO:Copying training dataset
2025-02-26 14:19:11,278:INFO:Defining folds
2025-02-26 14:19:11,279:INFO:Declaring metric variables
2025-02-26 14:19:11,281:INFO:Importing untrained model
2025-02-26 14:19:11,283:INFO:Least Angle Regression Imported successfully
2025-02-26 14:19:11,286:INFO:Starting cross validation
2025-02-26 14:19:11,387:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-26 14:19:54,405:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:19:54,731:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:19:55,105:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:19:55,113:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:19:55,299:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:19:55,372:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:19:55,556:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:19:56,154:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:20:26,196:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:20:26,648:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:20:41,435:INFO:Calculating mean and std
2025-02-26 14:20:41,437:INFO:Creating metrics dataframe
2025-02-26 14:20:41,453:INFO:Uploading results into container
2025-02-26 14:20:41,453:INFO:Uploading model into container now
2025-02-26 14:20:41,454:INFO:_master_model_container: 5
2025-02-26 14:20:41,454:INFO:_display_container: 2
2025-02-26 14:20:41,454:INFO:Lars(random_state=123)
2025-02-26 14:20:41,454:INFO:create_model() successfully completed......................................
2025-02-26 14:20:41,547:INFO:SubProcess create_model() end ==================================
2025-02-26 14:20:41,547:INFO:Creating metrics dataframe
2025-02-26 14:20:41,551:INFO:Initializing Lasso Least Angle Regression
2025-02-26 14:20:41,552:INFO:Total runtime is 7.412632052103678 minutes
2025-02-26 14:20:41,554:INFO:SubProcess create_model() called ==================================
2025-02-26 14:20:41,554:INFO:Initializing create_model()
2025-02-26 14:20:41,554:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x2822888e0>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x29abe3280>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-26 14:20:41,554:INFO:Checking exceptions
2025-02-26 14:20:41,554:INFO:Importing libraries
2025-02-26 14:20:41,554:INFO:Copying training dataset
2025-02-26 14:20:41,560:INFO:Defining folds
2025-02-26 14:20:41,561:INFO:Declaring metric variables
2025-02-26 14:20:41,562:INFO:Importing untrained model
2025-02-26 14:20:41,564:INFO:Lasso Least Angle Regression Imported successfully
2025-02-26 14:20:41,569:INFO:Starting cross validation
2025-02-26 14:20:41,675:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-26 14:21:26,047:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:21:26,103:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:21:26,223:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:21:26,332:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:21:26,627:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:21:26,733:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:21:26,790:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:21:26,915:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:21:58,347:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:21:58,607:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:22:13,260:INFO:Calculating mean and std
2025-02-26 14:22:13,263:INFO:Creating metrics dataframe
2025-02-26 14:22:13,279:INFO:Uploading results into container
2025-02-26 14:22:13,279:INFO:Uploading model into container now
2025-02-26 14:22:13,280:INFO:_master_model_container: 6
2025-02-26 14:22:13,280:INFO:_display_container: 2
2025-02-26 14:22:13,280:INFO:LassoLars(random_state=123)
2025-02-26 14:22:13,280:INFO:create_model() successfully completed......................................
2025-02-26 14:22:13,386:INFO:SubProcess create_model() end ==================================
2025-02-26 14:22:13,386:INFO:Creating metrics dataframe
2025-02-26 14:22:13,390:INFO:Initializing Orthogonal Matching Pursuit
2025-02-26 14:22:13,391:INFO:Total runtime is 8.943282953898112 minutes
2025-02-26 14:22:13,393:INFO:SubProcess create_model() called ==================================
2025-02-26 14:22:13,393:INFO:Initializing create_model()
2025-02-26 14:22:13,393:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x2822888e0>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x29abe3280>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-26 14:22:13,393:INFO:Checking exceptions
2025-02-26 14:22:13,394:INFO:Importing libraries
2025-02-26 14:22:13,394:INFO:Copying training dataset
2025-02-26 14:22:13,399:INFO:Defining folds
2025-02-26 14:22:13,400:INFO:Declaring metric variables
2025-02-26 14:22:13,402:INFO:Importing untrained model
2025-02-26 14:22:13,404:INFO:Orthogonal Matching Pursuit Imported successfully
2025-02-26 14:22:13,407:INFO:Starting cross validation
2025-02-26 14:22:13,578:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-26 14:22:56,405:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:22:56,859:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:22:56,938:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:22:57,172:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:22:57,189:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:22:57,410:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:22:57,560:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:22:57,907:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:23:29,232:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:23:29,869:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:23:44,442:INFO:Calculating mean and std
2025-02-26 14:23:44,443:INFO:Creating metrics dataframe
2025-02-26 14:23:44,459:INFO:Uploading results into container
2025-02-26 14:23:44,460:INFO:Uploading model into container now
2025-02-26 14:23:44,460:INFO:_master_model_container: 7
2025-02-26 14:23:44,460:INFO:_display_container: 2
2025-02-26 14:23:44,460:INFO:OrthogonalMatchingPursuit()
2025-02-26 14:23:44,460:INFO:create_model() successfully completed......................................
2025-02-26 14:23:44,570:INFO:SubProcess create_model() end ==================================
2025-02-26 14:23:44,570:INFO:Creating metrics dataframe
2025-02-26 14:23:44,574:INFO:Initializing Bayesian Ridge
2025-02-26 14:23:44,574:INFO:Total runtime is 10.463011852900188 minutes
2025-02-26 14:23:44,576:INFO:SubProcess create_model() called ==================================
2025-02-26 14:23:44,577:INFO:Initializing create_model()
2025-02-26 14:23:44,577:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x2822888e0>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x29abe3280>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-26 14:23:44,577:INFO:Checking exceptions
2025-02-26 14:23:44,577:INFO:Importing libraries
2025-02-26 14:23:44,577:INFO:Copying training dataset
2025-02-26 14:23:44,587:INFO:Defining folds
2025-02-26 14:23:44,588:INFO:Declaring metric variables
2025-02-26 14:23:44,590:INFO:Importing untrained model
2025-02-26 14:23:44,594:INFO:Bayesian Ridge Imported successfully
2025-02-26 14:23:44,597:INFO:Starting cross validation
2025-02-26 14:23:44,716:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-26 14:24:29,150:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:24:29,270:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:24:29,404:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:24:29,547:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:24:29,687:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:24:29,743:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:24:29,836:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:24:30,042:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:25:01,653:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:25:01,825:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:25:16,616:INFO:Calculating mean and std
2025-02-26 14:25:16,619:INFO:Creating metrics dataframe
2025-02-26 14:25:16,637:INFO:Uploading results into container
2025-02-26 14:25:16,637:INFO:Uploading model into container now
2025-02-26 14:25:16,638:INFO:_master_model_container: 8
2025-02-26 14:25:16,638:INFO:_display_container: 2
2025-02-26 14:25:16,638:INFO:BayesianRidge()
2025-02-26 14:25:16,638:INFO:create_model() successfully completed......................................
2025-02-26 14:25:16,741:INFO:SubProcess create_model() end ==================================
2025-02-26 14:25:16,742:INFO:Creating metrics dataframe
2025-02-26 14:25:16,746:INFO:Initializing Passive Aggressive Regressor
2025-02-26 14:25:16,746:INFO:Total runtime is 11.999209400018056 minutes
2025-02-26 14:25:16,748:INFO:SubProcess create_model() called ==================================
2025-02-26 14:25:16,748:INFO:Initializing create_model()
2025-02-26 14:25:16,748:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x2822888e0>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x29abe3280>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-26 14:25:16,748:INFO:Checking exceptions
2025-02-26 14:25:16,749:INFO:Importing libraries
2025-02-26 14:25:16,749:INFO:Copying training dataset
2025-02-26 14:25:16,757:INFO:Defining folds
2025-02-26 14:25:16,758:INFO:Declaring metric variables
2025-02-26 14:25:16,760:INFO:Importing untrained model
2025-02-26 14:25:16,761:INFO:Passive Aggressive Regressor Imported successfully
2025-02-26 14:25:16,765:INFO:Starting cross validation
2025-02-26 14:25:16,896:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-26 14:26:00,908:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:26:01,366:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:26:01,464:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:26:01,483:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:26:01,834:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:26:02,026:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:26:02,368:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:26:02,479:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:26:23,283:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py:1575: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
  warnings.warn(

2025-02-26 14:26:24,026:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py:1575: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
  warnings.warn(

2025-02-26 14:26:24,031:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py:1575: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
  warnings.warn(

2025-02-26 14:26:24,076:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py:1575: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
  warnings.warn(

2025-02-26 14:26:24,113:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py:1575: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
  warnings.warn(

2025-02-26 14:26:24,393:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py:1575: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
  warnings.warn(

2025-02-26 14:26:24,549:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py:1575: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
  warnings.warn(

2025-02-26 14:26:24,653:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py:1575: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
  warnings.warn(

2025-02-26 14:26:34,022:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:26:34,278:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:26:48,772:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py:1575: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
  warnings.warn(

2025-02-26 14:26:49,211:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py:1575: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
  warnings.warn(

2025-02-26 14:26:49,304:INFO:Calculating mean and std
2025-02-26 14:26:49,305:INFO:Creating metrics dataframe
2025-02-26 14:26:49,321:INFO:Uploading results into container
2025-02-26 14:26:49,321:INFO:Uploading model into container now
2025-02-26 14:26:49,322:INFO:_master_model_container: 9
2025-02-26 14:26:49,322:INFO:_display_container: 2
2025-02-26 14:26:49,322:INFO:PassiveAggressiveRegressor(random_state=123)
2025-02-26 14:26:49,322:INFO:create_model() successfully completed......................................
2025-02-26 14:26:49,417:INFO:SubProcess create_model() end ==================================
2025-02-26 14:26:49,417:INFO:Creating metrics dataframe
2025-02-26 14:26:49,422:INFO:Initializing Huber Regressor
2025-02-26 14:26:49,422:INFO:Total runtime is 13.54380871852239 minutes
2025-02-26 14:26:49,424:INFO:SubProcess create_model() called ==================================
2025-02-26 14:26:49,424:INFO:Initializing create_model()
2025-02-26 14:26:49,424:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x2822888e0>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x29abe3280>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-26 14:26:49,424:INFO:Checking exceptions
2025-02-26 14:26:49,424:INFO:Importing libraries
2025-02-26 14:26:49,424:INFO:Copying training dataset
2025-02-26 14:26:49,430:INFO:Defining folds
2025-02-26 14:26:49,430:INFO:Declaring metric variables
2025-02-26 14:26:49,432:INFO:Importing untrained model
2025-02-26 14:26:49,434:INFO:Huber Regressor Imported successfully
2025-02-26 14:26:49,438:INFO:Starting cross validation
2025-02-26 14:26:49,546:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-26 14:27:34,531:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:27:34,849:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:27:34,977:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:27:35,211:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:27:35,510:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:27:35,680:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:27:35,696:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:27:35,729:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:28:04,095:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:28:04,203:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:28:18,302:INFO:Calculating mean and std
2025-02-26 14:28:18,303:INFO:Creating metrics dataframe
2025-02-26 14:28:18,320:INFO:Uploading results into container
2025-02-26 14:28:18,321:INFO:Uploading model into container now
2025-02-26 14:28:18,321:INFO:_master_model_container: 10
2025-02-26 14:28:18,321:INFO:_display_container: 2
2025-02-26 14:28:18,322:INFO:HuberRegressor()
2025-02-26 14:28:18,322:INFO:create_model() successfully completed......................................
2025-02-26 14:28:18,418:INFO:SubProcess create_model() end ==================================
2025-02-26 14:28:18,418:INFO:Creating metrics dataframe
2025-02-26 14:28:18,422:INFO:Initializing K Neighbors Regressor
2025-02-26 14:28:18,422:INFO:Total runtime is 15.027142083644868 minutes
2025-02-26 14:28:18,424:INFO:SubProcess create_model() called ==================================
2025-02-26 14:28:18,424:INFO:Initializing create_model()
2025-02-26 14:28:18,424:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x2822888e0>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x29abe3280>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-26 14:28:18,424:INFO:Checking exceptions
2025-02-26 14:28:18,425:INFO:Importing libraries
2025-02-26 14:28:18,425:INFO:Copying training dataset
2025-02-26 14:28:18,430:INFO:Defining folds
2025-02-26 14:28:18,431:INFO:Declaring metric variables
2025-02-26 14:28:18,432:INFO:Importing untrained model
2025-02-26 14:28:18,434:INFO:K Neighbors Regressor Imported successfully
2025-02-26 14:28:18,436:INFO:Starting cross validation
2025-02-26 14:28:18,583:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-26 14:28:57,480:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:28:57,897:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:28:58,107:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:28:58,362:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:28:58,523:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:28:58,613:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:28:58,711:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:28:58,864:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:29:24,627:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:29:24,974:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:29:38,903:INFO:Calculating mean and std
2025-02-26 14:29:38,904:INFO:Creating metrics dataframe
2025-02-26 14:29:38,917:INFO:Uploading results into container
2025-02-26 14:29:38,917:INFO:Uploading model into container now
2025-02-26 14:29:38,917:INFO:_master_model_container: 11
2025-02-26 14:29:38,917:INFO:_display_container: 2
2025-02-26 14:29:38,918:INFO:KNeighborsRegressor(n_jobs=-1)
2025-02-26 14:29:38,918:INFO:create_model() successfully completed......................................
2025-02-26 14:29:39,007:INFO:SubProcess create_model() end ==================================
2025-02-26 14:29:39,007:INFO:Creating metrics dataframe
2025-02-26 14:29:39,012:INFO:Initializing Decision Tree Regressor
2025-02-26 14:29:39,012:INFO:Total runtime is 16.37030215263367 minutes
2025-02-26 14:29:39,013:INFO:SubProcess create_model() called ==================================
2025-02-26 14:29:39,014:INFO:Initializing create_model()
2025-02-26 14:29:39,014:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x2822888e0>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x29abe3280>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-26 14:29:39,014:INFO:Checking exceptions
2025-02-26 14:29:39,014:INFO:Importing libraries
2025-02-26 14:29:39,014:INFO:Copying training dataset
2025-02-26 14:29:39,021:INFO:Defining folds
2025-02-26 14:29:39,021:INFO:Declaring metric variables
2025-02-26 14:29:39,022:INFO:Importing untrained model
2025-02-26 14:29:39,024:INFO:Decision Tree Regressor Imported successfully
2025-02-26 14:29:39,027:INFO:Starting cross validation
2025-02-26 14:29:39,133:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-26 14:30:22,327:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:30:22,563:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:30:23,150:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:30:23,176:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:30:23,498:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:30:23,623:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:30:23,772:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:30:23,822:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:30:54,929:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:30:55,150:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:31:09,861:INFO:Calculating mean and std
2025-02-26 14:31:09,863:INFO:Creating metrics dataframe
2025-02-26 14:31:09,882:INFO:Uploading results into container
2025-02-26 14:31:09,882:INFO:Uploading model into container now
2025-02-26 14:31:09,883:INFO:_master_model_container: 12
2025-02-26 14:31:09,883:INFO:_display_container: 2
2025-02-26 14:31:09,883:INFO:DecisionTreeRegressor(random_state=123)
2025-02-26 14:31:09,883:INFO:create_model() successfully completed......................................
2025-02-26 14:31:09,987:INFO:SubProcess create_model() end ==================================
2025-02-26 14:31:09,987:INFO:Creating metrics dataframe
2025-02-26 14:31:09,991:INFO:Initializing Random Forest Regressor
2025-02-26 14:31:09,992:INFO:Total runtime is 17.886632637182874 minutes
2025-02-26 14:31:09,993:INFO:SubProcess create_model() called ==================================
2025-02-26 14:31:09,994:INFO:Initializing create_model()
2025-02-26 14:31:09,994:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x2822888e0>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x29abe3280>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-26 14:31:09,994:INFO:Checking exceptions
2025-02-26 14:31:09,994:INFO:Importing libraries
2025-02-26 14:31:09,994:INFO:Copying training dataset
2025-02-26 14:31:10,002:INFO:Defining folds
2025-02-26 14:31:10,002:INFO:Declaring metric variables
2025-02-26 14:31:10,004:INFO:Importing untrained model
2025-02-26 14:31:10,006:INFO:Random Forest Regressor Imported successfully
2025-02-26 14:31:10,010:INFO:Starting cross validation
2025-02-26 14:31:10,141:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-26 14:31:53,435:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:31:53,588:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:31:53,703:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:31:54,161:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:31:54,334:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:31:54,356:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:31:54,358:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:31:54,878:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:32:35,018:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:32:35,660:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:32:52,363:INFO:Calculating mean and std
2025-02-26 14:32:52,365:INFO:Creating metrics dataframe
2025-02-26 14:32:52,387:INFO:Uploading results into container
2025-02-26 14:32:52,388:INFO:Uploading model into container now
2025-02-26 14:32:52,388:INFO:_master_model_container: 13
2025-02-26 14:32:52,388:INFO:_display_container: 2
2025-02-26 14:32:52,389:INFO:RandomForestRegressor(n_jobs=-1, random_state=123)
2025-02-26 14:32:52,389:INFO:create_model() successfully completed......................................
2025-02-26 14:32:52,505:INFO:SubProcess create_model() end ==================================
2025-02-26 14:32:52,506:INFO:Creating metrics dataframe
2025-02-26 14:32:52,511:INFO:Initializing Extra Trees Regressor
2025-02-26 14:32:52,511:INFO:Total runtime is 19.595293517907464 minutes
2025-02-26 14:32:52,513:INFO:SubProcess create_model() called ==================================
2025-02-26 14:32:52,514:INFO:Initializing create_model()
2025-02-26 14:32:52,514:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x2822888e0>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x29abe3280>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-26 14:32:52,514:INFO:Checking exceptions
2025-02-26 14:32:52,514:INFO:Importing libraries
2025-02-26 14:32:52,514:INFO:Copying training dataset
2025-02-26 14:32:52,521:INFO:Defining folds
2025-02-26 14:32:52,521:INFO:Declaring metric variables
2025-02-26 14:32:52,523:INFO:Importing untrained model
2025-02-26 14:32:52,525:INFO:Extra Trees Regressor Imported successfully
2025-02-26 14:32:52,529:INFO:Starting cross validation
2025-02-26 14:32:52,654:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-26 14:33:35,786:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:33:35,953:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:33:36,017:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:33:36,393:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:33:36,619:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:33:36,703:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:33:36,902:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:33:37,120:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:34:10,148:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:34:10,879:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:34:25,678:INFO:Calculating mean and std
2025-02-26 14:34:25,681:INFO:Creating metrics dataframe
2025-02-26 14:34:25,701:INFO:Uploading results into container
2025-02-26 14:34:25,702:INFO:Uploading model into container now
2025-02-26 14:34:25,703:INFO:_master_model_container: 14
2025-02-26 14:34:25,703:INFO:_display_container: 2
2025-02-26 14:34:25,703:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=123)
2025-02-26 14:34:25,703:INFO:create_model() successfully completed......................................
2025-02-26 14:34:25,838:INFO:SubProcess create_model() end ==================================
2025-02-26 14:34:25,838:INFO:Creating metrics dataframe
2025-02-26 14:34:25,843:INFO:Initializing AdaBoost Regressor
2025-02-26 14:34:25,843:INFO:Total runtime is 21.150831917921707 minutes
2025-02-26 14:34:25,845:INFO:SubProcess create_model() called ==================================
2025-02-26 14:34:25,845:INFO:Initializing create_model()
2025-02-26 14:34:25,846:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x2822888e0>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x29abe3280>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-26 14:34:25,846:INFO:Checking exceptions
2025-02-26 14:34:25,846:INFO:Importing libraries
2025-02-26 14:34:25,846:INFO:Copying training dataset
2025-02-26 14:34:25,854:INFO:Defining folds
2025-02-26 14:34:25,854:INFO:Declaring metric variables
2025-02-26 14:34:25,855:INFO:Importing untrained model
2025-02-26 14:34:25,864:INFO:AdaBoost Regressor Imported successfully
2025-02-26 14:34:25,896:INFO:Starting cross validation
2025-02-26 14:34:26,051:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-26 14:35:04,622:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:35:04,670:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:35:04,855:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:35:05,100:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:35:05,129:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:35:05,389:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:35:05,646:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:35:06,178:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:35:37,853:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:35:38,069:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:35:53,242:INFO:Calculating mean and std
2025-02-26 14:35:53,243:INFO:Creating metrics dataframe
2025-02-26 14:35:53,255:INFO:Uploading results into container
2025-02-26 14:35:53,256:INFO:Uploading model into container now
2025-02-26 14:35:53,256:INFO:_master_model_container: 15
2025-02-26 14:35:53,256:INFO:_display_container: 2
2025-02-26 14:35:53,257:INFO:AdaBoostRegressor(random_state=123)
2025-02-26 14:35:53,257:INFO:create_model() successfully completed......................................
2025-02-26 14:35:53,367:INFO:SubProcess create_model() end ==================================
2025-02-26 14:35:53,367:INFO:Creating metrics dataframe
2025-02-26 14:35:53,373:INFO:Initializing Gradient Boosting Regressor
2025-02-26 14:35:53,373:INFO:Total runtime is 22.60965523719788 minutes
2025-02-26 14:35:53,374:INFO:SubProcess create_model() called ==================================
2025-02-26 14:35:53,375:INFO:Initializing create_model()
2025-02-26 14:35:53,375:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x2822888e0>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x29abe3280>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-26 14:35:53,375:INFO:Checking exceptions
2025-02-26 14:35:53,375:INFO:Importing libraries
2025-02-26 14:35:53,375:INFO:Copying training dataset
2025-02-26 14:35:53,382:INFO:Defining folds
2025-02-26 14:35:53,382:INFO:Declaring metric variables
2025-02-26 14:35:53,384:INFO:Importing untrained model
2025-02-26 14:35:53,386:INFO:Gradient Boosting Regressor Imported successfully
2025-02-26 14:35:53,391:INFO:Starting cross validation
2025-02-26 14:35:53,500:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-26 14:36:36,039:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:36:36,613:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:36:36,968:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:36:37,277:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:36:37,286:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:36:37,662:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:36:37,824:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:36:37,939:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:37:13,328:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:37:13,355:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:37:29,914:INFO:Calculating mean and std
2025-02-26 14:37:29,915:INFO:Creating metrics dataframe
2025-02-26 14:37:29,921:INFO:Uploading results into container
2025-02-26 14:37:29,921:INFO:Uploading model into container now
2025-02-26 14:37:29,922:INFO:_master_model_container: 16
2025-02-26 14:37:29,922:INFO:_display_container: 2
2025-02-26 14:37:29,922:INFO:GradientBoostingRegressor(random_state=123)
2025-02-26 14:37:29,922:INFO:create_model() successfully completed......................................
2025-02-26 14:37:30,005:INFO:SubProcess create_model() end ==================================
2025-02-26 14:37:30,005:INFO:Creating metrics dataframe
2025-02-26 14:37:30,010:INFO:Initializing Extreme Gradient Boosting
2025-02-26 14:37:30,010:INFO:Total runtime is 24.220271499951686 minutes
2025-02-26 14:37:30,012:INFO:SubProcess create_model() called ==================================
2025-02-26 14:37:30,012:INFO:Initializing create_model()
2025-02-26 14:37:30,012:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x2822888e0>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x29abe3280>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-26 14:37:30,012:INFO:Checking exceptions
2025-02-26 14:37:30,012:INFO:Importing libraries
2025-02-26 14:37:30,012:INFO:Copying training dataset
2025-02-26 14:37:30,018:INFO:Defining folds
2025-02-26 14:37:30,019:INFO:Declaring metric variables
2025-02-26 14:37:30,020:INFO:Importing untrained model
2025-02-26 14:37:30,022:INFO:Extreme Gradient Boosting Imported successfully
2025-02-26 14:37:30,024:INFO:Starting cross validation
2025-02-26 14:37:30,123:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-26 14:38:09,371:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:38:09,983:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:38:10,388:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:38:10,800:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:38:10,957:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:38:11,016:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:38:11,166:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:38:11,255:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:38:42,734:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:38:43,629:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:38:58,590:INFO:Calculating mean and std
2025-02-26 14:38:58,591:INFO:Creating metrics dataframe
2025-02-26 14:38:58,613:INFO:Uploading results into container
2025-02-26 14:38:58,613:INFO:Uploading model into container now
2025-02-26 14:38:58,613:INFO:_master_model_container: 17
2025-02-26 14:38:58,614:INFO:_display_container: 2
2025-02-26 14:38:58,614:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device='cpu', early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, random_state=123, ...)
2025-02-26 14:38:58,614:INFO:create_model() successfully completed......................................
2025-02-26 14:38:58,712:INFO:SubProcess create_model() end ==================================
2025-02-26 14:38:58,712:INFO:Creating metrics dataframe
2025-02-26 14:38:58,718:INFO:Initializing Light Gradient Boosting Machine
2025-02-26 14:38:58,718:INFO:Total runtime is 25.698741718133295 minutes
2025-02-26 14:38:58,720:INFO:SubProcess create_model() called ==================================
2025-02-26 14:38:58,721:INFO:Initializing create_model()
2025-02-26 14:38:58,721:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x2822888e0>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x29abe3280>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-26 14:38:58,721:INFO:Checking exceptions
2025-02-26 14:38:58,721:INFO:Importing libraries
2025-02-26 14:38:58,721:INFO:Copying training dataset
2025-02-26 14:38:58,727:INFO:Defining folds
2025-02-26 14:38:58,727:INFO:Declaring metric variables
2025-02-26 14:38:58,729:INFO:Importing untrained model
2025-02-26 14:38:58,731:INFO:Light Gradient Boosting Machine Imported successfully
2025-02-26 14:38:58,734:INFO:Starting cross validation
2025-02-26 14:38:58,843:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-26 14:39:43,231:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:39:43,707:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:39:44,205:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:39:44,265:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:39:44,620:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:39:44,744:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:39:44,806:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:39:44,983:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:40:12,962:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:40:13,031:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:40:27,624:INFO:Calculating mean and std
2025-02-26 14:40:27,625:INFO:Creating metrics dataframe
2025-02-26 14:40:27,641:INFO:Uploading results into container
2025-02-26 14:40:27,641:INFO:Uploading model into container now
2025-02-26 14:40:27,642:INFO:_master_model_container: 18
2025-02-26 14:40:27,642:INFO:_display_container: 2
2025-02-26 14:40:27,642:INFO:LGBMRegressor(n_jobs=-1, random_state=123)
2025-02-26 14:40:27,642:INFO:create_model() successfully completed......................................
2025-02-26 14:40:27,736:INFO:SubProcess create_model() end ==================================
2025-02-26 14:40:27,736:INFO:Creating metrics dataframe
2025-02-26 14:40:27,742:INFO:Initializing CatBoost Regressor
2025-02-26 14:40:27,742:INFO:Total runtime is 27.18247225284577 minutes
2025-02-26 14:40:27,743:INFO:SubProcess create_model() called ==================================
2025-02-26 14:40:27,744:INFO:Initializing create_model()
2025-02-26 14:40:27,744:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x2822888e0>, estimator=catboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x29abe3280>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-26 14:40:27,744:INFO:Checking exceptions
2025-02-26 14:40:27,744:INFO:Importing libraries
2025-02-26 14:40:27,745:INFO:Copying training dataset
2025-02-26 14:40:27,750:INFO:Defining folds
2025-02-26 14:40:27,751:INFO:Declaring metric variables
2025-02-26 14:40:27,753:INFO:Importing untrained model
2025-02-26 14:40:27,755:INFO:CatBoost Regressor Imported successfully
2025-02-26 14:40:27,758:INFO:Starting cross validation
2025-02-26 14:40:27,882:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-26 14:41:07,057:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:41:07,287:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:41:07,499:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:41:08,003:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:41:08,060:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:41:08,097:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:41:08,258:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:41:08,262:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:41:38,988:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:41:39,042:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:41:54,797:INFO:Calculating mean and std
2025-02-26 14:41:54,800:INFO:Creating metrics dataframe
2025-02-26 14:41:54,822:INFO:Uploading results into container
2025-02-26 14:41:54,823:INFO:Uploading model into container now
2025-02-26 14:41:54,823:INFO:_master_model_container: 19
2025-02-26 14:41:54,823:INFO:_display_container: 2
2025-02-26 14:41:54,823:INFO:<catboost.core.CatBoostRegressor object at 0x28c331c30>
2025-02-26 14:41:54,823:INFO:create_model() successfully completed......................................
2025-02-26 14:41:54,930:INFO:SubProcess create_model() end ==================================
2025-02-26 14:41:54,930:INFO:Creating metrics dataframe
2025-02-26 14:41:54,935:INFO:Initializing Dummy Regressor
2025-02-26 14:41:54,936:INFO:Total runtime is 28.635700150330866 minutes
2025-02-26 14:41:54,937:INFO:SubProcess create_model() called ==================================
2025-02-26 14:41:54,938:INFO:Initializing create_model()
2025-02-26 14:41:54,938:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x2822888e0>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x29abe3280>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-26 14:41:54,938:INFO:Checking exceptions
2025-02-26 14:41:54,938:INFO:Importing libraries
2025-02-26 14:41:54,939:INFO:Copying training dataset
2025-02-26 14:41:54,944:INFO:Defining folds
2025-02-26 14:41:54,944:INFO:Declaring metric variables
2025-02-26 14:41:54,946:INFO:Importing untrained model
2025-02-26 14:41:54,948:INFO:Dummy Regressor Imported successfully
2025-02-26 14:41:54,951:INFO:Starting cross validation
2025-02-26 14:41:55,050:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-26 14:42:32,818:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:42:33,015:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:42:33,665:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:42:33,731:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:42:33,878:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:42:33,921:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:42:34,154:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:42:34,166:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:43:00,627:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:43:00,741:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:43:14,948:INFO:Calculating mean and std
2025-02-26 14:43:14,949:INFO:Creating metrics dataframe
2025-02-26 14:43:14,971:INFO:Uploading results into container
2025-02-26 14:43:14,972:INFO:Uploading model into container now
2025-02-26 14:43:14,972:INFO:_master_model_container: 20
2025-02-26 14:43:14,972:INFO:_display_container: 2
2025-02-26 14:43:14,972:INFO:DummyRegressor()
2025-02-26 14:43:14,972:INFO:create_model() successfully completed......................................
2025-02-26 14:43:15,068:INFO:SubProcess create_model() end ==================================
2025-02-26 14:43:15,068:INFO:Creating metrics dataframe
2025-02-26 14:43:15,077:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/pycaret_experiment/supervised_experiment.py:323: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  master_display_.apply(

2025-02-26 14:43:15,082:INFO:Initializing create_model()
2025-02-26 14:43:15,082:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x2822888e0>, estimator=<catboost.core.CatBoostRegressor object at 0x28c331c30>, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-26 14:43:15,082:INFO:Checking exceptions
2025-02-26 14:43:15,083:INFO:Importing libraries
2025-02-26 14:43:15,083:INFO:Copying training dataset
2025-02-26 14:43:15,088:INFO:Defining folds
2025-02-26 14:43:15,089:INFO:Declaring metric variables
2025-02-26 14:43:15,089:INFO:Importing untrained model
2025-02-26 14:43:15,089:INFO:Declaring custom model
2025-02-26 14:43:15,089:INFO:CatBoost Regressor Imported successfully
2025-02-26 14:43:15,218:INFO:Cross validation set to False
2025-02-26 14:43:15,219:INFO:Fitting Model
2025-02-26 14:43:15,244:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 14:43:15,244:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 14:43:15,247:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000478 seconds.
2025-02-26 14:43:15,247:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 14:43:15,247:INFO:[LightGBM] [Info] Total Bins 1877
2025-02-26 14:43:15,247:INFO:[LightGBM] [Info] Number of data points in the train set: 9467, number of used features: 17
2025-02-26 14:43:15,248:INFO:[LightGBM] [Info] Start training from score 1.610964
2025-02-26 14:43:15,586:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 14:43:15,587:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 14:43:15,589:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000181 seconds.
2025-02-26 14:43:15,589:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 14:43:15,589:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 14:43:15,589:INFO:[LightGBM] [Info] Total Bins 1810
2025-02-26 14:43:15,589:INFO:[LightGBM] [Info] Number of data points in the train set: 8543, number of used features: 17
2025-02-26 14:43:15,589:INFO:[LightGBM] [Info] Start training from score 14.875219
2025-02-26 14:43:15,931:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 14:43:15,931:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 14:43:15,933:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000360 seconds.
2025-02-26 14:43:15,933:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 14:43:15,933:INFO:[LightGBM] [Info] Total Bins 1813
2025-02-26 14:43:15,933:INFO:[LightGBM] [Info] Number of data points in the train set: 5729, number of used features: 17
2025-02-26 14:43:15,933:INFO:[LightGBM] [Info] Start training from score 1965.296038
2025-02-26 14:43:16,236:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 14:43:16,236:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 14:43:16,239:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000232 seconds.
2025-02-26 14:43:16,239:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 14:43:16,239:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 14:43:16,239:INFO:[LightGBM] [Info] Total Bins 2245
2025-02-26 14:43:16,239:INFO:[LightGBM] [Info] Number of data points in the train set: 9467, number of used features: 17
2025-02-26 14:43:16,239:INFO:[LightGBM] [Info] Start training from score 1.610964
2025-02-26 14:43:16,613:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 14:43:16,613:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 14:43:16,615:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000408 seconds.
2025-02-26 14:43:16,615:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 14:43:16,616:INFO:[LightGBM] [Info] Total Bins 1962
2025-02-26 14:43:16,616:INFO:[LightGBM] [Info] Number of data points in the train set: 8543, number of used features: 17
2025-02-26 14:43:16,616:INFO:[LightGBM] [Info] Start training from score 14.875219
2025-02-26 14:43:16,918:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 14:43:16,918:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 14:43:16,919:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000212 seconds.
2025-02-26 14:43:16,919:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 14:43:16,919:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 14:43:16,919:INFO:[LightGBM] [Info] Total Bins 1812
2025-02-26 14:43:16,919:INFO:[LightGBM] [Info] Number of data points in the train set: 5729, number of used features: 17
2025-02-26 14:43:16,920:INFO:[LightGBM] [Info] Start training from score 1965.296038
2025-02-26 14:43:17,266:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 14:43:17,266:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 14:43:17,268:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000211 seconds.
2025-02-26 14:43:17,268:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 14:43:17,268:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 14:43:17,268:INFO:[LightGBM] [Info] Total Bins 2244
2025-02-26 14:43:17,268:INFO:[LightGBM] [Info] Number of data points in the train set: 9467, number of used features: 17
2025-02-26 14:43:17,268:INFO:[LightGBM] [Info] Start training from score 1.610964
2025-02-26 14:43:17,652:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 14:43:17,653:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 14:43:17,655:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000429 seconds.
2025-02-26 14:43:17,655:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 14:43:17,655:INFO:[LightGBM] [Info] Total Bins 1962
2025-02-26 14:43:17,655:INFO:[LightGBM] [Info] Number of data points in the train set: 8543, number of used features: 17
2025-02-26 14:43:17,655:INFO:[LightGBM] [Info] Start training from score 14.875219
2025-02-26 14:43:17,969:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 14:43:17,969:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 14:43:17,971:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000212 seconds.
2025-02-26 14:43:17,971:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 14:43:17,971:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 14:43:17,971:INFO:[LightGBM] [Info] Total Bins 1813
2025-02-26 14:43:17,971:INFO:[LightGBM] [Info] Number of data points in the train set: 5729, number of used features: 17
2025-02-26 14:43:17,972:INFO:[LightGBM] [Info] Start training from score 1965.296038
2025-02-26 14:43:18,301:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 14:43:18,301:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 14:43:18,304:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000230 seconds.
2025-02-26 14:43:18,304:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 14:43:18,304:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 14:43:18,304:INFO:[LightGBM] [Info] Total Bins 2243
2025-02-26 14:43:18,304:INFO:[LightGBM] [Info] Number of data points in the train set: 9467, number of used features: 17
2025-02-26 14:43:18,305:INFO:[LightGBM] [Info] Start training from score 1.610964
2025-02-26 14:43:18,665:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 14:43:18,665:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 14:43:18,667:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000443 seconds.
2025-02-26 14:43:18,667:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 14:43:18,668:INFO:[LightGBM] [Info] Total Bins 1961
2025-02-26 14:43:18,668:INFO:[LightGBM] [Info] Number of data points in the train set: 8543, number of used features: 17
2025-02-26 14:43:18,668:INFO:[LightGBM] [Info] Start training from score 14.875219
2025-02-26 14:43:18,975:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 14:43:18,976:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 14:43:18,977:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000346 seconds.
2025-02-26 14:43:18,977:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 14:43:18,977:INFO:[LightGBM] [Info] Total Bins 1813
2025-02-26 14:43:18,977:INFO:[LightGBM] [Info] Number of data points in the train set: 5729, number of used features: 17
2025-02-26 14:43:18,978:INFO:[LightGBM] [Info] Start training from score 1965.296038
2025-02-26 14:43:19,282:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 14:43:19,283:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 14:43:19,285:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000235 seconds.
2025-02-26 14:43:19,285:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 14:43:19,285:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 14:43:19,285:INFO:[LightGBM] [Info] Total Bins 2244
2025-02-26 14:43:19,285:INFO:[LightGBM] [Info] Number of data points in the train set: 9467, number of used features: 17
2025-02-26 14:43:19,286:INFO:[LightGBM] [Info] Start training from score 1.610964
2025-02-26 14:43:19,658:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 14:43:19,659:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 14:43:19,661:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000419 seconds.
2025-02-26 14:43:19,661:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 14:43:19,661:INFO:[LightGBM] [Info] Total Bins 1962
2025-02-26 14:43:19,661:INFO:[LightGBM] [Info] Number of data points in the train set: 8543, number of used features: 17
2025-02-26 14:43:19,661:INFO:[LightGBM] [Info] Start training from score 14.875219
2025-02-26 14:43:19,968:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 14:43:19,968:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 14:43:19,970:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000323 seconds.
2025-02-26 14:43:19,970:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 14:43:19,970:INFO:[LightGBM] [Info] Total Bins 1813
2025-02-26 14:43:19,970:INFO:[LightGBM] [Info] Number of data points in the train set: 5729, number of used features: 17
2025-02-26 14:43:19,970:INFO:[LightGBM] [Info] Start training from score 1965.296038
2025-02-26 14:43:20,284:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:43:37,414:INFO:<catboost.core.CatBoostRegressor object at 0x281940550>
2025-02-26 14:43:37,414:INFO:create_model() successfully completed......................................
2025-02-26 14:43:37,497:INFO:Creating Dashboard logs
2025-02-26 14:43:37,499:INFO:Model: CatBoost Regressor
2025-02-26 14:43:38,160:INFO:Logged params: {'nan_mode': 'Min', 'eval_metric': 'RMSE', 'iterations': 1000, 'sampling_frequency': 'PerTree', 'leaf_estimation_method': 'Newton', 'grow_policy': 'SymmetricTree', 'penalties_coefficient': 1, 'boosting_type': 'Plain', 'model_shrink_mode': 'Constant', 'feature_border_type': 'GreedyLogSum', 'bayesian_matrix_reg': 0.10000000149011612, 'eval_fraction': 0, 'force_unit_auto_pair_weights': False, 'l2_leaf_reg': 3, 'random_strength': 1, 'rsm': 1, 'boost_from_average': True, 'model_size_reg': 0.5, 'pool_metainfo_options': {'tags': {}}, 'subsample': 0.800000011920929, 'use_best_model': False, 'random_seed': 123, 'depth': 6, 'posterior_sampling': False, 'border_count': 254, 'classes_count': 0, 'auto_class_weights': 'None', 'sparse_features_conflict_fraction': 0, 'leaf_estimation_backtracking': 'AnyImprovement', 'best_model_min_trees': 1, 'model_shrink_rate': 0, 'min_data_in_leaf': 1, 'loss_function': 'RMSE', 'learning_rate': 0.05843900144100189, 'score_function': 'Cosine', 'task_type': 'CPU', 'leaf_estimation_iterations': 1, 'bootstrap_type': 'MVS', 'max_leaves': 64}
2025-02-26 14:43:43,309:INFO:Initializing predict_model()
2025-02-26 14:43:43,310:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x2822888e0>, estimator=<catboost.core.CatBoostRegressor object at 0x281940550>, probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=False, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x281f44790>)
2025-02-26 14:43:43,310:INFO:Checking exceptions
2025-02-26 14:43:43,310:INFO:Preloading libraries
2025-02-26 14:43:43,595:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.
  warnings.warn(

2025-02-26 14:43:59,252:INFO:Creating Dashboard logs
2025-02-26 14:43:59,254:INFO:Model: Light Gradient Boosting Machine
2025-02-26 14:43:59,881:INFO:Logged params: {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'num_leaves': 31, 'objective': None, 'random_state': 123, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0}
2025-02-26 14:44:22,031:INFO:Creating Dashboard logs
2025-02-26 14:44:22,033:INFO:Model: Extra Trees Regressor
2025-02-26 14:44:22,839:INFO:Logged params: {'bootstrap': False, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False}
2025-02-26 14:44:43,629:INFO:Creating Dashboard logs
2025-02-26 14:44:43,631:INFO:Model: Random Forest Regressor
2025-02-26 14:44:44,442:INFO:Logged params: {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False}
2025-02-26 14:45:06,285:INFO:Creating Dashboard logs
2025-02-26 14:45:06,288:INFO:Model: Gradient Boosting Regressor
2025-02-26 14:45:06,946:INFO:Logged params: {'alpha': 0.9, 'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.1, 'loss': 'squared_error', 'max_depth': 3, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_iter_no_change': None, 'random_state': 123, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}
2025-02-26 14:45:29,284:INFO:Creating Dashboard logs
2025-02-26 14:45:29,286:INFO:Model: Extreme Gradient Boosting
2025-02-26 14:45:30,015:INFO:Logged params: {'objective': 'reg:squarederror', 'base_score': None, 'booster': 'gbtree', 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': None, 'device': 'cpu', 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': None, 'feature_types': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': None, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': None, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': None, 'n_jobs': -1, 'num_parallel_tree': None, 'random_state': 123, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': None, 'tree_method': 'auto', 'validate_parameters': None, 'verbosity': 0}
2025-02-26 14:45:53,783:INFO:Creating Dashboard logs
2025-02-26 14:45:53,785:INFO:Model: K Neighbors Regressor
2025-02-26 14:45:54,407:INFO:Logged params: {'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': -1, 'n_neighbors': 5, 'p': 2, 'weights': 'uniform'}
2025-02-26 14:46:15,704:INFO:Creating Dashboard logs
2025-02-26 14:46:15,706:INFO:Model: Huber Regressor
2025-02-26 14:46:16,505:INFO:Logged params: {'alpha': 0.0001, 'epsilon': 1.35, 'fit_intercept': True, 'max_iter': 100, 'tol': 1e-05, 'warm_start': False}
2025-02-26 14:46:39,966:INFO:Creating Dashboard logs
2025-02-26 14:46:39,968:INFO:Model: Bayesian Ridge
2025-02-26 14:46:40,771:INFO:Logged params: {'alpha_1': 1e-06, 'alpha_2': 1e-06, 'alpha_init': None, 'compute_score': False, 'copy_X': True, 'fit_intercept': True, 'lambda_1': 1e-06, 'lambda_2': 1e-06, 'lambda_init': None, 'max_iter': None, 'n_iter': 'deprecated', 'tol': 0.001, 'verbose': False}
2025-02-26 14:47:02,856:INFO:Creating Dashboard logs
2025-02-26 14:47:02,858:INFO:Model: Ridge Regression
2025-02-26 14:47:03,611:INFO:Logged params: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'positive': False, 'random_state': 123, 'solver': 'auto', 'tol': 0.0001}
2025-02-26 14:47:24,508:INFO:Creating Dashboard logs
2025-02-26 14:47:24,510:INFO:Model: Lasso Regression
2025-02-26 14:47:25,419:INFO:Logged params: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': 1000, 'positive': False, 'precompute': False, 'random_state': 123, 'selection': 'cyclic', 'tol': 0.0001, 'warm_start': False}
2025-02-26 14:47:47,649:INFO:Creating Dashboard logs
2025-02-26 14:47:47,651:INFO:Model: Passive Aggressive Regressor
2025-02-26 14:47:48,459:INFO:Logged params: {'C': 1.0, 'average': False, 'early_stopping': False, 'epsilon': 0.1, 'fit_intercept': True, 'loss': 'epsilon_insensitive', 'max_iter': 1000, 'n_iter_no_change': 5, 'random_state': 123, 'shuffle': True, 'tol': 0.001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}
2025-02-26 14:48:11,921:INFO:Creating Dashboard logs
2025-02-26 14:48:11,922:INFO:Model: Linear Regression
2025-02-26 14:48:12,830:INFO:Logged params: {'copy_X': True, 'fit_intercept': True, 'n_jobs': -1, 'positive': False}
2025-02-26 14:48:34,958:INFO:Creating Dashboard logs
2025-02-26 14:48:34,960:INFO:Model: Least Angle Regression
2025-02-26 14:48:35,767:INFO:Logged params: {'copy_X': True, 'eps': 2.220446049250313e-16, 'fit_intercept': True, 'fit_path': True, 'jitter': None, 'n_nonzero_coefs': 500, 'precompute': 'auto', 'random_state': 123, 'verbose': False}
2025-02-26 14:48:56,505:INFO:Creating Dashboard logs
2025-02-26 14:48:56,506:INFO:Model: Lasso Least Angle Regression
2025-02-26 14:48:57,131:INFO:Logged params: {'alpha': 1.0, 'copy_X': True, 'eps': 2.220446049250313e-16, 'fit_intercept': True, 'fit_path': True, 'jitter': None, 'max_iter': 500, 'positive': False, 'precompute': 'auto', 'random_state': 123, 'verbose': False}
2025-02-26 14:49:19,605:INFO:Creating Dashboard logs
2025-02-26 14:49:19,606:INFO:Model: Elastic Net
2025-02-26 14:49:20,209:INFO:Logged params: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'l1_ratio': 0.5, 'max_iter': 1000, 'positive': False, 'precompute': False, 'random_state': 123, 'selection': 'cyclic', 'tol': 0.0001, 'warm_start': False}
2025-02-26 14:49:42,746:INFO:Creating Dashboard logs
2025-02-26 14:49:42,747:INFO:Model: Decision Tree Regressor
2025-02-26 14:49:43,557:INFO:Logged params: {'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 123, 'splitter': 'best'}
2025-02-26 14:50:05,378:INFO:Creating Dashboard logs
2025-02-26 14:50:05,379:INFO:Model: Orthogonal Matching Pursuit
2025-02-26 14:50:06,186:INFO:Logged params: {'fit_intercept': True, 'n_nonzero_coefs': None, 'precompute': 'auto', 'tol': None}
2025-02-26 14:50:29,031:INFO:Creating Dashboard logs
2025-02-26 14:50:29,032:INFO:Model: AdaBoost Regressor
2025-02-26 14:50:29,755:INFO:Logged params: {'estimator': None, 'learning_rate': 1.0, 'loss': 'linear', 'n_estimators': 50, 'random_state': 123}
2025-02-26 14:50:51,757:INFO:Creating Dashboard logs
2025-02-26 14:50:51,759:INFO:Model: Dummy Regressor
2025-02-26 14:50:52,573:INFO:Logged params: {'constant': None, 'quantile': None, 'strategy': 'mean'}
2025-02-26 14:51:14,505:INFO:_master_model_container: 20
2025-02-26 14:51:14,506:INFO:_display_container: 2
2025-02-26 14:51:14,506:INFO:<catboost.core.CatBoostRegressor object at 0x281940550>
2025-02-26 14:51:14,506:INFO:compare_models() successfully completed......................................
2025-02-26 14:51:14,508:INFO:Initializing create_model()
2025-02-26 14:51:14,508:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x2822888e0>, estimator=et, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-26 14:51:14,508:INFO:Checking exceptions
2025-02-26 14:51:14,516:INFO:Importing libraries
2025-02-26 14:51:14,516:INFO:Copying training dataset
2025-02-26 14:51:14,524:INFO:Defining folds
2025-02-26 14:51:14,524:INFO:Declaring metric variables
2025-02-26 14:51:14,527:INFO:Importing untrained model
2025-02-26 14:51:14,529:INFO:Extra Trees Regressor Imported successfully
2025-02-26 14:51:14,533:INFO:Starting cross validation
2025-02-26 14:51:14,631:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-26 14:51:52,322:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:51:52,574:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:51:52,650:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:51:52,783:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:51:53,022:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:51:53,030:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:51:53,386:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:51:53,415:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:52:17,966:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:52:18,237:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:52:32,267:INFO:Calculating mean and std
2025-02-26 14:52:32,270:INFO:Creating metrics dataframe
2025-02-26 14:52:32,275:INFO:Finalizing model
2025-02-26 14:52:32,314:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 14:52:32,314:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 14:52:32,317:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000516 seconds.
2025-02-26 14:52:32,317:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 14:52:32,317:INFO:[LightGBM] [Info] Total Bins 1877
2025-02-26 14:52:32,317:INFO:[LightGBM] [Info] Number of data points in the train set: 9467, number of used features: 17
2025-02-26 14:52:32,318:INFO:[LightGBM] [Info] Start training from score 1.610964
2025-02-26 14:52:32,633:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 14:52:32,633:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 14:52:32,635:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000509 seconds.
2025-02-26 14:52:32,635:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 14:52:32,635:INFO:[LightGBM] [Info] Total Bins 1810
2025-02-26 14:52:32,635:INFO:[LightGBM] [Info] Number of data points in the train set: 8543, number of used features: 17
2025-02-26 14:52:32,636:INFO:[LightGBM] [Info] Start training from score 14.875219
2025-02-26 14:52:32,920:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 14:52:32,920:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 14:52:32,921:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000381 seconds.
2025-02-26 14:52:32,921:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 14:52:32,921:INFO:[LightGBM] [Info] Total Bins 1813
2025-02-26 14:52:32,921:INFO:[LightGBM] [Info] Number of data points in the train set: 5729, number of used features: 17
2025-02-26 14:52:32,922:INFO:[LightGBM] [Info] Start training from score 1965.296038
2025-02-26 14:52:33,200:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 14:52:33,200:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 14:52:33,202:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000229 seconds.
2025-02-26 14:52:33,202:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 14:52:33,202:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 14:52:33,202:INFO:[LightGBM] [Info] Total Bins 2245
2025-02-26 14:52:33,203:INFO:[LightGBM] [Info] Number of data points in the train set: 9467, number of used features: 17
2025-02-26 14:52:33,203:INFO:[LightGBM] [Info] Start training from score 1.610964
2025-02-26 14:52:33,543:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 14:52:33,544:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 14:52:33,546:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000475 seconds.
2025-02-26 14:52:33,546:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 14:52:33,546:INFO:[LightGBM] [Info] Total Bins 1962
2025-02-26 14:52:33,546:INFO:[LightGBM] [Info] Number of data points in the train set: 8543, number of used features: 17
2025-02-26 14:52:33,546:INFO:[LightGBM] [Info] Start training from score 14.875219
2025-02-26 14:52:33,828:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 14:52:33,828:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 14:52:33,830:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000402 seconds.
2025-02-26 14:52:33,830:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 14:52:33,830:INFO:[LightGBM] [Info] Total Bins 1812
2025-02-26 14:52:33,830:INFO:[LightGBM] [Info] Number of data points in the train set: 5729, number of used features: 17
2025-02-26 14:52:33,830:INFO:[LightGBM] [Info] Start training from score 1965.296038
2025-02-26 14:52:34,109:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 14:52:34,109:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 14:52:34,111:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000453 seconds.
2025-02-26 14:52:34,111:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 14:52:34,111:INFO:[LightGBM] [Info] Total Bins 2244
2025-02-26 14:52:34,111:INFO:[LightGBM] [Info] Number of data points in the train set: 9467, number of used features: 17
2025-02-26 14:52:34,111:INFO:[LightGBM] [Info] Start training from score 1.610964
2025-02-26 14:52:34,428:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 14:52:34,428:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 14:52:34,430:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000242 seconds.
2025-02-26 14:52:34,430:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 14:52:34,430:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 14:52:34,430:INFO:[LightGBM] [Info] Total Bins 1962
2025-02-26 14:52:34,430:INFO:[LightGBM] [Info] Number of data points in the train set: 8543, number of used features: 17
2025-02-26 14:52:34,430:INFO:[LightGBM] [Info] Start training from score 14.875219
2025-02-26 14:52:34,740:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 14:52:34,740:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 14:52:34,742:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000402 seconds.
2025-02-26 14:52:34,742:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 14:52:34,742:INFO:[LightGBM] [Info] Total Bins 1813
2025-02-26 14:52:34,742:INFO:[LightGBM] [Info] Number of data points in the train set: 5729, number of used features: 17
2025-02-26 14:52:34,742:INFO:[LightGBM] [Info] Start training from score 1965.296038
2025-02-26 14:52:35,020:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 14:52:35,020:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 14:52:35,022:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000255 seconds.
2025-02-26 14:52:35,022:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 14:52:35,022:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 14:52:35,022:INFO:[LightGBM] [Info] Total Bins 2243
2025-02-26 14:52:35,022:INFO:[LightGBM] [Info] Number of data points in the train set: 9467, number of used features: 17
2025-02-26 14:52:35,023:INFO:[LightGBM] [Info] Start training from score 1.610964
2025-02-26 14:52:35,365:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 14:52:35,365:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 14:52:35,367:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000445 seconds.
2025-02-26 14:52:35,367:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 14:52:35,367:INFO:[LightGBM] [Info] Total Bins 1961
2025-02-26 14:52:35,367:INFO:[LightGBM] [Info] Number of data points in the train set: 8543, number of used features: 17
2025-02-26 14:52:35,368:INFO:[LightGBM] [Info] Start training from score 14.875219
2025-02-26 14:52:35,651:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 14:52:35,651:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 14:52:35,653:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000370 seconds.
2025-02-26 14:52:35,653:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 14:52:35,653:INFO:[LightGBM] [Info] Total Bins 1813
2025-02-26 14:52:35,653:INFO:[LightGBM] [Info] Number of data points in the train set: 5729, number of used features: 17
2025-02-26 14:52:35,653:INFO:[LightGBM] [Info] Start training from score 1965.296038
2025-02-26 14:52:35,929:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 14:52:35,929:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 14:52:35,931:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000486 seconds.
2025-02-26 14:52:35,932:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 14:52:35,932:INFO:[LightGBM] [Info] Total Bins 2244
2025-02-26 14:52:35,932:INFO:[LightGBM] [Info] Number of data points in the train set: 9467, number of used features: 17
2025-02-26 14:52:35,932:INFO:[LightGBM] [Info] Start training from score 1.610964
2025-02-26 14:52:36,249:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 14:52:36,249:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 14:52:36,251:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000444 seconds.
2025-02-26 14:52:36,251:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 14:52:36,251:INFO:[LightGBM] [Info] Total Bins 1962
2025-02-26 14:52:36,251:INFO:[LightGBM] [Info] Number of data points in the train set: 8543, number of used features: 17
2025-02-26 14:52:36,251:INFO:[LightGBM] [Info] Start training from score 14.875219
2025-02-26 14:52:36,528:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 14:52:36,529:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 14:52:36,530:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000369 seconds.
2025-02-26 14:52:36,530:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 14:52:36,530:INFO:[LightGBM] [Info] Total Bins 1813
2025-02-26 14:52:36,530:INFO:[LightGBM] [Info] Number of data points in the train set: 5729, number of used features: 17
2025-02-26 14:52:36,531:INFO:[LightGBM] [Info] Start training from score 1965.296038
2025-02-26 14:52:36,805:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:52:52,055:INFO:Creating Dashboard logs
2025-02-26 14:52:52,057:INFO:Model: Extra Trees Regressor
2025-02-26 14:52:52,894:INFO:Logged params: {'bootstrap': False, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False}
2025-02-26 14:52:58,222:INFO:Initializing predict_model()
2025-02-26 14:52:58,222:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x2822888e0>, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=123), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=False, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x28d878280>)
2025-02-26 14:52:58,222:INFO:Checking exceptions
2025-02-26 14:52:58,223:INFO:Preloading libraries
2025-02-26 14:52:58,518:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.
  warnings.warn(

2025-02-26 14:53:26,388:INFO:Uploading results into container
2025-02-26 14:53:26,390:INFO:Uploading model into container now
2025-02-26 14:53:26,393:INFO:_master_model_container: 21
2025-02-26 14:53:26,393:INFO:_display_container: 3
2025-02-26 14:53:26,394:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=123)
2025-02-26 14:53:26,394:INFO:create_model() successfully completed......................................
2025-02-26 14:53:26,484:INFO:Initializing tune_model()
2025-02-26 14:53:26,484:INFO:tune_model(estimator=ExtraTreesRegressor(n_jobs=-1, random_state=123), fold=None, round=4, n_iter=25, custom_grid=None, optimize=RMSLE, custom_scorer=None, search_library=optuna, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=True, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.regression.oop.RegressionExperiment object at 0x2822888e0>)
2025-02-26 14:53:26,484:INFO:Checking exceptions
2025-02-26 14:53:26,484:INFO:Soft dependency imported: optuna: 4.1.0
2025-02-26 14:53:26,531:INFO:Copying training dataset
2025-02-26 14:53:26,537:INFO:Checking base model
2025-02-26 14:53:26,537:INFO:Base model : Extra Trees Regressor
2025-02-26 14:53:26,539:INFO:Declaring metric variables
2025-02-26 14:53:26,541:INFO:Defining Hyperparameters
2025-02-26 14:53:26,712:INFO:Tuning with n_jobs=-1
2025-02-26 14:53:26,713:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/optuna/_experimental.py:31: ExperimentalWarning: Argument ``multivariate`` is an experimental feature. The interface can change in the future.
  warnings.warn(

2025-02-26 14:53:26,713:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/optuna/_experimental.py:31: ExperimentalWarning: Argument ``constant_liar`` is an experimental feature. The interface can change in the future.
  warnings.warn(

2025-02-26 14:53:26,714:INFO:Initializing optuna.integration.OptunaSearchCV
2025-02-26 14:53:26,719:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/pycaret_experiment/supervised_experiment.py:2458: ExperimentalWarning: OptunaSearchCV is experimental (supported from v0.17.0). The interface can change in the future.
  model_grid = optuna.integration.OptunaSearchCV(  # type: ignore

2025-02-26 14:54:09,691:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:54:10,353:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:54:10,376:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:54:10,581:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:54:10,583:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:54:10,649:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:54:10,825:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 14:54:10,988:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 15:37:13,651:INFO:best_params: {'actual_estimator__n_estimators': 293, 'actual_estimator__max_depth': 11, 'actual_estimator__min_samples_split': 10, 'actual_estimator__min_samples_leaf': 3, 'actual_estimator__max_features': 0.9759482987209925, 'actual_estimator__min_impurity_decrease': 0.00011024586039351623, 'actual_estimator__criterion': 'absolute_error', 'actual_estimator__bootstrap': False}
2025-02-26 15:37:13,654:INFO:Hyperparameter search completed
2025-02-26 15:37:13,654:INFO:SubProcess create_model() called ==================================
2025-02-26 15:37:13,654:INFO:Initializing create_model()
2025-02-26 15:37:13,654:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x2822888e0>, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=123), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x28c28b250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'n_estimators': 293, 'max_depth': 11, 'min_samples_split': 10, 'min_samples_leaf': 3, 'max_features': 0.9759482987209925, 'min_impurity_decrease': 0.00011024586039351623, 'criterion': 'absolute_error', 'bootstrap': False})
2025-02-26 15:37:13,654:INFO:Checking exceptions
2025-02-26 15:37:13,654:INFO:Importing libraries
2025-02-26 15:37:13,654:INFO:Copying training dataset
2025-02-26 15:37:13,661:INFO:Defining folds
2025-02-26 15:37:13,662:INFO:Declaring metric variables
2025-02-26 15:37:13,663:INFO:Importing untrained model
2025-02-26 15:37:13,663:INFO:Declaring custom model
2025-02-26 15:37:13,665:INFO:Extra Trees Regressor Imported successfully
2025-02-26 15:37:13,668:INFO:Starting cross validation
2025-02-26 15:37:13,771:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-26 15:37:54,975:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 15:37:55,010:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 15:37:55,349:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 15:37:55,512:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 15:37:55,623:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 15:37:55,785:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 15:37:55,810:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 15:37:56,211:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 15:42:12,247:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 15:42:13,685:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 15:43:28,945:INFO:Calculating mean and std
2025-02-26 15:43:28,948:INFO:Creating metrics dataframe
2025-02-26 15:43:28,953:INFO:Finalizing model
2025-02-26 15:43:28,990:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 15:43:28,990:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 15:43:28,992:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000550 seconds.
2025-02-26 15:43:28,992:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 15:43:28,992:INFO:[LightGBM] [Info] Total Bins 1877
2025-02-26 15:43:28,992:INFO:[LightGBM] [Info] Number of data points in the train set: 9467, number of used features: 17
2025-02-26 15:43:28,993:INFO:[LightGBM] [Info] Start training from score 1.610964
2025-02-26 15:43:29,455:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 15:43:29,455:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 15:43:29,457:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000512 seconds.
2025-02-26 15:43:29,457:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 15:43:29,457:INFO:[LightGBM] [Info] Total Bins 1810
2025-02-26 15:43:29,457:INFO:[LightGBM] [Info] Number of data points in the train set: 8543, number of used features: 17
2025-02-26 15:43:29,458:INFO:[LightGBM] [Info] Start training from score 14.875219
2025-02-26 15:43:29,796:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 15:43:29,796:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 15:43:29,798:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000383 seconds.
2025-02-26 15:43:29,798:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 15:43:29,798:INFO:[LightGBM] [Info] Total Bins 1813
2025-02-26 15:43:29,798:INFO:[LightGBM] [Info] Number of data points in the train set: 5729, number of used features: 17
2025-02-26 15:43:29,799:INFO:[LightGBM] [Info] Start training from score 1965.296038
2025-02-26 15:43:30,149:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 15:43:30,149:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 15:43:30,151:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000435 seconds.
2025-02-26 15:43:30,151:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 15:43:30,151:INFO:[LightGBM] [Info] Total Bins 2245
2025-02-26 15:43:30,151:INFO:[LightGBM] [Info] Number of data points in the train set: 9467, number of used features: 17
2025-02-26 15:43:30,152:INFO:[LightGBM] [Info] Start training from score 1.610964
2025-02-26 15:43:30,578:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 15:43:30,578:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 15:43:30,580:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000502 seconds.
2025-02-26 15:43:30,580:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 15:43:30,580:INFO:[LightGBM] [Info] Total Bins 1962
2025-02-26 15:43:30,580:INFO:[LightGBM] [Info] Number of data points in the train set: 8543, number of used features: 17
2025-02-26 15:43:30,581:INFO:[LightGBM] [Info] Start training from score 14.875219
2025-02-26 15:43:30,920:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 15:43:30,921:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 15:43:30,922:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000401 seconds.
2025-02-26 15:43:30,922:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 15:43:30,922:INFO:[LightGBM] [Info] Total Bins 1812
2025-02-26 15:43:30,923:INFO:[LightGBM] [Info] Number of data points in the train set: 5729, number of used features: 17
2025-02-26 15:43:30,923:INFO:[LightGBM] [Info] Start training from score 1965.296038
2025-02-26 15:43:31,247:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 15:43:31,247:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 15:43:31,249:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000441 seconds.
2025-02-26 15:43:31,249:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 15:43:31,249:INFO:[LightGBM] [Info] Total Bins 2244
2025-02-26 15:43:31,249:INFO:[LightGBM] [Info] Number of data points in the train set: 9467, number of used features: 17
2025-02-26 15:43:31,250:INFO:[LightGBM] [Info] Start training from score 1.610964
2025-02-26 15:43:31,622:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 15:43:31,623:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 15:43:31,625:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000412 seconds.
2025-02-26 15:43:31,625:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 15:43:31,625:INFO:[LightGBM] [Info] Total Bins 1962
2025-02-26 15:43:31,625:INFO:[LightGBM] [Info] Number of data points in the train set: 8543, number of used features: 17
2025-02-26 15:43:31,626:INFO:[LightGBM] [Info] Start training from score 14.875219
2025-02-26 15:43:31,942:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 15:43:31,942:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 15:43:31,944:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000367 seconds.
2025-02-26 15:43:31,944:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 15:43:31,944:INFO:[LightGBM] [Info] Total Bins 1813
2025-02-26 15:43:31,944:INFO:[LightGBM] [Info] Number of data points in the train set: 5729, number of used features: 17
2025-02-26 15:43:31,944:INFO:[LightGBM] [Info] Start training from score 1965.296038
2025-02-26 15:43:32,279:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 15:43:32,279:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 15:43:32,282:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000241 seconds.
2025-02-26 15:43:32,282:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 15:43:32,282:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 15:43:32,282:INFO:[LightGBM] [Info] Total Bins 2243
2025-02-26 15:43:32,282:INFO:[LightGBM] [Info] Number of data points in the train set: 9467, number of used features: 17
2025-02-26 15:43:32,282:INFO:[LightGBM] [Info] Start training from score 1.610964
2025-02-26 15:43:32,697:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 15:43:32,697:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 15:43:32,700:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000425 seconds.
2025-02-26 15:43:32,700:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 15:43:32,700:INFO:[LightGBM] [Info] Total Bins 1961
2025-02-26 15:43:32,700:INFO:[LightGBM] [Info] Number of data points in the train set: 8543, number of used features: 17
2025-02-26 15:43:32,700:INFO:[LightGBM] [Info] Start training from score 14.875219
2025-02-26 15:43:33,029:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 15:43:33,029:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 15:43:33,031:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000226 seconds.
2025-02-26 15:43:33,031:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 15:43:33,031:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 15:43:33,031:INFO:[LightGBM] [Info] Total Bins 1813
2025-02-26 15:43:33,031:INFO:[LightGBM] [Info] Number of data points in the train set: 5729, number of used features: 17
2025-02-26 15:43:33,032:INFO:[LightGBM] [Info] Start training from score 1965.296038
2025-02-26 15:43:33,371:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 15:43:33,371:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 15:43:33,374:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000241 seconds.
2025-02-26 15:43:33,374:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 15:43:33,374:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 15:43:33,374:INFO:[LightGBM] [Info] Total Bins 2244
2025-02-26 15:43:33,374:INFO:[LightGBM] [Info] Number of data points in the train set: 9467, number of used features: 17
2025-02-26 15:43:33,374:INFO:[LightGBM] [Info] Start training from score 1.610964
2025-02-26 15:43:33,763:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 15:43:33,763:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 15:43:33,765:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000436 seconds.
2025-02-26 15:43:33,765:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 15:43:33,766:INFO:[LightGBM] [Info] Total Bins 1962
2025-02-26 15:43:33,766:INFO:[LightGBM] [Info] Number of data points in the train set: 8543, number of used features: 17
2025-02-26 15:43:33,766:INFO:[LightGBM] [Info] Start training from score 14.875219
2025-02-26 15:43:34,069:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 15:43:34,069:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 15:43:34,070:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000371 seconds.
2025-02-26 15:43:34,070:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 15:43:34,070:INFO:[LightGBM] [Info] Total Bins 1813
2025-02-26 15:43:34,070:INFO:[LightGBM] [Info] Number of data points in the train set: 5729, number of used features: 17
2025-02-26 15:43:34,071:INFO:[LightGBM] [Info] Start training from score 1965.296038
2025-02-26 15:44:27,171:INFO:Uploading results into container
2025-02-26 15:44:27,172:INFO:Uploading model into container now
2025-02-26 15:44:27,172:INFO:_master_model_container: 22
2025-02-26 15:44:27,172:INFO:_display_container: 4
2025-02-26 15:44:27,173:INFO:ExtraTreesRegressor(criterion='absolute_error', max_depth=11,
                    max_features=0.9759482987209925,
                    min_impurity_decrease=0.00011024586039351623,
                    min_samples_leaf=3, min_samples_split=10, n_estimators=293,
                    n_jobs=-1, random_state=123)
2025-02-26 15:44:27,173:INFO:create_model() successfully completed......................................
2025-02-26 15:44:27,298:INFO:SubProcess create_model() end ==================================
2025-02-26 15:44:27,298:INFO:choose_better activated
2025-02-26 15:44:27,300:INFO:SubProcess create_model() called ==================================
2025-02-26 15:44:27,300:INFO:Initializing create_model()
2025-02-26 15:44:27,300:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x2822888e0>, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=123), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-26 15:44:27,300:INFO:Checking exceptions
2025-02-26 15:44:27,301:INFO:Importing libraries
2025-02-26 15:44:27,301:INFO:Copying training dataset
2025-02-26 15:44:27,307:INFO:Defining folds
2025-02-26 15:44:27,307:INFO:Declaring metric variables
2025-02-26 15:44:27,307:INFO:Importing untrained model
2025-02-26 15:44:27,307:INFO:Declaring custom model
2025-02-26 15:44:27,307:INFO:Extra Trees Regressor Imported successfully
2025-02-26 15:44:27,307:INFO:Starting cross validation
2025-02-26 15:44:27,430:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-26 15:45:04,270:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 15:45:05,556:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 15:45:05,855:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 15:45:05,928:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 15:45:06,027:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 15:45:06,220:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 15:45:06,251:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 15:45:06,305:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 15:45:31,420:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 15:45:31,728:WARNING:/Users/daaa/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/pycaret/internal/preprocess/iterative_imputer.py:420: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2025-02-26 15:45:45,935:INFO:Calculating mean and std
2025-02-26 15:45:45,935:INFO:Creating metrics dataframe
2025-02-26 15:45:45,936:INFO:Finalizing model
2025-02-26 15:45:45,961:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 15:45:45,961:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 15:45:45,964:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000498 seconds.
2025-02-26 15:45:45,964:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 15:45:45,964:INFO:[LightGBM] [Info] Total Bins 1877
2025-02-26 15:45:45,964:INFO:[LightGBM] [Info] Number of data points in the train set: 9467, number of used features: 17
2025-02-26 15:45:45,965:INFO:[LightGBM] [Info] Start training from score 1.610964
2025-02-26 15:45:46,497:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 15:45:46,497:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 15:45:46,499:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000258 seconds.
2025-02-26 15:45:46,499:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 15:45:46,499:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 15:45:46,500:INFO:[LightGBM] [Info] Total Bins 1810
2025-02-26 15:45:46,500:INFO:[LightGBM] [Info] Number of data points in the train set: 8543, number of used features: 17
2025-02-26 15:45:46,500:INFO:[LightGBM] [Info] Start training from score 14.875219
2025-02-26 15:45:46,861:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 15:45:46,861:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 15:45:46,863:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000200 seconds.
2025-02-26 15:45:46,863:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 15:45:46,863:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 15:45:46,863:INFO:[LightGBM] [Info] Total Bins 1813
2025-02-26 15:45:46,863:INFO:[LightGBM] [Info] Number of data points in the train set: 5729, number of used features: 17
2025-02-26 15:45:46,863:INFO:[LightGBM] [Info] Start training from score 1965.296038
2025-02-26 15:45:47,205:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 15:45:47,205:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 15:45:47,208:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000426 seconds.
2025-02-26 15:45:47,208:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 15:45:47,208:INFO:[LightGBM] [Info] Total Bins 2245
2025-02-26 15:45:47,208:INFO:[LightGBM] [Info] Number of data points in the train set: 9467, number of used features: 17
2025-02-26 15:45:47,208:INFO:[LightGBM] [Info] Start training from score 1.610964
2025-02-26 15:45:47,634:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 15:45:47,634:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 15:45:47,636:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000219 seconds.
2025-02-26 15:45:47,636:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 15:45:47,636:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 15:45:47,636:INFO:[LightGBM] [Info] Total Bins 1962
2025-02-26 15:45:47,636:INFO:[LightGBM] [Info] Number of data points in the train set: 8543, number of used features: 17
2025-02-26 15:45:47,637:INFO:[LightGBM] [Info] Start training from score 14.875219
2025-02-26 15:45:47,960:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 15:45:47,960:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 15:45:47,962:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000229 seconds.
2025-02-26 15:45:47,962:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 15:45:47,962:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 15:45:47,962:INFO:[LightGBM] [Info] Total Bins 1812
2025-02-26 15:45:47,962:INFO:[LightGBM] [Info] Number of data points in the train set: 5729, number of used features: 17
2025-02-26 15:45:47,962:INFO:[LightGBM] [Info] Start training from score 1965.296038
2025-02-26 15:45:48,312:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 15:45:48,312:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 15:45:48,314:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000443 seconds.
2025-02-26 15:45:48,314:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 15:45:48,314:INFO:[LightGBM] [Info] Total Bins 2244
2025-02-26 15:45:48,314:INFO:[LightGBM] [Info] Number of data points in the train set: 9467, number of used features: 17
2025-02-26 15:45:48,314:INFO:[LightGBM] [Info] Start training from score 1.610964
2025-02-26 15:45:48,658:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 15:45:48,658:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 15:45:48,660:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000452 seconds.
2025-02-26 15:45:48,660:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 15:45:48,660:INFO:[LightGBM] [Info] Total Bins 1962
2025-02-26 15:45:48,660:INFO:[LightGBM] [Info] Number of data points in the train set: 8543, number of used features: 17
2025-02-26 15:45:48,661:INFO:[LightGBM] [Info] Start training from score 14.875219
2025-02-26 15:45:48,970:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 15:45:48,970:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 15:45:48,972:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000357 seconds.
2025-02-26 15:45:48,972:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 15:45:48,972:INFO:[LightGBM] [Info] Total Bins 1813
2025-02-26 15:45:48,972:INFO:[LightGBM] [Info] Number of data points in the train set: 5729, number of used features: 17
2025-02-26 15:45:48,972:INFO:[LightGBM] [Info] Start training from score 1965.296038
2025-02-26 15:45:49,266:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 15:45:49,266:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 15:45:49,268:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000437 seconds.
2025-02-26 15:45:49,268:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 15:45:49,268:INFO:[LightGBM] [Info] Total Bins 2243
2025-02-26 15:45:49,268:INFO:[LightGBM] [Info] Number of data points in the train set: 9467, number of used features: 17
2025-02-26 15:45:49,269:INFO:[LightGBM] [Info] Start training from score 1.610964
2025-02-26 15:45:49,661:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 15:45:49,661:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 15:45:49,663:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000242 seconds.
2025-02-26 15:45:49,663:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 15:45:49,663:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 15:45:49,663:INFO:[LightGBM] [Info] Total Bins 1961
2025-02-26 15:45:49,663:INFO:[LightGBM] [Info] Number of data points in the train set: 8543, number of used features: 17
2025-02-26 15:45:49,663:INFO:[LightGBM] [Info] Start training from score 14.875219
2025-02-26 15:45:50,160:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 15:45:50,160:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 15:45:50,161:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000345 seconds.
2025-02-26 15:45:50,161:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 15:45:50,162:INFO:[LightGBM] [Info] Total Bins 1813
2025-02-26 15:45:50,162:INFO:[LightGBM] [Info] Number of data points in the train set: 5729, number of used features: 17
2025-02-26 15:45:50,162:INFO:[LightGBM] [Info] Start training from score 1965.296038
2025-02-26 15:45:50,512:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 15:45:50,512:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 15:45:50,515:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000252 seconds.
2025-02-26 15:45:50,515:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 15:45:50,515:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 15:45:50,515:INFO:[LightGBM] [Info] Total Bins 2244
2025-02-26 15:45:50,515:INFO:[LightGBM] [Info] Number of data points in the train set: 9467, number of used features: 17
2025-02-26 15:45:50,515:INFO:[LightGBM] [Info] Start training from score 1.610964
2025-02-26 15:45:51,000:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 15:45:51,000:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 15:45:51,002:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000436 seconds.
2025-02-26 15:45:51,002:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 15:45:51,002:INFO:[LightGBM] [Info] Total Bins 1962
2025-02-26 15:45:51,002:INFO:[LightGBM] [Info] Number of data points in the train set: 8543, number of used features: 17
2025-02-26 15:45:51,002:INFO:[LightGBM] [Info] Start training from score 14.875219
2025-02-26 15:45:51,342:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 15:45:51,342:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 15:45:51,343:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000385 seconds.
2025-02-26 15:45:51,343:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 15:45:51,343:INFO:[LightGBM] [Info] Total Bins 1813
2025-02-26 15:45:51,343:INFO:[LightGBM] [Info] Number of data points in the train set: 5729, number of used features: 17
2025-02-26 15:45:51,344:INFO:[LightGBM] [Info] Start training from score 1965.296038
2025-02-26 15:46:06,881:INFO:Uploading results into container
2025-02-26 15:46:06,882:INFO:Uploading model into container now
2025-02-26 15:46:06,882:INFO:_master_model_container: 23
2025-02-26 15:46:06,882:INFO:_display_container: 5
2025-02-26 15:46:06,882:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=123)
2025-02-26 15:46:06,882:INFO:create_model() successfully completed......................................
2025-02-26 15:46:06,961:INFO:SubProcess create_model() end ==================================
2025-02-26 15:46:06,962:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=123) result for RMSLE is 0.2253
2025-02-26 15:46:06,962:INFO:ExtraTreesRegressor(criterion='absolute_error', max_depth=11,
                    max_features=0.9759482987209925,
                    min_impurity_decrease=0.00011024586039351623,
                    min_samples_leaf=3, min_samples_split=10, n_estimators=293,
                    n_jobs=-1, random_state=123) result for RMSLE is 0.238
2025-02-26 15:46:06,962:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=123) is best model
2025-02-26 15:46:06,962:INFO:choose_better completed
2025-02-26 15:46:06,962:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2025-02-26 15:46:06,962:INFO:Creating Dashboard logs
2025-02-26 15:46:06,964:INFO:Model: Extra Trees Regressor
2025-02-26 15:46:10,336:INFO:Logged params: {'bootstrap': False, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False}
2025-02-26 15:46:17,822:INFO:Initializing predict_model()
2025-02-26 15:46:17,823:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x2822888e0>, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=123), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=False, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x28fcd3a30>)
2025-02-26 15:46:17,823:INFO:Checking exceptions
2025-02-26 15:46:17,823:INFO:Preloading libraries
2025-02-26 15:46:49,065:INFO:_master_model_container: 23
2025-02-26 15:46:49,065:INFO:_display_container: 4
2025-02-26 15:46:49,065:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=123)
2025-02-26 15:46:49,065:INFO:tune_model() successfully completed......................................
2025-02-26 15:46:49,151:INFO:Initializing predict_model()
2025-02-26 15:46:49,151:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x2822888e0>, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=123), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x28fcd3760>)
2025-02-26 15:46:49,152:INFO:Checking exceptions
2025-02-26 15:46:49,152:INFO:Preloading libraries
2025-02-26 15:46:49,519:INFO:Initializing evaluate_model()
2025-02-26 15:46:49,519:INFO:evaluate_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x2822888e0>, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=123), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2025-02-26 15:46:49,532:INFO:Initializing plot_model()
2025-02-26 15:46:49,532:INFO:plot_model(plot=pipeline, fold=KFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=123), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x2822888e0>, system=True)
2025-02-26 15:46:49,532:INFO:Checking exceptions
2025-02-26 15:46:49,549:INFO:Preloading libraries
2025-02-26 15:46:49,604:INFO:Copying training dataset
2025-02-26 15:46:49,604:INFO:Plot type: pipeline
2025-02-26 15:46:49,716:INFO:Visual Rendered Successfully
2025-02-26 15:46:49,800:INFO:plot_model() successfully completed......................................
2025-02-26 15:46:49,808:INFO:Initializing plot_model()
2025-02-26 15:46:49,808:INFO:plot_model(plot=residuals, fold=None, verbose=True, display=None, display_format=None, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=123), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs={'observed': True, 'color': 'blue'}, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x2822888e0>, system=True)
2025-02-26 15:46:49,808:INFO:Checking exceptions
2025-02-26 15:46:49,827:INFO:Preloading libraries
2025-02-26 15:46:49,881:INFO:Copying training dataset
2025-02-26 15:46:49,881:INFO:Plot type: residuals
2025-02-26 15:46:50,352:INFO:Fitting Model
2025-02-26 15:46:50,447:INFO:Scoring test/hold-out set
2025-02-26 15:46:50,698:INFO:Visual Rendered Successfully
2025-02-26 15:46:50,787:INFO:plot_model() successfully completed......................................
2025-02-26 15:46:50,794:INFO:Initializing plot_model()
2025-02-26 15:46:50,794:INFO:plot_model(plot=error, fold=None, verbose=True, display=None, display_format=None, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=123), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs={'alpha': 0.5, 'title': 'Actual vs Predicted'}, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x2822888e0>, system=True)
2025-02-26 15:46:50,794:INFO:Checking exceptions
2025-02-26 15:46:50,813:INFO:Preloading libraries
2025-02-26 15:46:50,868:INFO:Copying training dataset
2025-02-26 15:46:50,868:INFO:Plot type: error
2025-02-26 15:46:51,319:INFO:Fitting Model
2025-02-26 15:46:51,320:INFO:Scoring test/hold-out set
2025-02-26 15:46:51,465:INFO:Visual Rendered Successfully
2025-02-26 15:46:51,551:INFO:plot_model() successfully completed......................................
2025-02-26 15:46:51,562:INFO:Initializing plot_model()
2025-02-26 15:46:51,562:INFO:plot_model(plot=feature, fold=None, verbose=True, display=None, display_format=None, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=123), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs={'top_n': 15, 'figsize': (10, 6)}, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x2822888e0>, system=True)
2025-02-26 15:46:51,562:INFO:Checking exceptions
2025-02-26 15:46:51,581:INFO:Preloading libraries
2025-02-26 15:46:51,634:INFO:Copying training dataset
2025-02-26 15:46:51,634:INFO:Plot type: feature
2025-02-26 15:46:51,635:WARNING:No coef_ found. Trying feature_importances_
2025-02-26 15:46:51,821:INFO:Visual Rendered Successfully
2025-02-26 15:46:51,908:INFO:plot_model() successfully completed......................................
2025-02-26 15:46:51,917:INFO:Initializing plot_model()
2025-02-26 15:46:51,917:INFO:plot_model(plot=learning, fold=None, verbose=True, display=None, display_format=None, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=123), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x2822888e0>, system=True)
2025-02-26 15:46:51,918:INFO:Checking exceptions
2025-02-26 15:46:51,937:INFO:Preloading libraries
2025-02-26 15:46:51,994:INFO:Copying training dataset
2025-02-26 15:46:51,994:INFO:Plot type: learning
2025-02-26 15:46:52,438:INFO:Fitting Model
2025-02-26 15:47:13,596:INFO:Initializing plot_model()
2025-02-26 15:47:13,597:INFO:plot_model(plot=learning, fold=None, verbose=True, display=None, display_format=None, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=123), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x2822888e0>, system=True)
2025-02-26 15:47:13,599:INFO:Checking exceptions
2025-02-26 15:47:13,630:INFO:Preloading libraries
2025-02-26 15:47:13,698:INFO:Copying training dataset
2025-02-26 15:47:13,698:INFO:Plot type: learning
2025-02-26 15:47:14,163:INFO:Fitting Model
2025-02-26 15:48:07,034:INFO:Visual Rendered Successfully
2025-02-26 15:48:07,189:INFO:plot_model() successfully completed......................................
2025-02-26 15:48:07,195:INFO:Initializing evaluate_model()
2025-02-26 15:48:07,195:INFO:evaluate_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x2822888e0>, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=123), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2025-02-26 15:48:07,204:INFO:Initializing plot_model()
2025-02-26 15:48:07,204:INFO:plot_model(plot=pipeline, fold=KFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=123), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x2822888e0>, system=True)
2025-02-26 15:48:07,204:INFO:Checking exceptions
2025-02-26 15:48:07,271:INFO:Preloading libraries
2025-02-26 15:48:07,340:INFO:Copying training dataset
2025-02-26 15:48:07,340:INFO:Plot type: pipeline
2025-02-26 15:48:07,439:INFO:Visual Rendered Successfully
2025-02-26 15:48:07,547:INFO:plot_model() successfully completed......................................
2025-02-26 15:48:36,467:INFO:Initializing evaluate_model()
2025-02-26 15:48:36,468:INFO:evaluate_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x2822888e0>, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=123), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2025-02-26 15:48:36,477:INFO:Initializing plot_model()
2025-02-26 15:48:36,477:INFO:plot_model(plot=pipeline, fold=KFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=123), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x2822888e0>, system=True)
2025-02-26 15:48:36,478:INFO:Checking exceptions
2025-02-26 15:48:36,494:INFO:Preloading libraries
2025-02-26 15:48:36,555:INFO:Copying training dataset
2025-02-26 15:48:36,555:INFO:Plot type: pipeline
2025-02-26 15:48:36,718:INFO:Visual Rendered Successfully
2025-02-26 15:48:36,824:INFO:plot_model() successfully completed......................................
2025-02-26 15:48:39,523:INFO:Initializing evaluate_model()
2025-02-26 15:48:39,524:INFO:evaluate_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x2822888e0>, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=123), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2025-02-26 15:48:39,536:INFO:Initializing plot_model()
2025-02-26 15:48:39,536:INFO:plot_model(plot=pipeline, fold=KFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=123), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x2822888e0>, system=True)
2025-02-26 15:48:39,536:INFO:Checking exceptions
2025-02-26 15:48:39,555:INFO:Preloading libraries
2025-02-26 15:48:39,615:INFO:Copying training dataset
2025-02-26 15:48:39,615:INFO:Plot type: pipeline
2025-02-26 15:48:39,778:INFO:Visual Rendered Successfully
2025-02-26 15:48:39,887:INFO:plot_model() successfully completed......................................
2025-02-26 15:48:42,649:INFO:Initializing evaluate_model()
2025-02-26 15:48:42,650:INFO:evaluate_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x2822888e0>, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=123), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2025-02-26 15:48:42,689:INFO:Initializing plot_model()
2025-02-26 15:48:42,689:INFO:plot_model(plot=pipeline, fold=KFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=123), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x2822888e0>, system=True)
2025-02-26 15:48:42,689:INFO:Checking exceptions
2025-02-26 15:48:42,718:INFO:Preloading libraries
2025-02-26 15:48:42,772:INFO:Copying training dataset
2025-02-26 15:48:42,772:INFO:Plot type: pipeline
2025-02-26 15:48:42,937:INFO:Visual Rendered Successfully
2025-02-26 15:48:43,046:INFO:plot_model() successfully completed......................................
2025-02-26 15:48:44,816:INFO:Initializing evaluate_model()
2025-02-26 15:48:44,817:INFO:evaluate_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x2822888e0>, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=123), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2025-02-26 15:48:44,834:INFO:Initializing plot_model()
2025-02-26 15:48:44,834:INFO:plot_model(plot=pipeline, fold=KFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=123), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x2822888e0>, system=True)
2025-02-26 15:48:44,834:INFO:Checking exceptions
2025-02-26 15:48:44,854:INFO:Preloading libraries
2025-02-26 15:48:44,910:INFO:Copying training dataset
2025-02-26 15:48:44,911:INFO:Plot type: pipeline
2025-02-26 15:48:45,079:INFO:Visual Rendered Successfully
2025-02-26 15:48:45,187:INFO:plot_model() successfully completed......................................
2025-02-26 15:48:46,611:INFO:Initializing evaluate_model()
2025-02-26 15:48:46,611:INFO:evaluate_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x2822888e0>, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=123), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2025-02-26 15:48:46,632:INFO:Initializing plot_model()
2025-02-26 15:48:46,632:INFO:plot_model(plot=pipeline, fold=KFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=123), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x2822888e0>, system=True)
2025-02-26 15:48:46,632:INFO:Checking exceptions
2025-02-26 15:48:46,652:INFO:Preloading libraries
2025-02-26 15:48:46,709:INFO:Copying training dataset
2025-02-26 15:48:46,709:INFO:Plot type: pipeline
2025-02-26 15:48:46,872:INFO:Visual Rendered Successfully
2025-02-26 15:48:46,982:INFO:plot_model() successfully completed......................................
2025-02-26 15:48:49,857:INFO:Initializing finalize_model()
2025-02-26 15:48:49,858:INFO:finalize_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x2822888e0>, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=123), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2025-02-26 15:48:49,859:INFO:Finalizing ExtraTreesRegressor(n_jobs=-1, random_state=123)
2025-02-26 15:48:49,871:INFO:Initializing create_model()
2025-02-26 15:48:49,871:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x2822888e0>, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=123), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, error_score=0.0, kwargs={})
2025-02-26 15:48:49,871:INFO:Checking exceptions
2025-02-26 15:48:49,873:INFO:Importing libraries
2025-02-26 15:48:49,873:INFO:Copying training dataset
2025-02-26 15:48:49,874:INFO:Defining folds
2025-02-26 15:48:49,874:INFO:Declaring metric variables
2025-02-26 15:48:49,874:INFO:Importing untrained model
2025-02-26 15:48:49,874:INFO:Declaring custom model
2025-02-26 15:48:49,875:INFO:Extra Trees Regressor Imported successfully
2025-02-26 15:48:49,980:INFO:Cross validation set to False
2025-02-26 15:48:49,980:INFO:Fitting Model
2025-02-26 15:48:50,007:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 15:48:50,007:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 15:48:50,010:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000266 seconds.
2025-02-26 15:48:50,010:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 15:48:50,010:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 15:48:50,010:INFO:[LightGBM] [Info] Total Bins 2003
2025-02-26 15:48:50,010:INFO:[LightGBM] [Info] Number of data points in the train set: 13518, number of used features: 17
2025-02-26 15:48:50,011:INFO:[LightGBM] [Info] Start training from score 1.610075
2025-02-26 15:48:50,473:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 15:48:50,473:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 15:48:50,476:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000285 seconds.
2025-02-26 15:48:50,476:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 15:48:50,476:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 15:48:50,476:INFO:[LightGBM] [Info] Total Bins 1930
2025-02-26 15:48:50,476:INFO:[LightGBM] [Info] Number of data points in the train set: 12211, number of used features: 17
2025-02-26 15:48:50,476:INFO:[LightGBM] [Info] Start training from score 14.767668
2025-02-26 15:48:50,845:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 15:48:50,845:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 15:48:50,847:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000439 seconds.
2025-02-26 15:48:50,847:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 15:48:50,847:INFO:[LightGBM] [Info] Total Bins 1971
2025-02-26 15:48:50,847:INFO:[LightGBM] [Info] Number of data points in the train set: 8205, number of used features: 17
2025-02-26 15:48:50,847:INFO:[LightGBM] [Info] Start training from score 1964.684217
2025-02-26 15:48:51,231:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 15:48:51,231:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 15:48:51,234:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000508 seconds.
2025-02-26 15:48:51,234:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 15:48:51,234:INFO:[LightGBM] [Info] Total Bins 2354
2025-02-26 15:48:51,234:INFO:[LightGBM] [Info] Number of data points in the train set: 13518, number of used features: 17
2025-02-26 15:48:51,234:INFO:[LightGBM] [Info] Start training from score 1.610075
2025-02-26 15:48:51,617:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 15:48:51,617:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 15:48:51,619:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000283 seconds.
2025-02-26 15:48:51,619:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 15:48:51,620:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 15:48:51,620:INFO:[LightGBM] [Info] Total Bins 2067
2025-02-26 15:48:51,620:INFO:[LightGBM] [Info] Number of data points in the train set: 12211, number of used features: 17
2025-02-26 15:48:51,620:INFO:[LightGBM] [Info] Start training from score 14.767668
2025-02-26 15:48:51,989:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 15:48:51,989:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 15:48:51,991:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000393 seconds.
2025-02-26 15:48:51,991:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 15:48:51,991:INFO:[LightGBM] [Info] Total Bins 1972
2025-02-26 15:48:51,991:INFO:[LightGBM] [Info] Number of data points in the train set: 8205, number of used features: 17
2025-02-26 15:48:51,991:INFO:[LightGBM] [Info] Start training from score 1964.684217
2025-02-26 15:48:52,357:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 15:48:52,358:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 15:48:52,361:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000260 seconds.
2025-02-26 15:48:52,361:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 15:48:52,361:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 15:48:52,361:INFO:[LightGBM] [Info] Total Bins 2356
2025-02-26 15:48:52,361:INFO:[LightGBM] [Info] Number of data points in the train set: 13518, number of used features: 17
2025-02-26 15:48:52,361:INFO:[LightGBM] [Info] Start training from score 1.610075
2025-02-26 15:48:52,763:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 15:48:52,763:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 15:48:52,766:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000272 seconds.
2025-02-26 15:48:52,766:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 15:48:52,766:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 15:48:52,766:INFO:[LightGBM] [Info] Total Bins 2065
2025-02-26 15:48:52,766:INFO:[LightGBM] [Info] Number of data points in the train set: 12211, number of used features: 17
2025-02-26 15:48:52,766:INFO:[LightGBM] [Info] Start training from score 14.767668
2025-02-26 15:48:53,134:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 15:48:53,134:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 15:48:53,136:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000429 seconds.
2025-02-26 15:48:53,136:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 15:48:53,136:INFO:[LightGBM] [Info] Total Bins 1972
2025-02-26 15:48:53,136:INFO:[LightGBM] [Info] Number of data points in the train set: 8205, number of used features: 17
2025-02-26 15:48:53,136:INFO:[LightGBM] [Info] Start training from score 1964.684217
2025-02-26 15:48:53,501:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 15:48:53,501:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 15:48:53,504:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000523 seconds.
2025-02-26 15:48:53,504:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 15:48:53,504:INFO:[LightGBM] [Info] Total Bins 2353
2025-02-26 15:48:53,504:INFO:[LightGBM] [Info] Number of data points in the train set: 13518, number of used features: 17
2025-02-26 15:48:53,505:INFO:[LightGBM] [Info] Start training from score 1.610075
2025-02-26 15:48:53,909:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 15:48:53,909:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 15:48:53,912:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000553 seconds.
2025-02-26 15:48:53,912:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 15:48:53,912:INFO:[LightGBM] [Info] Total Bins 2066
2025-02-26 15:48:53,912:INFO:[LightGBM] [Info] Number of data points in the train set: 12211, number of used features: 17
2025-02-26 15:48:53,912:INFO:[LightGBM] [Info] Start training from score 14.767668
2025-02-26 15:48:54,260:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 15:48:54,260:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 15:48:54,262:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000241 seconds.
2025-02-26 15:48:54,262:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 15:48:54,262:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 15:48:54,262:INFO:[LightGBM] [Info] Total Bins 1971
2025-02-26 15:48:54,263:INFO:[LightGBM] [Info] Number of data points in the train set: 8205, number of used features: 17
2025-02-26 15:48:54,263:INFO:[LightGBM] [Info] Start training from score 1964.684217
2025-02-26 15:48:54,660:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 15:48:54,660:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 15:48:54,663:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000580 seconds.
2025-02-26 15:48:54,663:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 15:48:54,663:INFO:[LightGBM] [Info] Total Bins 2354
2025-02-26 15:48:54,663:INFO:[LightGBM] [Info] Number of data points in the train set: 13518, number of used features: 17
2025-02-26 15:48:54,663:INFO:[LightGBM] [Info] Start training from score 1.610075
2025-02-26 15:48:55,080:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 15:48:55,080:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 15:48:55,083:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000298 seconds.
2025-02-26 15:48:55,083:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 15:48:55,083:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 15:48:55,083:INFO:[LightGBM] [Info] Total Bins 2065
2025-02-26 15:48:55,084:INFO:[LightGBM] [Info] Number of data points in the train set: 12211, number of used features: 17
2025-02-26 15:48:55,084:INFO:[LightGBM] [Info] Start training from score 14.767668
2025-02-26 15:48:55,455:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 15:48:55,455:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 15:48:55,457:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000444 seconds.
2025-02-26 15:48:55,457:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 15:48:55,457:INFO:[LightGBM] [Info] Total Bins 1971
2025-02-26 15:48:55,457:INFO:[LightGBM] [Info] Number of data points in the train set: 8205, number of used features: 17
2025-02-26 15:48:55,457:INFO:[LightGBM] [Info] Start training from score 1964.684217
2025-02-26 15:49:17,027:INFO:Pipeline(memory=Memory(location=None),
         steps=[('date_feature_extractor',
                 TransformerWrapper(include=['Date'],
                                    transformer=ExtractDateTimeFeatures())),
                ('iterative_imputer',
                 TransformerWrapper(transformer=IterativeImputer(cat_estimator=LGBMClassifier(n_jobs=-1,
                                                                                              random_state=123),
                                                                 cat_estimator_prepare_for_categoricals_type='fit_params_categorical_feature',...
                 TransformerWrapper(exclude=[],
                                    transformer=RemoveMulticollinearity(threshold=0.55))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('feature_selection',
                 TransformerWrapper(exclude=[],
                                    transformer=SelectFromModel(estimator=RandomForestRegressor(),
                                                                max_features=12,
                                                                threshold=-inf))),
                ('actual_estimator',
                 ExtraTreesRegressor(n_jobs=-1, random_state=123))])
2025-02-26 15:49:17,027:INFO:create_model() successfully completed......................................
2025-02-26 15:49:17,132:INFO:Creating Dashboard logs
2025-02-26 15:49:17,132:INFO:Model: Extra Trees Regressor
2025-02-26 15:49:17,904:INFO:Logged params: {'bootstrap': False, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False}
2025-02-26 15:49:59,752:INFO:_master_model_container: 23
2025-02-26 15:49:59,752:INFO:_display_container: 5
2025-02-26 15:49:59,786:INFO:Pipeline(memory=Memory(location=None),
         steps=[('date_feature_extractor',
                 TransformerWrapper(include=['Date'],
                                    transformer=ExtractDateTimeFeatures())),
                ('iterative_imputer',
                 TransformerWrapper(transformer=IterativeImputer(cat_estimator=LGBMClassifier(n_jobs=-1,
                                                                                              random_state=123),
                                                                 cat_estimator_prepare_for_categoricals_type='fit_params_categorical_feature',...
                 TransformerWrapper(exclude=[],
                                    transformer=RemoveMulticollinearity(threshold=0.55))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('feature_selection',
                 TransformerWrapper(exclude=[],
                                    transformer=SelectFromModel(estimator=RandomForestRegressor(),
                                                                max_features=12,
                                                                threshold=-inf))),
                ('actual_estimator',
                 ExtraTreesRegressor(n_jobs=-1, random_state=123))])
2025-02-26 15:49:59,786:INFO:finalize_model() successfully completed......................................
2025-02-26 15:50:45,905:INFO:Initializing finalize_model()
2025-02-26 15:50:45,905:INFO:finalize_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x2822888e0>, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=123), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2025-02-26 15:50:45,906:INFO:Finalizing ExtraTreesRegressor(n_jobs=-1, random_state=123)
2025-02-26 15:50:45,920:INFO:Initializing create_model()
2025-02-26 15:50:45,921:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x2822888e0>, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=123), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, error_score=0.0, kwargs={})
2025-02-26 15:50:45,921:INFO:Checking exceptions
2025-02-26 15:50:45,923:INFO:Importing libraries
2025-02-26 15:50:45,923:INFO:Copying training dataset
2025-02-26 15:50:45,923:INFO:Defining folds
2025-02-26 15:50:45,923:INFO:Declaring metric variables
2025-02-26 15:50:45,923:INFO:Importing untrained model
2025-02-26 15:50:45,923:INFO:Declaring custom model
2025-02-26 15:50:45,924:INFO:Extra Trees Regressor Imported successfully
2025-02-26 15:50:46,013:INFO:Cross validation set to False
2025-02-26 15:50:46,013:INFO:Fitting Model
2025-02-26 15:50:46,042:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 15:50:46,042:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 15:50:46,044:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000267 seconds.
2025-02-26 15:50:46,044:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 15:50:46,044:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 15:50:46,045:INFO:[LightGBM] [Info] Total Bins 2003
2025-02-26 15:50:46,045:INFO:[LightGBM] [Info] Number of data points in the train set: 13518, number of used features: 17
2025-02-26 15:50:46,045:INFO:[LightGBM] [Info] Start training from score 1.610075
2025-02-26 15:50:46,445:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 15:50:46,445:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 15:50:46,448:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000528 seconds.
2025-02-26 15:50:46,448:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 15:50:46,448:INFO:[LightGBM] [Info] Total Bins 1930
2025-02-26 15:50:46,448:INFO:[LightGBM] [Info] Number of data points in the train set: 12211, number of used features: 17
2025-02-26 15:50:46,448:INFO:[LightGBM] [Info] Start training from score 14.767668
2025-02-26 15:50:46,785:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 15:50:46,785:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 15:50:46,787:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000426 seconds.
2025-02-26 15:50:46,787:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 15:50:46,787:INFO:[LightGBM] [Info] Total Bins 1971
2025-02-26 15:50:46,787:INFO:[LightGBM] [Info] Number of data points in the train set: 8205, number of used features: 17
2025-02-26 15:50:46,788:INFO:[LightGBM] [Info] Start training from score 1964.684217
2025-02-26 15:50:47,177:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 15:50:47,177:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 15:50:47,180:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000281 seconds.
2025-02-26 15:50:47,180:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 15:50:47,180:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 15:50:47,180:INFO:[LightGBM] [Info] Total Bins 2354
2025-02-26 15:50:47,180:INFO:[LightGBM] [Info] Number of data points in the train set: 13518, number of used features: 17
2025-02-26 15:50:47,181:INFO:[LightGBM] [Info] Start training from score 1.610075
2025-02-26 15:50:47,647:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 15:50:47,647:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 15:50:47,649:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000295 seconds.
2025-02-26 15:50:47,650:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 15:50:47,650:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 15:50:47,650:INFO:[LightGBM] [Info] Total Bins 2067
2025-02-26 15:50:47,650:INFO:[LightGBM] [Info] Number of data points in the train set: 12211, number of used features: 17
2025-02-26 15:50:47,650:INFO:[LightGBM] [Info] Start training from score 14.767668
2025-02-26 15:50:48,014:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 15:50:48,014:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 15:50:48,016:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000415 seconds.
2025-02-26 15:50:48,016:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 15:50:48,016:INFO:[LightGBM] [Info] Total Bins 1972
2025-02-26 15:50:48,016:INFO:[LightGBM] [Info] Number of data points in the train set: 8205, number of used features: 17
2025-02-26 15:50:48,017:INFO:[LightGBM] [Info] Start training from score 1964.684217
2025-02-26 15:50:48,356:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 15:50:48,356:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 15:50:48,359:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000514 seconds.
2025-02-26 15:50:48,359:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 15:50:48,359:INFO:[LightGBM] [Info] Total Bins 2356
2025-02-26 15:50:48,359:INFO:[LightGBM] [Info] Number of data points in the train set: 13518, number of used features: 17
2025-02-26 15:50:48,360:INFO:[LightGBM] [Info] Start training from score 1.610075
2025-02-26 15:50:48,817:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 15:50:48,817:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 15:50:48,820:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000564 seconds.
2025-02-26 15:50:48,820:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 15:50:48,820:INFO:[LightGBM] [Info] Total Bins 2065
2025-02-26 15:50:48,820:INFO:[LightGBM] [Info] Number of data points in the train set: 12211, number of used features: 17
2025-02-26 15:50:48,820:INFO:[LightGBM] [Info] Start training from score 14.767668
2025-02-26 15:50:49,144:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 15:50:49,144:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 15:50:49,146:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000429 seconds.
2025-02-26 15:50:49,146:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 15:50:49,146:INFO:[LightGBM] [Info] Total Bins 1972
2025-02-26 15:50:49,146:INFO:[LightGBM] [Info] Number of data points in the train set: 8205, number of used features: 17
2025-02-26 15:50:49,147:INFO:[LightGBM] [Info] Start training from score 1964.684217
2025-02-26 15:50:49,501:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 15:50:49,501:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 15:50:49,504:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000524 seconds.
2025-02-26 15:50:49,504:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 15:50:49,504:INFO:[LightGBM] [Info] Total Bins 2353
2025-02-26 15:50:49,504:INFO:[LightGBM] [Info] Number of data points in the train set: 13518, number of used features: 17
2025-02-26 15:50:49,505:INFO:[LightGBM] [Info] Start training from score 1.610075
2025-02-26 15:50:49,893:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 15:50:49,894:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 15:50:49,896:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000521 seconds.
2025-02-26 15:50:49,896:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 15:50:49,896:INFO:[LightGBM] [Info] Total Bins 2066
2025-02-26 15:50:49,896:INFO:[LightGBM] [Info] Number of data points in the train set: 12211, number of used features: 17
2025-02-26 15:50:49,897:INFO:[LightGBM] [Info] Start training from score 14.767668
2025-02-26 15:50:50,232:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 15:50:50,232:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 15:50:50,234:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000420 seconds.
2025-02-26 15:50:50,234:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 15:50:50,234:INFO:[LightGBM] [Info] Total Bins 1971
2025-02-26 15:50:50,234:INFO:[LightGBM] [Info] Number of data points in the train set: 8205, number of used features: 17
2025-02-26 15:50:50,235:INFO:[LightGBM] [Info] Start training from score 1964.684217
2025-02-26 15:50:50,591:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 15:50:50,591:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 15:50:50,594:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000281 seconds.
2025-02-26 15:50:50,594:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 15:50:50,594:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 15:50:50,594:INFO:[LightGBM] [Info] Total Bins 2354
2025-02-26 15:50:50,594:INFO:[LightGBM] [Info] Number of data points in the train set: 13518, number of used features: 17
2025-02-26 15:50:50,594:INFO:[LightGBM] [Info] Start training from score 1.610075
2025-02-26 15:50:51,095:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 15:50:51,095:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 15:50:51,098:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000274 seconds.
2025-02-26 15:50:51,098:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 15:50:51,098:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 15:50:51,098:INFO:[LightGBM] [Info] Total Bins 2065
2025-02-26 15:50:51,098:INFO:[LightGBM] [Info] Number of data points in the train set: 12211, number of used features: 17
2025-02-26 15:50:51,098:INFO:[LightGBM] [Info] Start training from score 14.767668
2025-02-26 15:50:51,509:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 15:50:51,509:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 15:50:51,511:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000431 seconds.
2025-02-26 15:50:51,511:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 15:50:51,511:INFO:[LightGBM] [Info] Total Bins 1971
2025-02-26 15:50:51,511:INFO:[LightGBM] [Info] Number of data points in the train set: 8205, number of used features: 17
2025-02-26 15:50:51,512:INFO:[LightGBM] [Info] Start training from score 1964.684217
2025-02-26 15:51:12,908:INFO:Pipeline(memory=Memory(location=None),
         steps=[('date_feature_extractor',
                 TransformerWrapper(include=['Date'],
                                    transformer=ExtractDateTimeFeatures())),
                ('iterative_imputer',
                 TransformerWrapper(transformer=IterativeImputer(cat_estimator=LGBMClassifier(n_jobs=-1,
                                                                                              random_state=123),
                                                                 cat_estimator_prepare_for_categoricals_type='fit_params_categorical_feature',...
                 TransformerWrapper(exclude=[],
                                    transformer=RemoveMulticollinearity(threshold=0.55))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('feature_selection',
                 TransformerWrapper(exclude=[],
                                    transformer=SelectFromModel(estimator=RandomForestRegressor(),
                                                                max_features=12,
                                                                threshold=-inf))),
                ('actual_estimator',
                 ExtraTreesRegressor(n_jobs=-1, random_state=123))])
2025-02-26 15:51:12,908:INFO:create_model() successfully completed......................................
2025-02-26 15:51:13,016:INFO:Creating Dashboard logs
2025-02-26 15:51:13,016:INFO:Model: Extra Trees Regressor
2025-02-26 15:51:13,958:INFO:Logged params: {'bootstrap': False, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False}
2025-02-26 15:51:56,390:INFO:_master_model_container: 23
2025-02-26 15:51:56,391:INFO:_display_container: 5
2025-02-26 15:51:56,425:INFO:Pipeline(memory=Memory(location=None),
         steps=[('date_feature_extractor',
                 TransformerWrapper(include=['Date'],
                                    transformer=ExtractDateTimeFeatures())),
                ('iterative_imputer',
                 TransformerWrapper(transformer=IterativeImputer(cat_estimator=LGBMClassifier(n_jobs=-1,
                                                                                              random_state=123),
                                                                 cat_estimator_prepare_for_categoricals_type='fit_params_categorical_feature',...
                 TransformerWrapper(exclude=[],
                                    transformer=RemoveMulticollinearity(threshold=0.55))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('feature_selection',
                 TransformerWrapper(exclude=[],
                                    transformer=SelectFromModel(estimator=RandomForestRegressor(),
                                                                max_features=12,
                                                                threshold=-inf))),
                ('actual_estimator',
                 ExtraTreesRegressor(n_jobs=-1, random_state=123))])
2025-02-26 15:51:56,425:INFO:finalize_model() successfully completed......................................
2025-02-26 15:51:56,573:INFO:Initializing save_model()
2025-02-26 15:51:56,573:INFO:save_model(model=ExtraTreesRegressor(n_jobs=-1, random_state=123), model_name=house_pricing_pipeline, prep_pipe_=Pipeline(memory=FastMemory(location=/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/joblib),
         steps=[('date_feature_extractor',
                 TransformerWrapper(include=['Date'],
                                    transformer=ExtractDateTimeFeatures())),
                ('iterative_imputer',
                 TransformerWrapper(transformer=IterativeImputer(cat_estimator=LGBMClassifier(n_jobs=-1,
                                                                                              random_state=123),
                                                                 cat_estimator_prepare_...
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('remove_multicollinearity',
                 TransformerWrapper(exclude=[],
                                    transformer=RemoveMulticollinearity(threshold=0.55))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('feature_selection',
                 TransformerWrapper(exclude=[],
                                    transformer=SelectFromModel(estimator=RandomForestRegressor(),
                                                                max_features=12,
                                                                threshold=-inf)))]), verbose=True, use_case=MLUsecase.REGRESSION, kwargs={})
2025-02-26 15:51:56,573:INFO:Adding model into prep_pipe
2025-02-26 15:51:56,789:INFO:house_pricing_pipeline.pkl saved in current working directory
2025-02-26 15:51:56,822:INFO:Pipeline(memory=Memory(location=None),
         steps=[('date_feature_extractor',
                 TransformerWrapper(include=['Date'],
                                    transformer=ExtractDateTimeFeatures())),
                ('iterative_imputer',
                 TransformerWrapper(transformer=IterativeImputer(cat_estimator=LGBMClassifier(n_jobs=-1,
                                                                                              random_state=123),
                                                                 cat_estimator_prepare_for_categoricals_type='fit_params_categorical_feature',...
                 TransformerWrapper(exclude=[],
                                    transformer=RemoveMulticollinearity(threshold=0.55))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('feature_selection',
                 TransformerWrapper(exclude=[],
                                    transformer=SelectFromModel(estimator=RandomForestRegressor(),
                                                                max_features=12,
                                                                threshold=-inf))),
                ('trained_model',
                 ExtraTreesRegressor(n_jobs=-1, random_state=123))])
2025-02-26 15:51:56,822:INFO:save_model() successfully completed......................................
2025-02-26 15:52:48,214:INFO:Initializing load_model()
2025-02-26 15:52:48,214:INFO:load_model(model_name=house_pricing_pipeline, platform=None, authentication=None, verbose=True)
2025-02-26 16:03:02,246:INFO:Initializing finalize_model()
2025-02-26 16:03:02,247:INFO:finalize_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x2822888e0>, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=123), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2025-02-26 16:03:02,248:INFO:Finalizing ExtraTreesRegressor(n_jobs=-1, random_state=123)
2025-02-26 16:03:02,264:INFO:Initializing create_model()
2025-02-26 16:03:02,264:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x2822888e0>, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=123), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, error_score=0.0, kwargs={})
2025-02-26 16:03:02,264:INFO:Checking exceptions
2025-02-26 16:03:02,265:INFO:Importing libraries
2025-02-26 16:03:02,266:INFO:Copying training dataset
2025-02-26 16:03:02,266:INFO:Defining folds
2025-02-26 16:03:02,266:INFO:Declaring metric variables
2025-02-26 16:03:02,266:INFO:Importing untrained model
2025-02-26 16:03:02,266:INFO:Declaring custom model
2025-02-26 16:03:02,267:INFO:Extra Trees Regressor Imported successfully
2025-02-26 16:03:02,362:INFO:Cross validation set to False
2025-02-26 16:03:02,362:INFO:Fitting Model
2025-02-26 16:03:02,391:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 16:03:02,391:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 16:03:02,394:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000572 seconds.
2025-02-26 16:03:02,394:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 16:03:02,394:INFO:[LightGBM] [Info] Total Bins 2003
2025-02-26 16:03:02,395:INFO:[LightGBM] [Info] Number of data points in the train set: 13518, number of used features: 17
2025-02-26 16:03:02,395:INFO:[LightGBM] [Info] Start training from score 1.610075
2025-02-26 16:03:02,984:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 16:03:02,984:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 16:03:02,987:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000301 seconds.
2025-02-26 16:03:02,987:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 16:03:02,987:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 16:03:02,987:INFO:[LightGBM] [Info] Total Bins 1930
2025-02-26 16:03:02,987:INFO:[LightGBM] [Info] Number of data points in the train set: 12211, number of used features: 17
2025-02-26 16:03:02,988:INFO:[LightGBM] [Info] Start training from score 14.767668
2025-02-26 16:03:03,379:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 16:03:03,379:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 16:03:03,381:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000381 seconds.
2025-02-26 16:03:03,382:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 16:03:03,382:INFO:[LightGBM] [Info] Total Bins 1971
2025-02-26 16:03:03,382:INFO:[LightGBM] [Info] Number of data points in the train set: 8205, number of used features: 17
2025-02-26 16:03:03,382:INFO:[LightGBM] [Info] Start training from score 1964.684217
2025-02-26 16:03:03,726:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 16:03:03,726:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 16:03:03,729:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000296 seconds.
2025-02-26 16:03:03,729:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 16:03:03,729:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 16:03:03,729:INFO:[LightGBM] [Info] Total Bins 2354
2025-02-26 16:03:03,729:INFO:[LightGBM] [Info] Number of data points in the train set: 13518, number of used features: 17
2025-02-26 16:03:03,730:INFO:[LightGBM] [Info] Start training from score 1.610075
2025-02-26 16:03:04,174:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 16:03:04,174:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 16:03:04,176:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000282 seconds.
2025-02-26 16:03:04,177:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 16:03:04,177:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 16:03:04,177:INFO:[LightGBM] [Info] Total Bins 2067
2025-02-26 16:03:04,177:INFO:[LightGBM] [Info] Number of data points in the train set: 12211, number of used features: 17
2025-02-26 16:03:04,177:INFO:[LightGBM] [Info] Start training from score 14.767668
2025-02-26 16:03:04,531:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 16:03:04,531:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 16:03:04,533:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000419 seconds.
2025-02-26 16:03:04,533:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 16:03:04,533:INFO:[LightGBM] [Info] Total Bins 1972
2025-02-26 16:03:04,534:INFO:[LightGBM] [Info] Number of data points in the train set: 8205, number of used features: 17
2025-02-26 16:03:04,534:INFO:[LightGBM] [Info] Start training from score 1964.684217
2025-02-26 16:03:04,871:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 16:03:04,872:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 16:03:04,874:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000551 seconds.
2025-02-26 16:03:04,875:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 16:03:04,875:INFO:[LightGBM] [Info] Total Bins 2356
2025-02-26 16:03:04,875:INFO:[LightGBM] [Info] Number of data points in the train set: 13518, number of used features: 17
2025-02-26 16:03:04,875:INFO:[LightGBM] [Info] Start training from score 1.610075
2025-02-26 16:03:05,400:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 16:03:05,400:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 16:03:05,403:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000250 seconds.
2025-02-26 16:03:05,403:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 16:03:05,403:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 16:03:05,403:INFO:[LightGBM] [Info] Total Bins 2065
2025-02-26 16:03:05,403:INFO:[LightGBM] [Info] Number of data points in the train set: 12211, number of used features: 17
2025-02-26 16:03:05,404:INFO:[LightGBM] [Info] Start training from score 14.767668
2025-02-26 16:03:05,769:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 16:03:05,770:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 16:03:05,772:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000428 seconds.
2025-02-26 16:03:05,772:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 16:03:05,772:INFO:[LightGBM] [Info] Total Bins 1972
2025-02-26 16:03:05,772:INFO:[LightGBM] [Info] Number of data points in the train set: 8205, number of used features: 17
2025-02-26 16:03:05,772:INFO:[LightGBM] [Info] Start training from score 1964.684217
2025-02-26 16:03:06,188:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 16:03:06,188:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 16:03:06,191:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000251 seconds.
2025-02-26 16:03:06,191:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 16:03:06,191:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 16:03:06,191:INFO:[LightGBM] [Info] Total Bins 2353
2025-02-26 16:03:06,191:INFO:[LightGBM] [Info] Number of data points in the train set: 13518, number of used features: 17
2025-02-26 16:03:06,191:INFO:[LightGBM] [Info] Start training from score 1.610075
2025-02-26 16:03:06,612:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 16:03:06,612:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 16:03:06,614:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000330 seconds.
2025-02-26 16:03:06,614:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 16:03:06,614:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 16:03:06,615:INFO:[LightGBM] [Info] Total Bins 2066
2025-02-26 16:03:06,615:INFO:[LightGBM] [Info] Number of data points in the train set: 12211, number of used features: 17
2025-02-26 16:03:06,615:INFO:[LightGBM] [Info] Start training from score 14.767668
2025-02-26 16:03:06,982:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 16:03:06,982:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 16:03:06,984:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000405 seconds.
2025-02-26 16:03:06,984:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 16:03:06,984:INFO:[LightGBM] [Info] Total Bins 1971
2025-02-26 16:03:06,984:INFO:[LightGBM] [Info] Number of data points in the train set: 8205, number of used features: 17
2025-02-26 16:03:06,984:INFO:[LightGBM] [Info] Start training from score 1964.684217
2025-02-26 16:03:07,359:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 16:03:07,359:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 16:03:07,362:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000256 seconds.
2025-02-26 16:03:07,362:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 16:03:07,362:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 16:03:07,362:INFO:[LightGBM] [Info] Total Bins 2354
2025-02-26 16:03:07,362:INFO:[LightGBM] [Info] Number of data points in the train set: 13518, number of used features: 17
2025-02-26 16:03:07,363:INFO:[LightGBM] [Info] Start training from score 1.610075
2025-02-26 16:03:07,776:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 16:03:07,776:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 16:03:07,779:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000502 seconds.
2025-02-26 16:03:07,779:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 16:03:07,779:INFO:[LightGBM] [Info] Total Bins 2065
2025-02-26 16:03:07,779:INFO:[LightGBM] [Info] Number of data points in the train set: 12211, number of used features: 17
2025-02-26 16:03:07,779:INFO:[LightGBM] [Info] Start training from score 14.767668
2025-02-26 16:03:08,121:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 16:03:08,121:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 16:03:08,123:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000434 seconds.
2025-02-26 16:03:08,123:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 16:03:08,123:INFO:[LightGBM] [Info] Total Bins 1971
2025-02-26 16:03:08,123:INFO:[LightGBM] [Info] Number of data points in the train set: 8205, number of used features: 17
2025-02-26 16:03:08,123:INFO:[LightGBM] [Info] Start training from score 1964.684217
2025-02-26 16:03:29,540:INFO:Pipeline(memory=Memory(location=None),
         steps=[('date_feature_extractor',
                 TransformerWrapper(include=['Date'],
                                    transformer=ExtractDateTimeFeatures())),
                ('iterative_imputer',
                 TransformerWrapper(transformer=IterativeImputer(cat_estimator=LGBMClassifier(n_jobs=-1,
                                                                                              random_state=123),
                                                                 cat_estimator_prepare_for_categoricals_type='fit_params_categorical_feature',...
                 TransformerWrapper(exclude=[],
                                    transformer=RemoveMulticollinearity(threshold=0.55))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('feature_selection',
                 TransformerWrapper(exclude=[],
                                    transformer=SelectFromModel(estimator=RandomForestRegressor(),
                                                                max_features=12,
                                                                threshold=-inf))),
                ('actual_estimator',
                 ExtraTreesRegressor(n_jobs=-1, random_state=123))])
2025-02-26 16:03:29,541:INFO:create_model() successfully completed......................................
2025-02-26 16:03:29,665:INFO:Creating Dashboard logs
2025-02-26 16:03:29,666:INFO:Model: Extra Trees Regressor
2025-02-26 16:03:30,971:INFO:Logged params: {'bootstrap': False, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False}
2025-02-26 16:04:14,117:INFO:_master_model_container: 23
2025-02-26 16:04:14,117:INFO:_display_container: 5
2025-02-26 16:04:14,151:INFO:Pipeline(memory=Memory(location=None),
         steps=[('date_feature_extractor',
                 TransformerWrapper(include=['Date'],
                                    transformer=ExtractDateTimeFeatures())),
                ('iterative_imputer',
                 TransformerWrapper(transformer=IterativeImputer(cat_estimator=LGBMClassifier(n_jobs=-1,
                                                                                              random_state=123),
                                                                 cat_estimator_prepare_for_categoricals_type='fit_params_categorical_feature',...
                 TransformerWrapper(exclude=[],
                                    transformer=RemoveMulticollinearity(threshold=0.55))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('feature_selection',
                 TransformerWrapper(exclude=[],
                                    transformer=SelectFromModel(estimator=RandomForestRegressor(),
                                                                max_features=12,
                                                                threshold=-inf))),
                ('actual_estimator',
                 ExtraTreesRegressor(n_jobs=-1, random_state=123))])
2025-02-26 16:04:14,151:INFO:finalize_model() successfully completed......................................
2025-02-26 16:04:27,775:INFO:Initializing finalize_model()
2025-02-26 16:04:27,775:INFO:finalize_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x2822888e0>, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=123), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2025-02-26 16:04:27,776:INFO:Finalizing ExtraTreesRegressor(n_jobs=-1, random_state=123)
2025-02-26 16:04:27,789:INFO:Initializing create_model()
2025-02-26 16:04:27,789:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x2822888e0>, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=123), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, error_score=0.0, kwargs={})
2025-02-26 16:04:27,790:INFO:Checking exceptions
2025-02-26 16:04:27,792:INFO:Importing libraries
2025-02-26 16:04:27,792:INFO:Copying training dataset
2025-02-26 16:04:27,793:INFO:Defining folds
2025-02-26 16:04:27,794:INFO:Declaring metric variables
2025-02-26 16:04:27,794:INFO:Importing untrained model
2025-02-26 16:04:27,794:INFO:Declaring custom model
2025-02-26 16:04:27,795:INFO:Extra Trees Regressor Imported successfully
2025-02-26 16:04:27,894:INFO:Cross validation set to False
2025-02-26 16:04:27,895:INFO:Fitting Model
2025-02-26 16:04:27,920:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 16:04:27,920:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 16:04:27,923:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000524 seconds.
2025-02-26 16:04:27,923:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 16:04:27,923:INFO:[LightGBM] [Info] Total Bins 2003
2025-02-26 16:04:27,923:INFO:[LightGBM] [Info] Number of data points in the train set: 13518, number of used features: 17
2025-02-26 16:04:27,924:INFO:[LightGBM] [Info] Start training from score 1.610075
2025-02-26 16:04:28,351:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 16:04:28,351:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 16:04:28,354:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000539 seconds.
2025-02-26 16:04:28,354:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 16:04:28,354:INFO:[LightGBM] [Info] Total Bins 1930
2025-02-26 16:04:28,354:INFO:[LightGBM] [Info] Number of data points in the train set: 12211, number of used features: 17
2025-02-26 16:04:28,354:INFO:[LightGBM] [Info] Start training from score 14.767668
2025-02-26 16:04:28,777:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 16:04:28,777:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 16:04:28,779:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000409 seconds.
2025-02-26 16:04:28,779:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 16:04:28,780:INFO:[LightGBM] [Info] Total Bins 1971
2025-02-26 16:04:28,780:INFO:[LightGBM] [Info] Number of data points in the train set: 8205, number of used features: 17
2025-02-26 16:04:28,780:INFO:[LightGBM] [Info] Start training from score 1964.684217
2025-02-26 16:04:29,132:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 16:04:29,132:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 16:04:29,135:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000343 seconds.
2025-02-26 16:04:29,135:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 16:04:29,135:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 16:04:29,135:INFO:[LightGBM] [Info] Total Bins 2354
2025-02-26 16:04:29,136:INFO:[LightGBM] [Info] Number of data points in the train set: 13518, number of used features: 17
2025-02-26 16:04:29,136:INFO:[LightGBM] [Info] Start training from score 1.610075
2025-02-26 16:04:29,591:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 16:04:29,591:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 16:04:29,593:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000282 seconds.
2025-02-26 16:04:29,593:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 16:04:29,593:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 16:04:29,594:INFO:[LightGBM] [Info] Total Bins 2067
2025-02-26 16:04:29,594:INFO:[LightGBM] [Info] Number of data points in the train set: 12211, number of used features: 17
2025-02-26 16:04:29,594:INFO:[LightGBM] [Info] Start training from score 14.767668
2025-02-26 16:04:29,960:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 16:04:29,960:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 16:04:29,962:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000392 seconds.
2025-02-26 16:04:29,962:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 16:04:29,962:INFO:[LightGBM] [Info] Total Bins 1972
2025-02-26 16:04:29,962:INFO:[LightGBM] [Info] Number of data points in the train set: 8205, number of used features: 17
2025-02-26 16:04:29,962:INFO:[LightGBM] [Info] Start training from score 1964.684217
2025-02-26 16:04:30,340:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 16:04:30,340:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 16:04:30,343:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000574 seconds.
2025-02-26 16:04:30,343:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 16:04:30,343:INFO:[LightGBM] [Info] Total Bins 2356
2025-02-26 16:04:30,343:INFO:[LightGBM] [Info] Number of data points in the train set: 13518, number of used features: 17
2025-02-26 16:04:30,344:INFO:[LightGBM] [Info] Start training from score 1.610075
2025-02-26 16:04:30,732:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 16:04:30,733:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 16:04:30,735:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000489 seconds.
2025-02-26 16:04:30,735:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 16:04:30,735:INFO:[LightGBM] [Info] Total Bins 2065
2025-02-26 16:04:30,735:INFO:[LightGBM] [Info] Number of data points in the train set: 12211, number of used features: 17
2025-02-26 16:04:30,736:INFO:[LightGBM] [Info] Start training from score 14.767668
2025-02-26 16:04:31,097:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 16:04:31,097:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 16:04:31,099:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000456 seconds.
2025-02-26 16:04:31,099:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 16:04:31,099:INFO:[LightGBM] [Info] Total Bins 1972
2025-02-26 16:04:31,099:INFO:[LightGBM] [Info] Number of data points in the train set: 8205, number of used features: 17
2025-02-26 16:04:31,099:INFO:[LightGBM] [Info] Start training from score 1964.684217
2025-02-26 16:04:31,472:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 16:04:31,472:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 16:04:31,475:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000524 seconds.
2025-02-26 16:04:31,475:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 16:04:31,475:INFO:[LightGBM] [Info] Total Bins 2353
2025-02-26 16:04:31,475:INFO:[LightGBM] [Info] Number of data points in the train set: 13518, number of used features: 17
2025-02-26 16:04:31,476:INFO:[LightGBM] [Info] Start training from score 1.610075
2025-02-26 16:04:31,883:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 16:04:31,884:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 16:04:31,886:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000538 seconds.
2025-02-26 16:04:31,886:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 16:04:31,886:INFO:[LightGBM] [Info] Total Bins 2066
2025-02-26 16:04:31,886:INFO:[LightGBM] [Info] Number of data points in the train set: 12211, number of used features: 17
2025-02-26 16:04:31,887:INFO:[LightGBM] [Info] Start training from score 14.767668
2025-02-26 16:04:54,048:INFO:Initializing finalize_model()
2025-02-26 16:04:54,049:INFO:finalize_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x2822888e0>, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=123), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2025-02-26 16:04:54,049:INFO:Finalizing ExtraTreesRegressor(n_jobs=-1, random_state=123)
2025-02-26 16:04:54,061:INFO:Initializing create_model()
2025-02-26 16:04:54,061:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x2822888e0>, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=123), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, error_score=0.0, kwargs={})
2025-02-26 16:04:54,062:INFO:Checking exceptions
2025-02-26 16:04:54,064:INFO:Importing libraries
2025-02-26 16:04:54,064:INFO:Copying training dataset
2025-02-26 16:04:54,066:INFO:Defining folds
2025-02-26 16:04:54,066:INFO:Declaring metric variables
2025-02-26 16:04:54,066:INFO:Importing untrained model
2025-02-26 16:04:54,066:INFO:Declaring custom model
2025-02-26 16:04:54,067:INFO:Extra Trees Regressor Imported successfully
2025-02-26 16:04:54,162:INFO:Cross validation set to False
2025-02-26 16:04:54,162:INFO:Fitting Model
2025-02-26 16:04:54,189:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 16:04:54,189:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 16:04:54,192:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000579 seconds.
2025-02-26 16:04:54,192:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 16:04:54,194:INFO:[LightGBM] [Info] Total Bins 2003
2025-02-26 16:04:54,194:INFO:[LightGBM] [Info] Number of data points in the train set: 13518, number of used features: 17
2025-02-26 16:04:54,194:INFO:[LightGBM] [Info] Start training from score 1.610075
2025-02-26 16:04:54,627:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 16:04:54,627:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 16:04:54,630:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000511 seconds.
2025-02-26 16:04:54,630:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 16:04:54,630:INFO:[LightGBM] [Info] Total Bins 1930
2025-02-26 16:04:54,630:INFO:[LightGBM] [Info] Number of data points in the train set: 12211, number of used features: 17
2025-02-26 16:04:54,630:INFO:[LightGBM] [Info] Start training from score 14.767668
2025-02-26 16:04:55,128:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 16:04:55,128:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 16:04:55,130:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000437 seconds.
2025-02-26 16:04:55,130:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 16:04:55,130:INFO:[LightGBM] [Info] Total Bins 1971
2025-02-26 16:04:55,130:INFO:[LightGBM] [Info] Number of data points in the train set: 8205, number of used features: 17
2025-02-26 16:04:55,131:INFO:[LightGBM] [Info] Start training from score 1964.684217
2025-02-26 16:04:55,502:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 16:04:55,502:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 16:04:55,505:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000516 seconds.
2025-02-26 16:04:55,505:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 16:04:55,506:INFO:[LightGBM] [Info] Total Bins 2354
2025-02-26 16:04:55,506:INFO:[LightGBM] [Info] Number of data points in the train set: 13518, number of used features: 17
2025-02-26 16:04:55,507:INFO:[LightGBM] [Info] Start training from score 1.610075
2025-02-26 16:04:55,992:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 16:04:55,992:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 16:04:55,995:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000276 seconds.
2025-02-26 16:04:55,995:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 16:04:55,995:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 16:04:55,995:INFO:[LightGBM] [Info] Total Bins 2067
2025-02-26 16:04:55,995:INFO:[LightGBM] [Info] Number of data points in the train set: 12211, number of used features: 17
2025-02-26 16:04:55,995:INFO:[LightGBM] [Info] Start training from score 14.767668
2025-02-26 16:04:56,386:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 16:04:56,386:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 16:04:56,388:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000411 seconds.
2025-02-26 16:04:56,388:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 16:04:56,388:INFO:[LightGBM] [Info] Total Bins 1972
2025-02-26 16:04:56,388:INFO:[LightGBM] [Info] Number of data points in the train set: 8205, number of used features: 17
2025-02-26 16:04:56,388:INFO:[LightGBM] [Info] Start training from score 1964.684217
2025-02-26 16:04:56,721:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 16:04:56,721:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 16:04:56,723:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000529 seconds.
2025-02-26 16:04:56,723:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 16:04:56,723:INFO:[LightGBM] [Info] Total Bins 2356
2025-02-26 16:04:56,724:INFO:[LightGBM] [Info] Number of data points in the train set: 13518, number of used features: 17
2025-02-26 16:04:56,724:INFO:[LightGBM] [Info] Start training from score 1.610075
2025-02-26 16:04:57,119:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 16:04:57,119:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 16:04:57,122:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000246 seconds.
2025-02-26 16:04:57,122:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 16:04:57,122:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 16:04:57,122:INFO:[LightGBM] [Info] Total Bins 2065
2025-02-26 16:04:57,122:INFO:[LightGBM] [Info] Number of data points in the train set: 12211, number of used features: 17
2025-02-26 16:04:57,122:INFO:[LightGBM] [Info] Start training from score 14.767668
2025-02-26 16:04:57,492:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 16:04:57,492:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 16:04:57,494:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000423 seconds.
2025-02-26 16:04:57,494:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 16:04:57,494:INFO:[LightGBM] [Info] Total Bins 1972
2025-02-26 16:04:57,494:INFO:[LightGBM] [Info] Number of data points in the train set: 8205, number of used features: 17
2025-02-26 16:04:57,494:INFO:[LightGBM] [Info] Start training from score 1964.684217
2025-02-26 16:04:57,927:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 16:04:57,927:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 16:04:57,929:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000517 seconds.
2025-02-26 16:04:57,930:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 16:04:57,930:INFO:[LightGBM] [Info] Total Bins 2353
2025-02-26 16:04:57,930:INFO:[LightGBM] [Info] Number of data points in the train set: 13518, number of used features: 17
2025-02-26 16:04:57,930:INFO:[LightGBM] [Info] Start training from score 1.610075
2025-02-26 16:04:58,379:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 16:04:58,379:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 16:04:58,382:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000290 seconds.
2025-02-26 16:04:58,382:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 16:04:58,382:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 16:04:58,382:INFO:[LightGBM] [Info] Total Bins 2066
2025-02-26 16:04:58,382:INFO:[LightGBM] [Info] Number of data points in the train set: 12211, number of used features: 17
2025-02-26 16:04:58,383:INFO:[LightGBM] [Info] Start training from score 14.767668
2025-02-26 16:04:58,753:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 16:04:58,753:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 16:04:58,755:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000436 seconds.
2025-02-26 16:04:58,755:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 16:04:58,755:INFO:[LightGBM] [Info] Total Bins 1971
2025-02-26 16:04:58,755:INFO:[LightGBM] [Info] Number of data points in the train set: 8205, number of used features: 17
2025-02-26 16:04:58,756:INFO:[LightGBM] [Info] Start training from score 1964.684217
2025-02-26 16:04:59,103:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 16:04:59,103:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 16:04:59,106:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000531 seconds.
2025-02-26 16:04:59,106:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 16:04:59,106:INFO:[LightGBM] [Info] Total Bins 2354
2025-02-26 16:04:59,106:INFO:[LightGBM] [Info] Number of data points in the train set: 13518, number of used features: 17
2025-02-26 16:04:59,107:INFO:[LightGBM] [Info] Start training from score 1.610075
2025-02-26 16:04:59,550:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 16:04:59,551:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 16:04:59,553:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000505 seconds.
2025-02-26 16:04:59,553:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 16:04:59,553:INFO:[LightGBM] [Info] Total Bins 2065
2025-02-26 16:04:59,553:INFO:[LightGBM] [Info] Number of data points in the train set: 12211, number of used features: 17
2025-02-26 16:04:59,554:INFO:[LightGBM] [Info] Start training from score 14.767668
2025-02-26 16:04:59,896:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 16:04:59,896:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 16:04:59,898:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000436 seconds.
2025-02-26 16:04:59,898:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 16:04:59,898:INFO:[LightGBM] [Info] Total Bins 1971
2025-02-26 16:04:59,898:INFO:[LightGBM] [Info] Number of data points in the train set: 8205, number of used features: 17
2025-02-26 16:04:59,899:INFO:[LightGBM] [Info] Start training from score 1964.684217
2025-02-26 16:05:21,262:INFO:Pipeline(memory=Memory(location=None),
         steps=[('date_feature_extractor',
                 TransformerWrapper(include=['Date'],
                                    transformer=ExtractDateTimeFeatures())),
                ('iterative_imputer',
                 TransformerWrapper(transformer=IterativeImputer(cat_estimator=LGBMClassifier(n_jobs=-1,
                                                                                              random_state=123),
                                                                 cat_estimator_prepare_for_categoricals_type='fit_params_categorical_feature',...
                 TransformerWrapper(exclude=[],
                                    transformer=RemoveMulticollinearity(threshold=0.55))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('feature_selection',
                 TransformerWrapper(exclude=[],
                                    transformer=SelectFromModel(estimator=RandomForestRegressor(),
                                                                max_features=12,
                                                                threshold=-inf))),
                ('actual_estimator',
                 ExtraTreesRegressor(n_jobs=-1, random_state=123))])
2025-02-26 16:05:21,262:INFO:create_model() successfully completed......................................
2025-02-26 16:05:21,440:INFO:Creating Dashboard logs
2025-02-26 16:05:21,440:INFO:Model: Extra Trees Regressor
2025-02-26 16:05:23,906:INFO:Logged params: {'bootstrap': False, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False}
2025-02-26 16:06:08,346:INFO:_master_model_container: 23
2025-02-26 16:06:08,347:INFO:_display_container: 5
2025-02-26 16:06:08,381:INFO:Pipeline(memory=Memory(location=None),
         steps=[('date_feature_extractor',
                 TransformerWrapper(include=['Date'],
                                    transformer=ExtractDateTimeFeatures())),
                ('iterative_imputer',
                 TransformerWrapper(transformer=IterativeImputer(cat_estimator=LGBMClassifier(n_jobs=-1,
                                                                                              random_state=123),
                                                                 cat_estimator_prepare_for_categoricals_type='fit_params_categorical_feature',...
                 TransformerWrapper(exclude=[],
                                    transformer=RemoveMulticollinearity(threshold=0.55))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('feature_selection',
                 TransformerWrapper(exclude=[],
                                    transformer=SelectFromModel(estimator=RandomForestRegressor(),
                                                                max_features=12,
                                                                threshold=-inf))),
                ('actual_estimator',
                 ExtraTreesRegressor(n_jobs=-1, random_state=123))])
2025-02-26 16:06:08,381:INFO:finalize_model() successfully completed......................................
2025-02-26 16:07:09,327:INFO:Initializing finalize_model()
2025-02-26 16:07:09,328:INFO:finalize_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x2822888e0>, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=123), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2025-02-26 16:07:09,329:INFO:Finalizing ExtraTreesRegressor(n_jobs=-1, random_state=123)
2025-02-26 16:07:09,338:INFO:Initializing create_model()
2025-02-26 16:07:09,338:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x2822888e0>, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=123), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, error_score=0.0, kwargs={})
2025-02-26 16:07:09,338:INFO:Checking exceptions
2025-02-26 16:07:09,340:INFO:Importing libraries
2025-02-26 16:07:09,340:INFO:Copying training dataset
2025-02-26 16:07:09,341:INFO:Defining folds
2025-02-26 16:07:09,341:INFO:Declaring metric variables
2025-02-26 16:07:09,341:INFO:Importing untrained model
2025-02-26 16:07:09,341:INFO:Declaring custom model
2025-02-26 16:07:09,342:INFO:Extra Trees Regressor Imported successfully
2025-02-26 16:07:09,430:INFO:Cross validation set to False
2025-02-26 16:07:09,430:INFO:Fitting Model
2025-02-26 16:07:09,457:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 16:07:09,457:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 16:07:09,461:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000538 seconds.
2025-02-26 16:07:09,461:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 16:07:09,461:INFO:[LightGBM] [Info] Total Bins 2003
2025-02-26 16:07:09,461:INFO:[LightGBM] [Info] Number of data points in the train set: 13518, number of used features: 17
2025-02-26 16:07:09,461:INFO:[LightGBM] [Info] Start training from score 1.610075
2025-02-26 16:07:09,905:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 16:07:09,905:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 16:07:09,908:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000307 seconds.
2025-02-26 16:07:09,908:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 16:07:09,908:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 16:07:09,908:INFO:[LightGBM] [Info] Total Bins 1930
2025-02-26 16:07:09,908:INFO:[LightGBM] [Info] Number of data points in the train set: 12211, number of used features: 17
2025-02-26 16:07:09,909:INFO:[LightGBM] [Info] Start training from score 14.767668
2025-02-26 16:07:10,300:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 16:07:10,300:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 16:07:10,302:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000491 seconds.
2025-02-26 16:07:10,302:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 16:07:10,303:INFO:[LightGBM] [Info] Total Bins 1971
2025-02-26 16:07:10,303:INFO:[LightGBM] [Info] Number of data points in the train set: 8205, number of used features: 17
2025-02-26 16:07:10,303:INFO:[LightGBM] [Info] Start training from score 1964.684217
2025-02-26 16:07:10,651:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 16:07:10,651:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 16:07:10,654:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000328 seconds.
2025-02-26 16:07:10,654:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 16:07:10,654:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 16:07:10,654:INFO:[LightGBM] [Info] Total Bins 2354
2025-02-26 16:07:10,654:INFO:[LightGBM] [Info] Number of data points in the train set: 13518, number of used features: 17
2025-02-26 16:07:10,655:INFO:[LightGBM] [Info] Start training from score 1.610075
2025-02-26 16:07:11,084:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 16:07:11,084:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 16:07:11,087:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000254 seconds.
2025-02-26 16:07:11,087:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 16:07:11,087:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 16:07:11,087:INFO:[LightGBM] [Info] Total Bins 2067
2025-02-26 16:07:11,087:INFO:[LightGBM] [Info] Number of data points in the train set: 12211, number of used features: 17
2025-02-26 16:07:11,088:INFO:[LightGBM] [Info] Start training from score 14.767668
2025-02-26 16:07:11,472:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 16:07:11,472:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 16:07:11,474:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000451 seconds.
2025-02-26 16:07:11,474:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 16:07:11,474:INFO:[LightGBM] [Info] Total Bins 1972
2025-02-26 16:07:11,474:INFO:[LightGBM] [Info] Number of data points in the train set: 8205, number of used features: 17
2025-02-26 16:07:11,474:INFO:[LightGBM] [Info] Start training from score 1964.684217
2025-02-26 16:07:11,828:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 16:07:11,828:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 16:07:11,831:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000506 seconds.
2025-02-26 16:07:11,831:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 16:07:11,831:INFO:[LightGBM] [Info] Total Bins 2356
2025-02-26 16:07:11,831:INFO:[LightGBM] [Info] Number of data points in the train set: 13518, number of used features: 17
2025-02-26 16:07:11,832:INFO:[LightGBM] [Info] Start training from score 1.610075
2025-02-26 16:07:12,239:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 16:07:12,239:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 16:07:12,242:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000288 seconds.
2025-02-26 16:07:12,242:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-26 16:07:12,242:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-26 16:07:12,242:INFO:[LightGBM] [Info] Total Bins 2065
2025-02-26 16:07:12,242:INFO:[LightGBM] [Info] Number of data points in the train set: 12211, number of used features: 17
2025-02-26 16:07:12,242:INFO:[LightGBM] [Info] Start training from score 14.767668
2025-02-26 16:07:12,586:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 16:07:12,586:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 16:07:12,588:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000461 seconds.
2025-02-26 16:07:12,588:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 16:07:12,588:INFO:[LightGBM] [Info] Total Bins 1972
2025-02-26 16:07:12,588:INFO:[LightGBM] [Info] Number of data points in the train set: 8205, number of used features: 17
2025-02-26 16:07:12,588:INFO:[LightGBM] [Info] Start training from score 1964.684217
2025-02-26 16:07:12,939:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 16:07:12,939:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 16:07:12,942:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000527 seconds.
2025-02-26 16:07:12,942:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 16:07:12,942:INFO:[LightGBM] [Info] Total Bins 2353
2025-02-26 16:07:12,942:INFO:[LightGBM] [Info] Number of data points in the train set: 13518, number of used features: 17
2025-02-26 16:07:12,943:INFO:[LightGBM] [Info] Start training from score 1.610075
2025-02-26 16:07:13,379:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 16:07:13,380:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 16:07:13,382:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000558 seconds.
2025-02-26 16:07:13,382:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 16:07:13,382:INFO:[LightGBM] [Info] Total Bins 2066
2025-02-26 16:07:13,382:INFO:[LightGBM] [Info] Number of data points in the train set: 12211, number of used features: 17
2025-02-26 16:07:13,383:INFO:[LightGBM] [Info] Start training from score 14.767668
2025-02-26 16:07:13,739:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 16:07:13,740:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 16:07:13,742:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000422 seconds.
2025-02-26 16:07:13,742:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 16:07:13,742:INFO:[LightGBM] [Info] Total Bins 1971
2025-02-26 16:07:13,742:INFO:[LightGBM] [Info] Number of data points in the train set: 8205, number of used features: 17
2025-02-26 16:07:13,742:INFO:[LightGBM] [Info] Start training from score 1964.684217
2025-02-26 16:07:14,145:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 16:07:14,145:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 16:07:14,148:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000535 seconds.
2025-02-26 16:07:14,148:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 16:07:14,148:INFO:[LightGBM] [Info] Total Bins 2354
2025-02-26 16:07:14,148:INFO:[LightGBM] [Info] Number of data points in the train set: 13518, number of used features: 17
2025-02-26 16:07:14,149:INFO:[LightGBM] [Info] Start training from score 1.610075
2025-02-26 16:07:14,608:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 16:07:14,608:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 16:07:14,611:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000525 seconds.
2025-02-26 16:07:14,611:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 16:07:14,611:INFO:[LightGBM] [Info] Total Bins 2065
2025-02-26 16:07:14,611:INFO:[LightGBM] [Info] Number of data points in the train set: 12211, number of used features: 17
2025-02-26 16:07:14,612:INFO:[LightGBM] [Info] Start training from score 14.767668
2025-02-26 16:07:14,965:INFO:[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.
2025-02-26 16:07:14,965:INFO:[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.
2025-02-26 16:07:14,967:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000468 seconds.
2025-02-26 16:07:14,967:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-26 16:07:14,967:INFO:[LightGBM] [Info] Total Bins 1971
2025-02-26 16:07:14,967:INFO:[LightGBM] [Info] Number of data points in the train set: 8205, number of used features: 17
2025-02-26 16:07:14,968:INFO:[LightGBM] [Info] Start training from score 1964.684217
2025-02-26 16:07:36,313:INFO:Pipeline(memory=Memory(location=None),
         steps=[('date_feature_extractor',
                 TransformerWrapper(include=['Date'],
                                    transformer=ExtractDateTimeFeatures())),
                ('iterative_imputer',
                 TransformerWrapper(transformer=IterativeImputer(cat_estimator=LGBMClassifier(n_jobs=-1,
                                                                                              random_state=123),
                                                                 cat_estimator_prepare_for_categoricals_type='fit_params_categorical_feature',...
                 TransformerWrapper(exclude=[],
                                    transformer=RemoveMulticollinearity(threshold=0.55))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('feature_selection',
                 TransformerWrapper(exclude=[],
                                    transformer=SelectFromModel(estimator=RandomForestRegressor(),
                                                                max_features=12,
                                                                threshold=-inf))),
                ('actual_estimator',
                 ExtraTreesRegressor(n_jobs=-1, random_state=123))])
2025-02-26 16:07:36,313:INFO:create_model() successfully completed......................................
2025-02-26 16:07:36,489:INFO:Creating Dashboard logs
2025-02-26 16:07:36,489:INFO:Model: Extra Trees Regressor
2025-02-26 16:07:37,524:INFO:Logged params: {'bootstrap': False, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False}
2025-02-26 16:08:19,523:INFO:_master_model_container: 23
2025-02-26 16:08:19,524:INFO:_display_container: 5
2025-02-26 16:08:19,559:INFO:Pipeline(memory=Memory(location=None),
         steps=[('date_feature_extractor',
                 TransformerWrapper(include=['Date'],
                                    transformer=ExtractDateTimeFeatures())),
                ('iterative_imputer',
                 TransformerWrapper(transformer=IterativeImputer(cat_estimator=LGBMClassifier(n_jobs=-1,
                                                                                              random_state=123),
                                                                 cat_estimator_prepare_for_categoricals_type='fit_params_categorical_feature',...
                 TransformerWrapper(exclude=[],
                                    transformer=RemoveMulticollinearity(threshold=0.55))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('feature_selection',
                 TransformerWrapper(exclude=[],
                                    transformer=SelectFromModel(estimator=RandomForestRegressor(),
                                                                max_features=12,
                                                                threshold=-inf))),
                ('actual_estimator',
                 ExtraTreesRegressor(n_jobs=-1, random_state=123))])
2025-02-26 16:08:19,559:INFO:finalize_model() successfully completed......................................
2025-02-26 16:59:32,219:INFO:Soft dependency imported: fastapi: 0.115.6
2025-02-26 16:59:32,220:INFO:Soft dependency imported: uvicorn: 0.34.0
2025-02-26 16:59:32,221:INFO:Soft dependency imported: pydantic: 2.10.5
2025-02-26 16:59:32,280:INFO:Initializing save_model()
2025-02-26 16:59:32,280:INFO:save_model(model=ExtraTreesRegressor(n_jobs=-1, random_state=123), model_name=housepricingmodel_karthik_api, prep_pipe_=Pipeline(memory=FastMemory(location=/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/joblib),
         steps=[('date_feature_extractor',
                 TransformerWrapper(include=['Date'],
                                    transformer=ExtractDateTimeFeatures())),
                ('iterative_imputer',
                 TransformerWrapper(transformer=IterativeImputer(cat_estimator=LGBMClassifier(n_jobs=-1,
                                                                                              random_state=123),
                                                                 cat_estimator_prepare_...
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('remove_multicollinearity',
                 TransformerWrapper(exclude=[],
                                    transformer=RemoveMulticollinearity(threshold=0.55))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('feature_selection',
                 TransformerWrapper(exclude=[],
                                    transformer=SelectFromModel(estimator=RandomForestRegressor(),
                                                                max_features=12,
                                                                threshold=-inf)))]), verbose=False, use_case=MLUsecase.REGRESSION, kwargs={})
2025-02-26 16:59:32,280:INFO:Adding model into prep_pipe
2025-02-26 16:59:32,711:INFO:housepricingmodel_karthik_api.pkl saved in current working directory
2025-02-26 16:59:32,753:INFO:Pipeline(memory=Memory(location=None),
         steps=[('date_feature_extractor',
                 TransformerWrapper(include=['Date'],
                                    transformer=ExtractDateTimeFeatures())),
                ('iterative_imputer',
                 TransformerWrapper(transformer=IterativeImputer(cat_estimator=LGBMClassifier(n_jobs=-1,
                                                                                              random_state=123),
                                                                 cat_estimator_prepare_for_categoricals_type='fit_params_categorical_feature',...
                 TransformerWrapper(exclude=[],
                                    transformer=RemoveMulticollinearity(threshold=0.55))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('feature_selection',
                 TransformerWrapper(exclude=[],
                                    transformer=SelectFromModel(estimator=RandomForestRegressor(),
                                                                max_features=12,
                                                                threshold=-inf))),
                ('trained_model',
                 ExtraTreesRegressor(n_jobs=-1, random_state=123))])
2025-02-26 16:59:32,753:INFO:save_model() successfully completed......................................
2025-02-27 23:48:12,314:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-27 23:48:12,315:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-27 23:48:12,315:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-27 23:48:12,315:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-27 23:48:17,205:INFO:PyCaret RegressionExperiment
2025-02-27 23:48:17,205:INFO:Logging name: house_pricing
2025-02-27 23:48:17,205:INFO:ML Usecase: MLUsecase.REGRESSION
2025-02-27 23:48:17,205:INFO:version 3.3.2
2025-02-27 23:48:17,205:INFO:Initializing setup()
2025-02-27 23:48:17,205:INFO:self.USI: 1bca
2025-02-27 23:48:17,205:INFO:self._variable_keys: {'_available_plots', 'fold_generator', 'memory', 'seed', 'transform_target_param', 'USI', 'pipeline', 'idx', 'fold_groups_param', 'X_train', 'fold_shuffle_param', 'gpu_param', 'html_param', 'n_jobs_param', 'target_param', 'data', 'X', 'log_plots_param', 'exp_id', 'logging_param', '_ml_usecase', 'X_test', 'y', 'exp_name_log', 'y_test', 'y_train', 'gpu_n_jobs_param'}
2025-02-27 23:48:17,205:INFO:Checking environment
2025-02-27 23:48:17,205:INFO:python_version: 3.10.15
2025-02-27 23:48:17,205:INFO:python_build: ('main', 'Oct  3 2024 02:24:49')
2025-02-27 23:48:17,205:INFO:machine: arm64
2025-02-27 23:48:17,205:INFO:platform: macOS-14.1-arm64-arm-64bit
2025-02-27 23:48:17,205:INFO:Memory: svmem(total=17179869184, available=4432265216, percent=74.2, used=6346342400, free=48693248, active=4508057600, inactive=4318363648, wired=1838284800)
2025-02-27 23:48:17,205:INFO:Physical Core: 8
2025-02-27 23:48:17,205:INFO:Logical Core: 8
2025-02-27 23:48:17,205:INFO:Checking libraries
2025-02-27 23:48:17,205:INFO:System:
2025-02-27 23:48:17,205:INFO:    python: 3.10.15 (main, Oct  3 2024, 02:24:49) [Clang 14.0.6 ]
2025-02-27 23:48:17,206:INFO:executable: /Users/daaa/opt/miniconda3/envs/mlops/bin/python
2025-02-27 23:48:17,206:INFO:   machine: macOS-14.1-arm64-arm-64bit
2025-02-27 23:48:17,206:INFO:PyCaret required dependencies:
2025-02-27 23:48:17,657:INFO:                 pip: 24.2
2025-02-27 23:48:17,657:INFO:          setuptools: 75.1.0
2025-02-27 23:48:17,657:INFO:             pycaret: 3.3.2
2025-02-27 23:48:17,657:INFO:             IPython: 8.30.0
2025-02-27 23:48:17,657:INFO:          ipywidgets: 8.1.5
2025-02-27 23:48:17,657:INFO:                tqdm: 4.67.1
2025-02-27 23:48:17,657:INFO:               numpy: 1.26.4
2025-02-27 23:48:17,657:INFO:              pandas: 2.1.4
2025-02-27 23:48:17,657:INFO:              jinja2: 3.1.4
2025-02-27 23:48:17,657:INFO:               scipy: 1.11.4
2025-02-27 23:48:17,657:INFO:              joblib: 1.3.2
2025-02-27 23:48:17,657:INFO:             sklearn: 1.4.2
2025-02-27 23:48:17,657:INFO:                pyod: 2.0.2
2025-02-27 23:48:17,657:INFO:            imblearn: 0.12.4
2025-02-27 23:48:17,657:INFO:   category_encoders: 2.6.4
2025-02-27 23:48:17,657:INFO:            lightgbm: 4.5.0
2025-02-27 23:48:17,657:INFO:               numba: 0.60.0
2025-02-27 23:48:17,657:INFO:            requests: 2.32.3
2025-02-27 23:48:17,657:INFO:          matplotlib: 3.10.0
2025-02-27 23:48:17,657:INFO:          scikitplot: 0.3.7
2025-02-27 23:48:17,657:INFO:         yellowbrick: 1.5
2025-02-27 23:48:17,657:INFO:              plotly: 5.24.1
2025-02-27 23:48:17,657:INFO:    plotly-resampler: Not installed
2025-02-27 23:48:17,657:INFO:             kaleido: 0.2.1
2025-02-27 23:48:17,657:INFO:           schemdraw: 0.15
2025-02-27 23:48:17,657:INFO:         statsmodels: 0.14.4
2025-02-27 23:48:17,657:INFO:              sktime: 0.26.0
2025-02-27 23:48:17,658:INFO:               tbats: 1.1.3
2025-02-27 23:48:17,658:INFO:            pmdarima: 2.0.4
2025-02-27 23:48:17,658:INFO:              psutil: 6.1.0
2025-02-27 23:48:17,658:INFO:          markupsafe: 2.1.5
2025-02-27 23:48:17,658:INFO:             pickle5: Not installed
2025-02-27 23:48:17,658:INFO:         cloudpickle: 3.1.0
2025-02-27 23:48:17,658:INFO:         deprecation: 2.1.0
2025-02-27 23:48:17,658:INFO:              xxhash: 3.5.0
2025-02-27 23:48:17,658:INFO:           wurlitzer: 3.1.1
2025-02-27 23:48:17,658:INFO:PyCaret optional dependencies:
2025-02-27 23:48:19,030:INFO:                shap: 0.44.1
2025-02-27 23:48:19,030:INFO:           interpret: 0.6.9
2025-02-27 23:48:19,030:INFO:                umap: 0.5.7
2025-02-27 23:48:19,030:INFO:     ydata_profiling: 4.12.1
2025-02-27 23:48:19,030:INFO:  explainerdashboard: 0.4.8
2025-02-27 23:48:19,030:INFO:             autoviz: Not installed
2025-02-27 23:48:19,030:INFO:           fairlearn: 0.7.0
2025-02-27 23:48:19,030:INFO:          deepchecks: Not installed
2025-02-27 23:48:19,030:INFO:             xgboost: 2.1.3
2025-02-27 23:48:19,030:INFO:            catboost: 1.1.1
2025-02-27 23:48:19,030:INFO:              kmodes: 0.12.2
2025-02-27 23:48:19,030:INFO:             mlxtend: 0.23.3
2025-02-27 23:48:19,030:INFO:       statsforecast: 1.5.0
2025-02-27 23:48:19,030:INFO:        tune_sklearn: 0.5.0
2025-02-27 23:48:19,030:INFO:                 ray: 2.40.0
2025-02-27 23:48:19,030:INFO:            hyperopt: 0.2.7
2025-02-27 23:48:19,030:INFO:              optuna: 4.1.0
2025-02-27 23:48:19,030:INFO:               skopt: 0.10.2
2025-02-27 23:48:19,030:INFO:              mlflow: 2.16.0
2025-02-27 23:48:19,030:INFO:              gradio: 5.12.0
2025-02-27 23:48:19,030:INFO:             fastapi: 0.115.6
2025-02-27 23:48:19,030:INFO:             uvicorn: 0.34.0
2025-02-27 23:48:19,030:INFO:              m2cgen: 0.10.0
2025-02-27 23:48:19,030:INFO:           evidently: 0.4.40
2025-02-27 23:48:19,030:INFO:               fugue: 0.8.7
2025-02-27 23:48:19,030:INFO:           streamlit: Not installed
2025-02-27 23:48:19,030:INFO:             prophet: Not installed
2025-02-27 23:48:19,030:INFO:None
2025-02-27 23:48:19,030:INFO:Set up data.
2025-02-27 23:48:19,051:INFO:Set up folding strategy.
2025-02-27 23:48:19,051:INFO:Set up train/test split.
2025-02-27 23:48:19,061:INFO:Set up index.
2025-02-27 23:48:19,061:INFO:Assigning column types.
2025-02-27 23:48:19,064:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-02-27 23:48:19,065:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2025-02-27 23:48:19,067:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-27 23:48:19,069:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-27 23:48:19,101:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-27 23:48:19,124:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-27 23:48:19,124:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-27 23:48:19,126:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-27 23:48:19,163:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2025-02-27 23:48:19,165:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-27 23:48:19,168:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-27 23:48:19,200:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-27 23:48:19,223:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-27 23:48:19,224:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-27 23:48:19,225:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-27 23:48:19,225:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2025-02-27 23:48:19,228:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-27 23:48:19,230:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-27 23:48:19,263:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-27 23:48:19,286:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-27 23:48:19,286:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-27 23:48:19,287:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-27 23:48:19,290:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-27 23:48:19,292:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-27 23:48:19,324:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-27 23:48:19,350:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-27 23:48:19,350:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-27 23:48:19,352:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-27 23:48:19,352:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2025-02-27 23:48:19,357:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-27 23:48:19,390:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-27 23:48:19,414:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-27 23:48:19,414:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-27 23:48:19,416:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-27 23:48:19,421:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-27 23:48:19,455:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-27 23:48:19,481:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-27 23:48:19,482:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-27 23:48:19,483:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-27 23:48:19,484:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2025-02-27 23:48:19,524:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-27 23:48:19,550:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-27 23:48:19,550:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-27 23:48:19,552:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-27 23:48:19,594:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-27 23:48:19,620:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-27 23:48:19,621:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-27 23:48:19,622:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-27 23:48:19,622:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-02-27 23:48:19,666:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-27 23:48:19,692:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-27 23:48:19,693:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-27 23:48:19,730:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-27 23:48:19,754:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-27 23:48:19,756:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-27 23:48:19,756:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2025-02-27 23:48:19,816:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-27 23:48:19,817:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-27 23:48:19,877:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-27 23:48:19,878:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-27 23:48:19,881:INFO:Preparing preprocessing pipeline...
2025-02-27 23:48:19,881:INFO:Set up date feature engineering.
2025-02-27 23:48:19,881:INFO:Set up iterative imputation.
2025-02-27 23:48:19,939:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-27 23:48:19,940:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-27 23:48:19,966:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-27 23:48:19,983:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-27 23:48:19,985:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-27 23:48:20,002:INFO:Set up encoding of ordinal features.
2025-02-27 23:48:20,007:WARNING:The number of classes passed to feature Method in the ordinal_features parameter (11) don't match with the number of classes in the data (5).
2025-02-27 23:48:20,007:INFO:Set up encoding of categorical features.
2025-02-27 23:48:20,007:INFO:Set up polynomial features.
2025-02-27 23:48:20,007:INFO:Set up removing multicollinearity.
2025-02-27 23:48:20,007:INFO:Set up feature normalization.
2025-02-27 23:48:20,007:INFO:Set up feature selection.
2025-02-27 23:48:20,066:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-27 23:48:20,068:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-27 23:48:20,529:INFO:Finished creating preprocessing pipeline.
2025-02-27 23:48:20,572:INFO:Pipeline: Pipeline(memory=FastMemory(location=/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/joblib),
         steps=[('date_feature_extractor',
                 TransformerWrapper(include=['Date'],
                                    transformer=ExtractDateTimeFeatures())),
                ('iterative_imputer',
                 TransformerWrapper(transformer=IterativeImputer(cat_estimator=LGBMClassifier(n_jobs=-1,
                                                                                              random_state=123),
                                                                 cat_estimator_prepare_...
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('remove_multicollinearity',
                 TransformerWrapper(exclude=[],
                                    transformer=RemoveMulticollinearity(threshold=0.55))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('feature_selection',
                 TransformerWrapper(exclude=[],
                                    transformer=SelectFromModel(estimator=RandomForestRegressor(),
                                                                max_features=12,
                                                                threshold=-inf)))])
2025-02-27 23:48:20,572:INFO:Creating final display dataframe.
2025-02-27 23:48:22,163:INFO:Setup _display_container:                         Description          Value
0                        Session id            123
1                            Target          Price
2                       Target type     Regression
3               Original data shape    (13580, 21)
4            Transformed data shape    (13580, 13)
5       Transformed train set shape     (9506, 13)
6        Transformed test set shape     (4074, 13)
7                   Ignore features              4
8                  Ordinal features              3
9                  Numeric features              8
10                    Date features              1
11             Categorical features              6
12         Rows with missing values          54.4%
13                       Preprocess           True
14                  Imputation type      iterative
15  Iterative imputation iterations              5
16        Numeric iterative imputer       lightgbm
17    Categorical iterative imputer       lightgbm
18         Maximum one-hot encoding             25
19                  Encoding method           None
20              Polynomial features           True
21                Polynomial degree              2
22         Remove multicollinearity           True
23      Multicollinearity threshold           0.55
24                        Normalize           True
25                 Normalize method         zscore
26                Feature selection           True
27         Feature selection method        classic
28      Feature selection estimator             rf
29      Number of features selected            0.8
30                   Fold Generator          KFold
31                      Fold Number             10
32                         CPU Jobs             -1
33                          Use GPU          False
34                   Log Experiment  DagshubLogger
35                  Experiment Name  house_pricing
36                              USI           1bca
2025-02-27 23:48:22,228:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-27 23:48:22,230:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-27 23:48:22,290:INFO:Soft dependency imported: xgboost: 2.1.3
2025-02-27 23:48:22,292:INFO:Soft dependency imported: catboost: 1.1.1
2025-02-27 23:48:22,292:INFO:Logging experiment in loggers
2025-02-27 23:48:50,728:INFO:SubProcess save_model() called ==================================
2025-02-27 23:48:50,826:INFO:Initializing save_model()
2025-02-27 23:48:50,826:INFO:save_model(model=Pipeline(memory=FastMemory(location=/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/joblib),
         steps=[('date_feature_extractor',
                 TransformerWrapper(include=['Date'],
                                    transformer=ExtractDateTimeFeatures())),
                ('iterative_imputer',
                 TransformerWrapper(transformer=IterativeImputer(cat_estimator=LGBMClassifier(n_jobs=-1,
                                                                                              random_state=123),
                                                                 cat_estimator_prepare_...
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('remove_multicollinearity',
                 TransformerWrapper(exclude=[],
                                    transformer=RemoveMulticollinearity(threshold=0.55))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('feature_selection',
                 TransformerWrapper(exclude=[],
                                    transformer=SelectFromModel(estimator=RandomForestRegressor(),
                                                                max_features=12,
                                                                threshold=-inf)))]), model_name=/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/tmpsqhqlz9z/Transformation Pipeline, prep_pipe_=Pipeline(memory=FastMemory(location=/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/joblib),
         steps=[('date_feature_extractor',
                 TransformerWrapper(include=['Date'],
                                    transformer=ExtractDateTimeFeatures())),
                ('iterative_imputer',
                 TransformerWrapper(transformer=IterativeImputer(cat_estimator=LGBMClassifier(n_jobs=-1,
                                                                                              random_state=123),
                                                                 cat_estimator_prepare_...
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('remove_multicollinearity',
                 TransformerWrapper(exclude=[],
                                    transformer=RemoveMulticollinearity(threshold=0.55))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('feature_selection',
                 TransformerWrapper(exclude=[],
                                    transformer=SelectFromModel(estimator=RandomForestRegressor(),
                                                                max_features=12,
                                                                threshold=-inf)))]), verbose=False, use_case=MLUsecase.REGRESSION, kwargs={})
2025-02-27 23:48:50,826:INFO:Adding model into prep_pipe
2025-02-27 23:48:50,826:WARNING:Only Model saved as it was a pipeline.
2025-02-27 23:48:51,026:INFO:/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/tmpsqhqlz9z/Transformation Pipeline.pkl saved in current working directory
2025-02-27 23:48:51,072:INFO:Pipeline(memory=FastMemory(location=/var/folders/90/q7dj_p6j3gx0cls87s3y0myc0000gq/T/joblib),
         steps=[('date_feature_extractor',
                 TransformerWrapper(include=['Date'],
                                    transformer=ExtractDateTimeFeatures())),
                ('iterative_imputer',
                 TransformerWrapper(transformer=IterativeImputer(cat_estimator=LGBMClassifier(n_jobs=-1,
                                                                                              random_state=123),
                                                                 cat_estimator_prepare_...
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('remove_multicollinearity',
                 TransformerWrapper(exclude=[],
                                    transformer=RemoveMulticollinearity(threshold=0.55))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('feature_selection',
                 TransformerWrapper(exclude=[],
                                    transformer=SelectFromModel(estimator=RandomForestRegressor(),
                                                                max_features=12,
                                                                threshold=-inf)))])
2025-02-27 23:48:51,072:INFO:save_model() successfully completed......................................
2025-02-27 23:48:51,168:INFO:SubProcess save_model() end ==================================
2025-02-27 23:49:02,986:INFO:setup() successfully completed in 5.1s...............
